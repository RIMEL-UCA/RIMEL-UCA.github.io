# Scripts et outils d‚Äôanalyse

Ce dossier regroupe l‚Äôensemble des scripts utilis√©s pour l‚Äôanalyse des √©valuations des projets, depuis la collecte et la structuration des notes jusqu‚Äôaux analyses statistiques et √† l‚Äô√©tude des d√©saccords entre jurys.

## Scripts d‚Äôanalyse et de traitement

### `analyse_number_of_contributors.py`

[`codes/analyse_number_of_contributors.py`](./codes/analyse_number_of_contributors.py)

Ce script analyse la distribution du nombre de contributeurs pour l‚Äôensemble des d√©p√¥ts d‚Äôune organisation GitHub (dans notre cas DataForGood).
Il interroge l‚ÄôAPI GitHub afin de r√©cup√©rer tous les repositories, puis calcule pr√©cis√©ment le nombre de contributeurs.

√Ä partir de ces donn√©es, il produit des statistiques globales (moyenne, m√©diane, √©cart-type, minimum et maximum)
et propose une cat√©gorisation des projets selon leur taille.
Les r√©sultats d√©taill√©s sont export√©s dans un fichier CSV contenant, pour chaque d√©p√¥t, le nombre de contributeurs et son URL.

> L‚Äôutilisation d‚Äôun *GitHub Personal Access Token* est requise afin d‚Äô√©viter les limitations de l‚ÄôAPI.

### `extract_notes.py`

[`codes/extract_notes.py`](./codes/extract_notes.py)

Ce script extrait et restructure les notes brutes des jurys √† partir du fichier unique `all.csv`.
Il regroupe les √©valuations par juge, par projet et par crit√®re, puis calcule s√©par√©ment les scores li√©s aux axes
tests et documentation.

Pour chaque membre du jury, un fichier CSV d√©di√© est g√©n√©r√©, contenant le d√©tail des notes par d√©p√¥t et par crit√®re.
Ces fichiers constituent la base des analyses statistiques ult√©rieures et sont enregistr√©s dans le dossier
`results/1_notes_per_judge/`.

### `statistiques_par_projet.py`

üìÑ [`codes/statistiques_par_projet.py`](./codes/statistiques_par_projet.py)

Ce script agr√®ge les notes attribu√©es par les quatre jurys pour chaque projet.
√Ä partir des fichiers de notes individuels, il calcule :

- les scores moyens pour les axes tests et documentation,
- l‚ÄôICC (*Intraclass Correlation Coefficient*) afin d‚Äô√©valuer le niveau d‚Äôaccord inter-juges.

L‚ÄôICC est automatiquement interpr√©t√© (*poor*, *moderate*, *good*, *excellent*) pour faciliter la lecture des r√©sultats.
Le script g√©n√®re un fichier CSV r√©capitulatif par projet, utilis√© ensuite comme base pour les analyses par question de recherche.

### `analysis_per_question.py`

üìÑ [`codes/analysis_per_question.py`](./codes/analysis_per_question.py)

Ce script permet d'analyser et et de r√©pondre aux trois questions de recherche de l‚Äô√©tude.

√Ä partir du fichier `statistiques_par_projet.csv` et du fichier de correspondance `mapping_projets.csv`,
il regroupe les projets selon :

- la p√©riode temporelle (Avant / Pendant / Apr√®s GenAI),
- le volume de contributeurs (peu / beaucoup),
- le type de projet (AI-related ou non AI-related).

Pour chaque question, il g√©n√®re des fichiers CSV interm√©diaires ainsi que des visualisations synth√©tiques
(courbes d‚Äô√©volution temporelle et matrices comparatives).
Les r√©sultats sont sauvegard√©s dans le dossier `results/3_analysis/` et correspondent aux figures finales de l‚Äô√©tude.

### `mapping_projets.csv`

üìÑ [`codes/mapping_projets.csv`](./codes/mapping_projets.csv)

Ce fichier CSV d√©finit les m√©tadonn√©es utilis√©es pour cat√©goriser chaque projet analys√©.
Il associe √† chaque d√©p√¥t trois dimensions cl√©s :

- la p√©riode temporelle (Avant, Pendant ou Apr√®s l‚Äô√©mergence de la GenAI),
- le volume de contributeurs (peu ou beaucoup),
- la nature du projet (AI-related ou non AI-related).

Ce mapping est utilis√© par le script `analysis_per_question.py` afin de regrouper les projets
et de produire les analyses comparatives et visualisations finales.
Il constitue un √©l√©ment central pour relier les r√©sultats statistiques aux questions de recherche.


### `analyse_desaccords_juges.py`

üìÑ [`codes/analyse_desaccords_juges.py`](./codes/analyse_desaccords_juges.py)

Ce script est d√©di√© √† l‚Äôanalyse des d√©saccords entre les membres du jury lors de la notation des projets.
√Ä partir des fichiers de notes individuels, il calcule les scores globaux par juge pour les axes tests et documentation,
puis analyse la s√©v√©rit√© relative de chaque juge (moyennes, m√©dianes, √©carts-types).

Il mesure √©galement les √©carts de notation entre chaque paire de juges et identifie les projets g√©n√©rant le plus de d√©saccords.
Le script produit des fichiers CSV synth√©tiques ainsi que des visualisations (graphiques de s√©v√©rit√© et matrices de d√©saccord),
sauvegard√©s dans le dossier `results/4_inter_rater_analysis/`.

### `visualisation_detaillee_desaccords.py`

üìÑ [`.codes/visualisation_detaillee_desaccords.py`](./codes/visualisation_detaillee_desaccords.py)

Ce script g√©n√®re des visualisations d√©taill√©es des notations, projet par projet, √† partir des r√©sultats de l‚Äôanalyse
des d√©saccords entre juges.
Il produit une repr√©sentation de type tableau comparatif, proche d‚Äôun export Excel,
montrant les scores attribu√©s par chaque juge ainsi que les moyennes et √©carts-types
pour les axes tests et documentation.

Il identifie √©galement, pour chaque projet, les √©carts minimum et maximum entre juges
et √©tablit un classement des projets selon leur niveau de d√©saccord.
Les figures et fichiers CSV g√©n√©r√©s sont sauvegard√©s dans le dossier `results/4_inter_rater_analysis/`.

---

## Structure du dossier `results/`

Le dossier `results/` regroupe l‚Äôensemble des donn√©es produites au cours du pipeline d‚Äôanalyse,
depuis les notes brutes des jurys jusqu‚Äôaux visualisations utilis√©es dans l‚Äô√©tude.

### `0_input/`

* **all.csv**
  Fichier source contenant l‚Äôensemble des notes brutes saisies par les membres du jury.

### `1_notes_per_judge/`

* **notes_antoine.csv**, **notes_baptiste.csv**, **notes_roxane.csv**, **notes_theo.csv**
  Fichiers de notes nettoy√©s et structur√©s, un par juge, contenant le d√©tail des scores par projet
  et par crit√®re pour les axes tests et documentation.

### `2_statistics/`

* **statistiques_par_projet.csv**
  Fichier r√©capitulatif, incluant les scores moyens par axe et les ICC par projet.

### `3_analysis/`

R√©sultats des analyses correspondant aux trois questions de recherche.

#### `question_1/` ‚Äì Impact de la p√©riode temporelle

* **impact_periode.csv**
* **evolution_temporelle.png**

#### `question_2/` ‚Äì Impact du volume de contributeurs

* **impact_volume_contributeurs.csv**
* **matrice_volume_contributeurs.png**

#### `question_3/` ‚Äì Impact du type de projet

* **impact_type_ai.csv**
* **matrice_type_projet.png**

### `4_inter_rater_analysis/`

R√©sultats d√©di√©s √† l‚Äôanalyse de l‚Äôaccord et des d√©saccords inter-juges.

#### `csv/`

* **comparaison_detaillee.csv**
* **desaccords_par_projet.csv**
* **differences_min_max_par_projet.csv**
* **differences_paires_tests.csv**, **differences_paires_doc.csv**
* **statistiques_par_juge.csv**

#### `images/`

* **severite_juges.png**
* **matrices_desaccord.png**
* **tableau_scores_detailles.png**
* **classement_desaccords.png**
