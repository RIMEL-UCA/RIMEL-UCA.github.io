{
  "embeddings": {
    "contextual_embeddings": {
      "sentence-transformers/paraphrase-MiniLM-L6-v2": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "for each excerpt in the train set, I used this model https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2 to generate sentence embeddings and retrieve the five text snippets which had the highest cosine similarity to the original excerpt"
      }
    }
  },
  "preprocessing": {
    "external_data_sources": {
      "simplewiki": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I compiled a corpus of texts that seemed relevant to this competition (simplewiki, wikipedia, bookcorpus, …)"
      },
      "wikipedia": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I compiled a corpus of texts that seemed relevant to this competition (simplewiki, wikipedia, bookcorpus, …)"
      },
      "bookcorpus": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I compiled a corpus of texts that seemed relevant to this competition (simplewiki, wikipedia, bookcorpus, …)"
      }
    },
    "augmentation_techniques": {
      "backtranslation": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I tried to run multiple rounds with my approach and I also tried to introduce noise in the form of backtranslation and word replacements (predicting MASK tokens)"
      },
      "word_replacement_mask": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I tried to run multiple rounds with my approach and I also tried to introduce noise in the form of backtranslation and word replacements (predicting MASK tokens)"
      }
    },
    "tokenization_method": {
      "transformer_tokenizer": {
        "source": "Inferred from transformer models usage",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "models trained were albert-xxlarge, deberta-large, roberta-large, electra-large and roberta-base"
      }
    }
  },
  "transformer_models": {
    "base_models": {
      "albert-xxlarge-v2": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "models trained were albert-xxlarge, deberta-large, roberta-large, electra-large and roberta-base"
      },
      "microsoft/deberta-large": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "models trained were albert-xxlarge, deberta-large, roberta-large, electra-large and roberta-base"
      },
      "roberta-large": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "models trained were albert-xxlarge, deberta-large, roberta-large, electra-large and roberta-base"
      },
      "roberta-base": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I trained a roberta base model on the train set and used my best model to label the external data that I retrieved in the first step"
      },
      "google/electra-large-discriminator": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "models trained were albert-xxlarge, deberta-large, roberta-large, electra-large and roberta-base"
      }
    },
    "fine_tuning_strategy": {
      "full_model_fine_tuning": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "first, I trained a single model just on the pseudo-labeled data... Then: I used the model from the previous step and trained 6 models on 6 folds of the original train set"
      }
    }
  },
  "loss_functions": {
    "primary_loss": {
      "mse": {
        "source": "Inferred from RMSE metric and standard regression task",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "The competition used RMSE as the evaluation metric, which typically corresponds to MSE loss during training"
      }
    }
  },
  "training_strategy": {
    "optimizer": {
      "adamw": {
        "source": "Standard optimizer for transformer models",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "Standard practice for transformer fine-tuning, inferred from the solution approach"
      }
    },
    "learning_rate": {
      "0.000007": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I used low learning rates (7e-6 to 1e-5)"
      }
    },
    "learning_rate_schedule": {
      "linear_warmup_decay": {
        "source": "Standard practice mentioned in solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "Standard transformer training approach with warmup"
      }
    },
    "batch_size": {
      "16": {
        "source": "GitHub training notebook",
        "link": "https://colab.research.google.com/drive/1DxfF_qp323oibSpevw94mIkv3woRrkVO",
        "quote": "hyperparams={'bs': 16, 'lr': 1e-4, 'ep': 5, 'bias': False, 'init': None}"
      }
    },
    "epochs": {
      "4": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I also trained a single albert-xxlarge on all of the training data without evaluation for 4 epochs"
      }
    },
    "pseudo_labeling": {
      "true": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I trained a roberta base model on the train set and used my best model to label the external data... first, I trained a single model just on the pseudo-labeled data"
      }
    },
    "cross_validation": {
      "6_fold": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I used 6-fold crossvalidation or bootstrapping for the models used in my final submission"
      },
      "bootstrap_sampling": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I used 6-fold crossvalidation or bootstrapping for the models used in my final submission... I also trained some models using bootstrap sampling instead of crossvalidation"
      }
    }
  },
  "model_architecture": {
    "pooling_strategy": {
      "cls_token": {
        "source": "Standard transformer architecture",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "Inferred from the use of BERT-based models which use CLS token for classification/regression"
      }
    }
  },
  "postprocessing": {
    "ensemble_method": {
      "ridge_regression": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I used ridge regression to ensemble these models... I got the out of fold predictions for each model that I trained... I used the new split to train 6 ridge regression models"
      },
      "weighted_average": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "ensembles and other models were aggregated using a weighted average (the weights for each ensemble were chosen by feel and public LB score)"
      }
    }
  },
  "frameworks": {
    "deep_learning_framework": {
      "pytorch": {
        "source": "GitHub repository",
        "link": "https://github.com/mathislucka/kaggle_clrp_1st_place_solution",
        "quote": "Repository uses PyTorch for implementation"
      }
    },
    "libraries": {
      "transformers": {
        "source": "GitHub repository",
        "link": "https://github.com/mathislucka/kaggle_clrp_1st_place_solution",
        "quote": "Uses HuggingFace transformers library for model implementation"
      },
      "sentence-transformers": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I used sentence bert (Reimers and Gurevych 2019 - https://github.com/UKPLab/sentence-transformers)"
      },
      "sklearn": {
        "source": "Kaggle discussion - 1st place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "I used ridge regression to ensemble these models - Ridge is from sklearn"
      }
    }
  }
}
