{
  "preprocessing": {
    "tokenization_method": {
      "transformer_tokenizer": {
        "source": "Inferred from transformer models usage",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257844",
        "quote": "Transformer models require their respective tokenizers"
      }
    },
    "feature_engineering": {
      "textstat_features": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "Textstat features like flesch_reading_ease, smog_index, etc. were concatenated with transformer outputs"
      }
    }
  },
  "transformer_models": {
    "base_models": {
      "microsoft/deberta-large": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "Final ensembles usually had about 38 models including microsoft/deberta-large with CV 0.484005"
      },
      "microsoft/deberta-base": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "microsoft/deberta-base with CV 0.486186 included in ensemble"
      },
      "deepset/roberta-base-squad2": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "deepset/roberta-base-squad2 models with CV 0.495078 and 0.490829"
      },
      "deepset/roberta-large-squad2": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "deepset/roberta-large-squad2 models with CV 0.482696, 0.489583, 0.494185"
      },
      "distilroberta-base": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "distilroberta-base models with CV 0.505654 and 0.502802"
      },
      "funnel-transformer/large-base": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "funnel-transformer/large-base models with CV 0.495666, 0.493789, 0.491098, 0.523333"
      },
      "bert-large-uncased": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "bert-large-uncased models with CV 0.524264 and 0.538173"
      },
      "bert-large-cased": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "bert-large-cased with CV 0.505559"
      },
      "bert-large-cased-whole-word-masking": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "bert-large-cased-whole-word-masking with CV 0.514011"
      },
      "facebook/bart-base": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "facebook/bart-base models with CV 0.534491, 0.556662, 0.530183"
      },
      "facebook/bart-large": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "facebook/bart-large models with CV 0.543126 and 0.526896"
      },
      "roberta-base": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "roberta-base with CV 0.49835"
      },
      "albert-large-v2": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "albert-large-v2 models with CV 0.545104, 0.506411, 0.508817, 0.529876"
      },
      "google/electra-large-discriminator": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "google/electra-large-discriminator with CV 0.514033"
      },
      "xlm-roberta-large": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "xlm-roberta-large with CV 0.505576"
      },
      "sentence-transformers/LaBSE": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "sentence-transformers/LaBSE with CV 0.525731"
      },
      "sentence-transformers/paraphrase-mpnet-base-v2": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "sentence-transformers/paraphrase-mpnet-base-v2 with CV 0.510096"
      }
    },
    "fine_tuning_strategy": {
      "full_model_fine_tuning": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "Full model fine-tuning approach from training scripts"
      }
    }
  },
  "loss_functions": {
    "primary_loss": {
      "mse": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "MSE loss used for regression: criterion = nn.MSELoss()"
      },
      "kl_divergence": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "KL divergence loss also experimented with for some models"
      }
    }
  },
  "training_strategy": {
    "optimizer": {
      "adamw": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "optimizer = AdamW(model.parameters(), lr=0.00005, weight_decay=1.0)"
      }
    },
    "learning_rate": {
      "0.00005": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "Learning rate set to 0.00005 or 0.000025: lr=0.00005"
      }
    },
    "learning_rate_schedule": {
      "cosine_annealing": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)"
      }
    },
    "batch_size": {
      "16": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "batch_size = 16 for most models, 12 for larger models"
      }
    },
    "epochs": {
      "6": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "Epochs: 6 with SWA after epoch 3"
      }
    },
    "weight_decay": {
      "1.0": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "Optimiser: AdamW, Weight decay: 1.0"
      }
    },
    "seed_averaging": {
      "true": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "On average I was seeing about 0.01 gap between CV & public LB... I used 5 seeds (5 fold CV) for a total of 25 runs per experiment, which I would then average"
      }
    },
    "cross_validation": {
      "5_fold": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "5-fold cross-validation used for model training"
      }
    },
    "regularization": {
      "swa-after-epoch-3": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "Epochs: 6 with SWA after epoch 3"
      },
      "no-dropout": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "No dropout used in the final models"
      }
    }
  },
  "model_architecture": {
    "pooling_strategy": {
      "attention-block": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "I used the AttentionBlock from this notebook, but applied it to the sequence last hidden state of the transformer"
      },
      "sequence-last-hidden-state": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "applied it to the sequence last hidden state of the transformer"
      }
    },
    "additional_features": {
      "flesch-reading-ease": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "I also added a couple of features using textstat, flesch_reading_ease and smog_index and concatenated them to the output of the AttentionBlock before the regression head"
      },
      "smog-index": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "I also added a couple of features using textstat, flesch_reading_ease and smog_index and concatenated them to the output of the AttentionBlock before the regression head"
      }
    },
    "attention_mechanism": {
      "attention_block": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "class AttentionBlock: custom attention mechanism for pooling"
      }
    },
    "custom_layers": {
      "attention_head": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "Custom attention head on top of transformer output"
      }
    }
  },
  "postprocessing": {
    "ensemble_method": {
      "ridge_regression": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "RidgeCV used for ensembling 38 models"
      },
      "bayesian_ridge": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "BayesianRidge regression used for stacking"
      },
      "netflix_method": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "Netflix method (collaborative filtering approach) for ensembling"
      },
      "weighted_average": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "Weighted averaging of predictions from different stacking methods"
      }
    }
  },
  "frameworks": {
    "deep_learning_framework": {
      "pytorch": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "import torch, torch.nn as nn"
      },
      "pytorch_lightning": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "PyTorch Lightning used for training: import pytorch_lightning as pl"
      }
    },
    "libraries": {
      "transformers": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "from transformers import AutoModel, AutoTokenizer"
      },
      "sklearn": {
        "source": "GitHub repository",
        "link": "https://github.com/Anjum48/commonlitreadabilityprize",
        "quote": "from sklearn.linear_model import RidgeCV, BayesianRidge"
      },
      "textstat": {
        "source": "Kaggle discussion - 4th place solution",
        "link": "https://www.kaggle.com/c/commonlitreadabilityprize/discussion/257897",
        "quote": "textstat library used for feature extraction: flesch_reading_ease, smog_index"
      }
    }
  }
}
