{
  "primary_loss": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "I used optuna to get 2 different sets of optimal hyperparameters for the leaf-wise growth xgboost model and an optimal keras MLP layout, always based on the AUC score to measure the overall discrimination"
  },
  "learning_rate": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "XGBClassifier(tree_method='gpu_hist', grow_policy='lossguide', eval_metric='auc', min_child_weight=50, subsample=0.65, colsample_bytree=0.6, colsample_bylevel=0.6, colsample_bynode=0.55, learning_rate=0.01096, max_depth=8, max_bin=320, max_leaves=0)"
  },
  "regularization": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "min_child_weight=50, subsample=0.65, colsample_bytree=0.6, colsample_bylevel=0.6, colsample_bynode=0.55"
  },
  "cross_validation": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "4x xgboost 1 (Full sample + 10 CV Folds) * 5 Seeds = 4x 55 models"
  },
  "additional_layers": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "I used optuna to get 2 different sets of optimal hyperparameters for the leaf-wise growth xgboost model and an optimal keras MLP layout"
  },
  "ensemble_method": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "Ensembling was done using oof predictions and finding the best weights using scipy minimizing the negative amex metric"
  },
  "prediction_averaging": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "4x xgboost 1 (Full sample + 10 CV Folds) * 5 Seeds = 4x 55 models. 4x xgboost 2 (Full sample + 10 CV Folds) * 5 Seeds = 4x 55 models. 4x Keras MLP (Full sample + 10 CV Folds) * 10 Seeds = 4x 110 models. Ensembling with public DART score"
  },
  "deep_learning_framework": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "4x Keras MLP (Full sample + 10 CV Folds) * 10 Seeds = 4x 110 models"
  },
  "libraries": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "XGBClassifier... I used optuna to get 2 different sets of optimal hyperparameters... finding the best weights using scipy"
  },
  "aggregation_features": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "The initial 188 features Avg / Max / Min / Std / Slope of the last 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 and 12 statements"
  },
  "time_series_features": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "The initial 188 features Avg / Max / Min / Std / Slope of the last 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 and 12 statements. Ratio and Difference of previous engineered features"
  },
  "categorical_encoding": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "Categorical features Weight of Evidence encoding"
  },
  "feature_scaling": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "For the MLP I scaled the data using rank gauss"
  },
  "feature_selection_method": {
    "source": "Kaggle 105th place solution",
    "link": "https://www.kaggle.com/competitions/amex-default-prediction/discussion/348140",
    "quote": "As I had way too many features to work with, I created an iterative process to get rid of the less important ones. It consisted in randomly allocate features to different 'buckets' then use the initial xgboost model to get the shap feature importances and drop the bad ones... Then I did some Aversarial Validation to drop unstable features"
  }
}
