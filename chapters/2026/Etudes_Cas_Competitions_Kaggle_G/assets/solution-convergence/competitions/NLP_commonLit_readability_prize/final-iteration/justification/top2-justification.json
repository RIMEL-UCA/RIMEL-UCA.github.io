{
  "transformer-tokenizer": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize",
    "quote": "All experiments use transformer tokenizers as shown in the code: RobertaTokenizer, AutoTokenizer, ElectraTokenizer, DebertaTokenizer, FunnelTokenizer, etc."
  },
  "albert-xxlarge-v2": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "15. albert-v2-xxlarge | 0.486 | 0.467 | 0.120 |"
  },
  "bert-base-uncased": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "17. bert-base-uncased | 0.507 |   | -0.140 |"
  },
  "distilbart-cnn-12-6": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "19. distilbart-cnn-12-6 | 0.489 | 0.479 | 0.090 |"
  },
  "facebook/bart-large": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "6. bart-large | 0.476 | 0.469 | 0.090 |"
  },
  "funnel-transformer/large": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "13. funnel-large | 0.475 | 0.464 | 0.110 |"
  },
  "funnel-transformer/large-base": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "8. funnel-large-base | 0.479 | 0.471 | 0.050 |"
  },
  "google/electra-base-discriminator": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "16. electra-base | 0.493 |   | -0.170 |"
  },
  "google/electra-large-discriminator": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "7. electra-large | 0.483 | 0.470 | 0.050 |"
  },
  "gpt2-medium": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "14. gpt2-medium | 0.498 | 0.478 | 0.170 |"
  },
  "microsoft/deberta-large": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "9. deberta-large | 0.481 | 0.460 | 0.230 |"
  },
  "microsoft/deberta-v2-xlarge": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "10. deberta-v2-xlarge | 0.486 | 0.466 | 0.050 |"
  },
  "microsoft/deberta-v2-xxlarge": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "12. deberta-v2-xxlarge | 0.482 | 0.465 | 0.140 |"
  },
  "muppet-roberta-large": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "5. muppet-roberta-large | 0.480 | 0.466 | 0.022 |"
  },
  "roberta-base": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "1. roberta-base -> svr | 0.500 | 0.476 | 0.020 | 2. roberta-base -> ridge | 0.500 |   | 0.020 | 3. roberta-base | 0.485 | 0.476 | 0.040 |"
  },
  "roberta-large": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "4. roberta-large | 0.483 | 0.463 | 0.088 |"
  },
  "sentence-transformers/paraphrase-mpnet-base-v2": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "11. mpnet-base | 0.482 | 0.470 | 0.130 |"
  },
  "t5-large": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "18. t5-large | 0.504 |   | -0.110 |"
  },
  "full-model-fine-tuning": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize",
    "quote": "All models are fully fine-tuned as shown in the experiment files (ex014.py, ex072.py, ex107.py, etc.)"
  },
  "mlm-pretraining": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "I trained the model with dropout set to 0 except for 1 and 2. And, I did mlm pretraining only for model 3."
  },
  "mse": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex107.py#L212-L245",
    "quote": "criterion = nn.MSELoss()"
  },
  "adamw": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex107.py#L212-L245",
    "quote": "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, betas=(0.9, 0.98), weight_decay=weight_decay)"
  },
  "learning_rate": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp",
    "quote": "Learning rates vary by model: ex107.py uses lr = 2e-5, ex194.py uses lr = 8e-6, ex272.py uses lr = 1e-5"
  },
  "linear-warmup-decay": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex107.py#L212-L245",
    "quote": "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_optimization_steps)"
  },
  "batch_size": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp",
    "quote": "Batch sizes vary: ex107.py BATCH_SIZE = 8, ex194.py BATCH_SIZE = 4, ex465.py BATCH_SIZE = 16"
  },
  "epochs": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex107.py#L33-L153",
    "quote": "n_epochs = 3 (for ex107.py bart-large) and n_epochs = 5 (for most other models)"
  },
  "weight_decay": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp",
    "quote": "weight_decay = 0.1 for most models, weight_decay = 0.01 for ex272.py (funnel-large-base)"
  },
  "dropout-0.2": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex014.py#L95-L123",
    "quote": "self.drop = nn.Dropout(0.2) ... self.drop2 = nn.Dropout(0.2)"
  },
  "dropout-zero": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "I trained the model with dropout set to 0 except for 1 and 2."
  },
  "5-fold": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "I used the following. https://www.kaggle.com/abhishek/step-1-create-folds (5-fold cross-validation)"
  },
  "attention-pooling": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex237.py#L99-L119",
    "quote": "self.attention = nn.Sequential(nn.Linear(768, 512), nn.Tanh(), nn.Linear(512, 1), nn.Softmax(dim=1))"
  },
  "cls-token": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex194.py#L98-L121",
    "quote": "emb = self.deberta_model(ids, attention_mask=mask, token_type_ids=token_type_ids)['last_hidden_state'][:, 0, :]"
  },
  "linear-head": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex194.py#L98-L121",
    "quote": "self.out = nn.Linear(1536, 1)"
  },
  "mean-pooling": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex107.py#L97-L121",
    "quote": "emb = self.bart(ids, attention_mask=mask)['last_hidden_state']; emb = torch.mean(emb, axis=1)"
  },
  "sequence-last-hidden-state": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex107.py#L97-L121",
    "quote": "emb = self.bart(ids, attention_mask=mask)['last_hidden_state']"
  },
  "nelder-mead-optimization": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "Weights were calculated by nelder-mead and then tuned for higher LB... result = minimize(f, weight_init , method=\"Nelder-Mead\")"
  },
  "post-processing-multipliers": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "Post process improved the score by about 0.001 ~ 0.002... pred >= 0.3 -> pred * 1.07, 0.3 > pred >= 0 -> pred * 1.2, 0 > pred >= -0.7 -> pred * 0.974, -0.7 > pred >= -0.9 -> pred * 1.01, -0.9 > pred > =-2 -> pred * 1.021, -2 > pred -> pred * 1.027"
  },
  "ridge-regression": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "2. roberta-base -> ridge | 0.500 |   | 0.020 |"
  },
  "stacking": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "1. roberta-base -> svr | 0.500 | 0.476 | 0.020 | 2. roberta-base -> ridge | 0.500 |   | 0.020 |"
  },
  "weighted-average": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "I ensembled 19 models and did a post process. I adjusted the weights of the model by looking at the LB and CV. I've included negative values for weights as well as positive ones."
  },
  "ridge": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "2. roberta-base -> ridge | 0.500 |   | 0.020 |"
  },
  "svr": {
    "source": "Kaggle discussion - 2nd place solution",
    "link": "https://www.kaggle.com/competitions/commonlitreadabilityprize/writeups/rist-takoi-2nd-place-solution",
    "quote": "1. roberta-base -> svr | 0.500 | 0.476 | 0.020 |"
  },
  "pytorch": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize",
    "quote": "All code uses PyTorch: import torch, import torch.nn as nn, from torch.utils.data import Dataset, DataLoader"
  },
  "sklearn": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize/tree/main/exp/ex015.py#L188-L220",
    "quote": "Uses sklearn for stacking models (SVR and Ridge) as shown in ensemble code"
  },
  "transformers": {
    "source": "GitHub repository - 2nd place solution",
    "link": "https://github.com/TakoiHirokazu/kaggle_commonLit_readability_prize",
    "quote": "from transformers import AutoModel, AutoConfig, AdamW, get_linear_schedule_with_warmup, RobertaTokenizer, etc."
  }
}
