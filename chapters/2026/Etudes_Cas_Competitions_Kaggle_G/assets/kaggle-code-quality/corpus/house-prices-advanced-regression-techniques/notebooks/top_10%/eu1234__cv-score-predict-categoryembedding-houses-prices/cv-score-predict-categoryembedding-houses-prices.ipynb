{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### More about libraries used in this notebook:\n\ncv-score-predict: \n- [https://www.kaggle.com/discussions/general/666850](https://www.kaggle.com/discussions/general/666850)\n- [https://pypi.org/project/cv-score-predict/](https://pypi.org/project/cv-score-predict/)\n\ncategory-embedding:\n - [https://pypi.org/project/category-embedding/](https://pypi.org/project/category-embedding/)","metadata":{}},{"cell_type":"code","source":"pip install cv-score-predict category-embedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:38:49.343621Z","iopub.execute_input":"2026-01-15T13:38:49.344451Z","iopub.status.idle":"2026-01-15T13:38:54.091631Z","shell.execute_reply.started":"2026-01-15T13:38:49.344406Z","shell.execute_reply":"2026-01-15T13:38:54.090222Z"},"scrolled":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport optuna\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom cv_score_predict import cv_score_predict\nfrom category_embedding import CategoryEmbedding\n\n# Use gpu if available\nfrom catboost.utils import get_gpu_device_count\ndevice = 'gpu' if get_gpu_device_count() > 0 else 'cpu'\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:38:54.094638Z","iopub.execute_input":"2026-01-15T13:38:54.095243Z","iopub.status.idle":"2026-01-15T13:38:54.109417Z","shell.execute_reply.started":"2026-01-15T13:38:54.095202Z","shell.execute_reply":"2026-01-15T13:38:54.108182Z"},"_kg_hide-output":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\nmain_dir = Path('/kaggle/input/house-prices-advanced-regression-techniques')\nX = pd.read_csv(main_dir / 'train.csv').drop(columns='Id')\ny = np.log1p(X.pop('SalePrice')) # log-transform target\nX_test = pd.read_csv(main_dir / 'test.csv').drop(columns='Id')\nsubmit = pd.read_csv(main_dir / 'sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:38:54.110953Z","iopub.execute_input":"2026-01-15T13:38:54.111408Z","iopub.status.idle":"2026-01-15T13:38:54.190076Z","shell.execute_reply.started":"2026-01-15T13:38:54.111377Z","shell.execute_reply":"2026-01-15T13:38:54.188693Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Select features","metadata":{}},{"cell_type":"code","source":"def columns_types(X):\n    '''Return lists of columns types'''\n    # Columns types\n    cat_cols = X.select_dtypes(include=['object', 'category']).columns.to_list()\n    num_cols = [c for c in X.columns if c not in cat_cols]\n    \n    # Unique values per column\n    cat_unique = X[cat_cols].nunique()\n    num_unique = X[num_cols].nunique()\n    \n    # Columns' cardinality\n    cat_low_card_cols = list(cat_unique.index[cat_unique.le(2)]) \n    num_low_card_cols = list(num_unique.index[num_unique.le(2)]) # will treat as categorical\n    low_card_cols = cat_low_card_cols + num_low_card_cols # will encode into 1 dimension\n    \n    cat_med_card_cols = list(cat_unique.index[cat_unique.gt(2) & cat_unique.le(10)]) \n    num_med_card_cols = list(num_unique.index[num_unique.gt(2) & num_unique.le(10)])\n    med_card_cols = cat_med_card_cols + num_med_card_cols # will encode into 2 dims\n    \n    high_card_cols = list(cat_unique.index[cat_unique.gt(10)])\n    \n    # Final columns types\n    cat_cols = low_card_cols + med_card_cols + high_card_cols\n    num_cols = [c for c in num_cols \n                if (c not in num_low_card_cols) \n                and (c not in num_med_card_cols)]\n    \n    # Create a list of encoding dimensions for each column\n    enc_dims_list = []\n    for c in cat_cols:\n        if c in low_card_cols:\n            enc_dims_list.append(1)  # encode to 1 dimension\n        elif c in med_card_cols:\n            enc_dims_list.append(2)  # encode to 2 dimensions\n        else:\n            enc_dims_list.append(10) # encode to 10 dimensions\n    \n    # Sanity check\n    assert(len(X.columns) == len(cat_cols + num_cols))\n    assert(len(cat_cols) == len(enc_dims_list))\n\n    return num_cols, cat_cols, enc_dims_list\n    \ndef feature_selector(X, y):\n    '''Plot model's feature importance and select the best features'''\n    # Avoid mutation\n    X = X.copy()\n    \n    # Detect columns type\n    _, cat_cols, _ = columns_types(X)\n    \n    # Convert to numerical categories\n    oe = OrdinalEncoder(dtype=int, \n                        handle_unknown='use_encoded_value', \n                        unknown_value=-1,\n                        encoded_missing_value=-1,\n                       ).set_output(transform='pandas')\n    X[cat_cols] = oe.fit_transform(X[cat_cols]).astype('category')\n\n    # Create model\n    model = lgb.LGBMRegressor(verbosity=-1)\n\n    # Feature selector\n    sfs = SequentialFeatureSelector(\n        model, \n        n_features_to_select='auto', \n        tol=0.001, # average per feature is ~0.0015 (rmse with all features is ~0.125)\n        direction='forward',\n        scoring='neg_root_mean_squared_error',\n        cv=3,\n        n_jobs=-1,\n        )\n    sfs.fit(X, y)\n\n    # Plot feature importance\n    model.fit(X, y)\n    fig, ax = plt.subplots(figsize=(8, 14))\n    lgb.plot_importance(\n        model, \n        importance_type='gain', \n        max_num_features=None, \n        height=0.5,\n        grid=False,\n        precision=0,\n        title='LGBM feature importance',\n        ax=ax,\n        );\n    return X.columns[sfs.get_support(indices=True)].to_list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:38:54.192457Z","iopub.execute_input":"2026-01-15T13:38:54.19281Z","iopub.status.idle":"2026-01-15T13:38:54.21608Z","shell.execute_reply.started":"2026-01-15T13:38:54.192782Z","shell.execute_reply":"2026-01-15T13:38:54.21478Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select best features\nselect_feats = False\n\nif select_feats:\n    best_features = feature_selector(X, y)\n    \nelse: # Previously selected\n    best_features = [\n        'MSSubClass', 'MSZoning', 'Neighborhood', 'OverallQual', 'OverallCond', \n        'YearBuilt', 'BsmtFinSF1', 'TotalBsmtSF', 'CentralAir', 'GrLivArea', \n        'Fireplaces', 'GarageCars', 'ScreenPorch',\n    ]\nX = X[best_features]\nX_test = X_test[best_features]\nnum_cols, cat_cols, enc_dims_list = columns_types(X)\n\nfor c in (num_cols, cat_cols, enc_dims_list):\n    print(c)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:38:54.21728Z","iopub.execute_input":"2026-01-15T13:38:54.217565Z","iopub.status.idle":"2026-01-15T13:38:54.258636Z","shell.execute_reply.started":"2026-01-15T13:38:54.21754Z","shell.execute_reply":"2026-01-15T13:38:54.256649Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Encoder tuning","metadata":{}},{"cell_type":"code","source":"def enc_objective(\n    trial, X_train, y_train, X_val, y_val, cat_cols=[], num_cols=[], random_state=42\n):\n    '''Optuna objective for CategoryEmbedding tuning'''\n    # Model capacity\n    hidden_units = trial.suggest_categorical('hidden_units', [32, 64, 128, 256])\n    n_blocks = trial.suggest_int('n_blocks', 1, 3)\n\n    # Regularization\n    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n    l2_emb = trial.suggest_float('l2_emb', 1e-7, 1e-5, log=True)\n    l2_dense = trial.suggest_float('l2_dense', 1e-7, 1e-5, log=True)\n\n    # Optimization\n    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n\n    # Build model with these params\n    model = CategoryEmbedding(\n        task='regression',\n        log_target=False, # Target is already transformed\n        categorical_cols=cat_cols,\n        numeric_cols=num_cols,\n        embedding_dims=enc_dims_list,\n        hidden_units=hidden_units,\n        n_blocks=n_blocks,\n        dropout_rate=dropout_rate,\n        l2_emb=l2_emb,\n        l2_dense=l2_dense,\n        lr=lr,\n        batch_size=batch_size,\n        epochs=100,\n        verbose=0,\n    )\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n\n    return np.sqrt(mean_squared_error(y_val, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:38:54.26028Z","iopub.execute_input":"2026-01-15T13:38:54.26058Z","iopub.status.idle":"2026-01-15T13:38:54.271457Z","shell.execute_reply.started":"2026-01-15T13:38:54.260553Z","shell.execute_reply":"2026-01-15T13:38:54.27002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\n# Tune categorical encoder\ntune_encoder = False\n\nif tune_encoder:\n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y)\n\n    # Impute missing values\n    cat_imp = SimpleImputer(strategy='constant', fill_value='##missing##')\n    num_imp = SimpleImputer(strategy='median')\n\n    X_train[cat_cols] = cat_imp.fit_transform(X_train[cat_cols])\n    X_train[num_cols] = num_imp.fit_transform(X_train[num_cols])\n    X_val[cat_cols] = cat_imp.transform(X_val[cat_cols])\n    X_val[num_cols] = num_imp.transform(X_val[num_cols])\n    \n    def objective(trial):\n        return enc_objective(\n            trial, X_train, y_train, X_val, y_val, \n            cat_cols=cat_cols, num_cols=num_cols, random_state=42,\n        )\n    sampler = optuna.samplers.TPESampler(multivariate=True, seed=42)\n    study = optuna.create_study(direction='minimize', sampler=sampler)\n    study.optimize(objective, n_trials=50)\n    \n    # Show best results\n    completed_trials = [t for t in study.trials \n                        if t.state == optuna.trial.TrialState.COMPLETE]\n    print(f'Number of completed trials: {len(completed_trials)}')\n    print(f'Best score: {study.best_trial.value:.3f}')\n    print(f'Best params:')\n    for k, v in study.best_trial.params.items(): \n        print(f\"'{k}': {v},\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:55:55.84891Z","iopub.execute_input":"2026-01-15T13:55:55.850316Z","iopub.status.idle":"2026-01-15T13:55:55.860832Z","shell.execute_reply.started":"2026-01-15T13:55:55.850266Z","shell.execute_reply":"2026-01-15T13:55:55.859816Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Category encoding","metadata":{"execution":{"iopub.status.busy":"2026-01-14T10:49:47.602967Z","iopub.execute_input":"2026-01-14T10:49:47.603358Z","iopub.status.idle":"2026-01-14T10:49:47.608575Z","shell.execute_reply.started":"2026-01-14T10:49:47.603324Z","shell.execute_reply":"2026-01-14T10:49:47.607429Z"}}},{"cell_type":"code","source":"%%time\n\n# Best parameters for the encoder resulted from optuna tuning\nenc_params = { # Custom embedding dims - rmse 0.152\n    'hidden_units': 128, \n    'n_blocks': 2, \n    'dropout_rate': 0.0013647988526280241, \n    'l2_emb': 2.9022738929868166e-06, \n    'l2_dense': 1.1505737606096325e-07, \n    'lr': 0.009647280154032394, \n    'batch_size': 64,\n}\n# Encode categorical\nenc = CategoryEmbedding(\n    task='regression',\n    log_target=False, # Target is already log transformed\n    categorical_cols=cat_cols,\n    numeric_cols=num_cols,\n    embedding_dims=enc_dims_list,\n    epochs=100,\n    scaled_num_out=True,   # return scaled numeric features\n    verbose=0,\n    **enc_params,\n)\n# Impute missing values\ncat_imp = SimpleImputer(strategy='constant', fill_value='##missing##')\nnum_imp = SimpleImputer(strategy='mean')\n\nX_enc = X.copy()\nX_enc[cat_cols] = cat_imp.fit_transform(X_enc[cat_cols])\nX_enc[num_cols] = num_imp.fit_transform(X_enc[num_cols])\nX_enc = enc.fit_transform(X_enc, y)\n\nX_test_enc = X_test.copy()\nX_test_enc[cat_cols] = cat_imp.transform(X_test_enc[cat_cols])\nX_test_enc[num_cols] = num_imp.transform(X_test_enc[num_cols])\nX_test_enc = enc.transform(X_test_enc)\n\n# Predict using encoder model to use it as meta-feature\nenc_preds = enc.predict(X_test)\n\nX_enc.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:56:09.495592Z","iopub.execute_input":"2026-01-15T13:56:09.495963Z","iopub.status.idle":"2026-01-15T13:56:31.580258Z","shell.execute_reply.started":"2026-01-15T13:56:09.495933Z","shell.execute_reply":"2026-01-15T13:56:31.579115Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models tuning","metadata":{}},{"cell_type":"code","source":"def model_objective(trial, X, y, model_type='lgb', processor=None, process_categorical=True,\n                    n_splits=3, device='cpu', random_state=42):\n    '''\n    Objective function for GBMs models hyperparameter tuning\n    '''\n    # Define hyperparameter search space\n    if model_type == 'lgb':\n        tune_params = {\n            'lgb': {\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n                'num_leaves': trial.suggest_int('num_leaves', 8, 256),\n                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 200),\n                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n                'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n                'subsample_freq': trial.suggest_int('subsample_freq', 0, 10),\n                \n                'n_estimators': 5000,\n                'metric': 'rmse',\n                'objective': 'regression',\n                'device': device,\n                'verbosity': -1,\n                'n_jobs': -1,\n                }}\n    elif model_type == 'xgb':\n        tune_params = {\n            'xgb': {\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n                'max_depth': trial.suggest_int('max_depth', 3, 20),\n                'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n                'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n    \n                'n_estimators': 5000,\n                'eval_metric': 'rmse',\n                'objective': 'reg:squarederror',\n                'device': 'cuda' if device == 'gpu' else 'cpu',\n                'verbosity': 0,\n                'n_jobs': -1,\n                }}\n    elif model_type == 'cb':\n        tune_params = {\n            'cb': {\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n                'depth': trial.suggest_int('depth', 4, 10),\n                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n                'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n                'rsm': trial.suggest_float('rsm', 0.5, 1.0),\n    \n                'iterations': 5000,\n                'objective': 'RMSE',\n                'eval_metric': 'RMSE',\n                'task_type': 'GPU' if device == 'gpu' else 'CPU',\n                'border_count': 128 if device == 'gpu' else 254,\n                'verbose': 0,\n                'thread_count': -1,\n            }}\n    # Train and predict\n    oof_preds_df, _, _ = cv_score_predict(\n        X=X,\n        y=y,\n        X_test=None,\n        pred_type='regression',\n        processor=processor,\n        process_categorical=True,        \n        models=model_type,\n        params_dict=tune_params,\n        random_state=random_state,     \n        n_splits=n_splits,\n        early_stopping_rounds=50,\n        verbose=0,\n    )\n    return np.sqrt(mean_squared_error(y, oof_preds_df.mean(axis=1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T13:33:32.13056Z","iopub.execute_input":"2026-01-15T13:33:32.131Z","iopub.status.idle":"2026-01-15T13:33:32.147651Z","shell.execute_reply.started":"2026-01-15T13:33:32.130966Z","shell.execute_reply":"2026-01-15T13:33:32.146552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ntune_model = None # Set to 'lgb', 'xgb', 'cb', or None to skip\n\nif tune_model != None: \n    # Define objective function with closure\n    def objective(trial):\n        return model_objective(\n            trial, X_enc, y, model_type=tune_model, processor=None, \n            process_categorical=True, n_splits=5, device=device, random_state=42,\n        )\n    # Optuna study\n    sampler = optuna.samplers.TPESampler(multivariate=True, seed=42)\n    study = optuna.create_study(direction='minimize', sampler=sampler)\n    study.optimize(objective, n_trials=50, timeout=60*60*12)  # 12 hours timeout\n\n    # Show best results\n    completed_trials = [t for t in study.trials \n                        if t.state == optuna.trial.TrialState.COMPLETE]\n    print(f'Number of completed trials: {len(completed_trials)}')\n    print(f'Best score: {study.best_trial.value:.3f}')\n    print(f'Best params:')\n    for k, v in study.best_trial.params.items(): \n        print(f\"'{k}': {v},\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T14:29:30.795384Z","iopub.execute_input":"2026-01-15T14:29:30.795713Z","iopub.status.idle":"2026-01-15T14:35:20.572757Z","shell.execute_reply.started":"2026-01-15T14:29:30.795688Z","shell.execute_reply":"2026-01-15T14:35:20.571642Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train models and cross-validate","metadata":{}},{"cell_type":"code","source":"# Custom parameters with high iterations number to allow space for early stopping\nparams_dict={ \n    'lgb': { # Encoded categoricals - rmse 0.127\n        'learning_rate': 0.010963839052288087,\n        'num_leaves': 175,\n        'min_data_in_leaf': 12,\n        'reg_alpha': 0.00012761285111661527,\n        'reg_lambda': 4.682367483456758e-08,\n        'feature_fraction': 0.4404415618024636,\n        'subsample': 0.6000289421974677,\n        'subsample_freq': 2,\n\n        'n_estimators': 5000,\n        'metric': 'rmse',\n        'objective': 'regression',\n        'device': device,\n        'verbosity': -1,\n        'n_jobs': -1,\n    },    \n    'xgb': { # Encoded categoricals - rmse 0.126\n        'learning_rate': 0.01779816671245614,\n        'max_depth': 4,\n        'min_child_weight': 20,\n        'gamma': 0.00044639940791729504,\n        'subsample': 0.6027616353743741,\n        'colsample_bytree': 0.595895749281706,\n        'reg_lambda': 0.2443730806255085,\n        'reg_alpha': 5.8430092166515796e-08,\n        \n        'n_estimators': 5000,\n        'eval_metric': 'rmse',\n        'objective': 'reg:squarederror',\n        'device': 'cuda' if device == 'gpu' else 'cpu',\n        'verbosity': 0,\n        'n_jobs': -1,\n    },\n     'cb': { # Encoded categoricals - rmse 0.125\n        'learning_rate': 0.03585432324860276,\n        'depth': 5,\n        'l2_leaf_reg': 0.0014782377549724024,\n        'random_strength': 1.1584710121736592e-05,\n        'bagging_temperature': 0.15955650079740968,\n        'subsample': 0.5189339778585789,\n        'rsm': 0.95732561692537,\n         \n        'iterations': 5000,\n        'objective': 'RMSE',\n        'eval_metric': 'RMSE',\n        'task_type': 'GPU' if device == 'gpu' else 'CPU',\n        'border_count': 128 if device == 'gpu' else 254,\n        'verbose': 0,\n        'thread_count': -1,\n     },\n}\n# Run Multi-Seed, Multi-Model CV \noof_preds_df, test_preds_df, trained_pipelines = cv_score_predict(\n    X=X_enc,\n    y=y,\n    X_test=X_test_enc,\n    pred_type='regression',\n    process_categorical=False,        \n    models=['lgb', 'xgb', 'cb'],\n    params_dict=params_dict,\n    random_state=[42, 123, 999],     # Repeat CV for stability\n    n_splits=5,\n    early_stopping_rounds=50,\n    return_trained=False,\n    verbose=2,\n)\nprint()\ntest_preds_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T14:51:46.438257Z","iopub.execute_input":"2026-01-15T14:51:46.438694Z","iopub.status.idle":"2026-01-15T14:52:32.018359Z","shell.execute_reply.started":"2026-01-15T14:51:46.43866Z","shell.execute_reply":"2026-01-15T14:52:32.016917Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Train a meta-model","metadata":{}},{"cell_type":"code","source":"# Train a meta-model on OOF space \nfrom sklearn.linear_model import LinearRegression\nmeta_model = LinearRegression()\nmeta_model.fit(oof_preds_df, y)\n\n# For test-time, average across folds to match OOF structure\ntest_meta_features = pd.DataFrame(index=test_preds_df.index)\nfor col in oof_preds_df.columns:\n    # Each OOF column is like 'lgb_seed_42'\n    # Find all test columns that start with this prefix + '_fold_'\n    matching_cols = [c for c in test_preds_df.columns if c.startswith(col + '_fold_')]\n    test_meta_features[col] = test_preds_df[matching_cols].mean(axis=1)\n\n# Final stacked prediction\nsubmit.SalePrice = np.expm1(meta_model.predict(test_meta_features)) # inverse of log1p\nsubmit.to_csv('submission.csv', index=False)\nsubmit.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T14:55:37.92098Z","iopub.execute_input":"2026-01-15T14:55:37.921713Z","iopub.status.idle":"2026-01-15T14:55:37.966847Z","shell.execute_reply.started":"2026-01-15T14:55:37.921676Z","shell.execute_reply":"2026-01-15T14:55:37.965786Z"}},"outputs":[],"execution_count":null}]}