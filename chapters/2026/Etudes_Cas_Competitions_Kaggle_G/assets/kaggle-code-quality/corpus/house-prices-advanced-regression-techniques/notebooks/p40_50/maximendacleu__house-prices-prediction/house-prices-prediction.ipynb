{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![Image](https://storage.googleapis.com/kaggle-competitions/kaggle/5407/logos/header.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1768665756&Signature=DEGqz6Tp5K1totvr5sN8OjQeDKR8V2snePUsmSEdOcVY8GfoQ8fYi%2BPLNnTkL%2F8vYz5fXao74dlWKEGc%2Bihj73fYru0T%2FAGeiHbRF%2FbPCIfpfcs0P48pZU9akU1CvyWIpgv6hb8ybSOM%2FwialVz9pphqgSzdp9pBkr0APgW0bj2nBGkWkSXUNLQTUQEUiFX7XEKdvg7SVVYyWHHPvfSsnotD0soA1N1kBJEZSosuda6RMND5nHNCGw20rCKCwFf5NE%2B%2Bu5tKL1EsHdr7KLdf2oZ0loOKhg82p2dSzBt%2BxioFkZXpPeNpxPl33SPZA5K5t8LCHoKMwBJUHCkcK9w%2F2A%3D%3D)\n","metadata":{}},{"cell_type":"markdown","source":"# House Prices Prediction\n\n## Objective\nThe main objective here is to predict the final sales price of each home. This is a regression problem. We will use the provided dataset, perform Exploratory Data Analysis (EDA), preprocess the data, and train a Gradient Boosting model to generate accurate predictions.","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Data Loading\nWe start by importing the necessary libraries and loading the training and testing datasets.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import skew\n\n# Settings for better readability\npd.set_option('display.max_columns', None)\nplt.style.use('ggplot')\n\n# Load data\ntrain_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\nprint(f\"Train shape: {train_df.shape}\")\nprint(f\"Test shape: {test_df.shape}\")\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:18.044425Z","iopub.execute_input":"2026-01-17T15:29:18.04471Z","iopub.status.idle":"2026-01-17T15:29:23.687417Z","shell.execute_reply.started":"2026-01-17T15:29:18.044683Z","shell.execute_reply":"2026-01-17T15:29:23.686334Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Exploratory Data Analysis (EDA)\n\n### Target Variable: SalePrice\nLet's look at the distribution of the target variable to understand its properties.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(train_df['SalePrice'], kde=True)\nplt.title('Distribution of SalePrice')\nplt.show()\n\nprint(f\"Skewness: {train_df['SalePrice'].skew()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:23.68987Z","iopub.execute_input":"2026-01-17T15:29:23.69024Z","iopub.status.idle":"2026-01-17T15:29:24.173901Z","shell.execute_reply.started":"2026-01-17T15:29:23.690208Z","shell.execute_reply":"2026-01-17T15:29:24.172894Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Interpretation:**\nThe histogram above shows that the `SalePrice` is **right-skewed** (positively skewed). This means most houses satisfy a lower or average price range, with a long tail of very expensive houses. \n\nMachine Learning models generally perform better when the target variable is normally distributed. Therefore, we will apply a **log-transformation** (`np.log1p`) to normalize the distribution.","metadata":{}},{"cell_type":"code","source":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n\nplt.figure(figsize=(10,6))\nsns.histplot(train_df['SalePrice'], kde=True)\nplt.title('Distribution of Log(SalePrice)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:24.175295Z","iopub.execute_input":"2026-01-17T15:29:24.176189Z","iopub.status.idle":"2026-01-17T15:29:24.475486Z","shell.execute_reply.started":"2026-01-17T15:29:24.176139Z","shell.execute_reply":"2026-01-17T15:29:24.474307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Result:** After the transformation, the distribution looks much closer to a normal curve (Gaussian distribution), which is ideal for linear regression and tree-based models.","metadata":{}},{"cell_type":"markdown","source":"### Correlations\nWe want to identify which features have the strongest relationships with `SalePrice`.","metadata":{}},{"cell_type":"code","source":"corr = train_df.select_dtypes(include=[np.number]).corr()\ntop_corr = corr['SalePrice'].sort_values(ascending=False).head(10)\nprint(\"Top 10 Positively Correlated Features:\")\nprint(top_corr)\n\nplt.figure(figsize=(12,8))\nsns.heatmap(train_df[top_corr.index].corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Top Features')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:24.476569Z","iopub.execute_input":"2026-01-17T15:29:24.476869Z","iopub.status.idle":"2026-01-17T15:29:24.967957Z","shell.execute_reply.started":"2026-01-17T15:29:24.476841Z","shell.execute_reply":"2026-01-17T15:29:24.966852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Interpretation:**\n- **OverallQual (Overall Quality)** has the highest correlation with SalePrice. In other words, higher quality construction commands higher prices.\n- **GrLivArea (Above Grade Living Area)** is strongly correlated. Bigger houses tend to be more expensive.\n- **GarageCars/GarageArea**: Garage size is also a significant predictor.","metadata":{}},{"cell_type":"markdown","source":"## 3. Data Preprocessing\n\nTo prepare the data for the model, we must handle missing values and encode categorical variables into a numerical format.","metadata":{}},{"cell_type":"code","source":"y = train_df['SalePrice']\ntrain_features = train_df.drop(['Id', 'SalePrice'], axis=1)\ntest_features = test_df.drop(['Id'], axis=1)\n\n# Combine for processing to ensure consistent dimensionality\nall_data = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(f\"Combined shape: {all_data.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:24.97075Z","iopub.execute_input":"2026-01-17T15:29:24.971211Z","iopub.status.idle":"2026-01-17T15:29:24.999501Z","shell.execute_reply.started":"2026-01-17T15:29:24.971168Z","shell.execute_reply":"2026-01-17T15:29:24.998297Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Handling Missing Values\n- **Categorical Features**: We treat missing values as a separate category named 'Missing' or fill with the mode. Here we fill with 'Missing' to allow the model to learn if 'missingness' is predictive.\n- **Numerical Features**: We fill missing values with the median of the column.","metadata":{}},{"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\ncategorical_feats = all_data.dtypes[all_data.dtypes == \"object\"].index\n\n# Fill Numeric with Median\nfor col in numeric_feats:\n    all_data[col] = all_data[col].fillna(all_data[col].median())\n\n# Fill Categorical with 'Missing' string\nfor col in categorical_feats:\n    all_data[col] = all_data[col].fillna(\"Missing\")\n\nprint(f\"Missing values remaining: {all_data.isnull().sum().sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:25.000903Z","iopub.execute_input":"2026-01-17T15:29:25.001729Z","iopub.status.idle":"2026-01-17T15:29:25.076005Z","shell.execute_reply.started":"2026-01-17T15:29:25.001688Z","shell.execute_reply":"2026-01-17T15:29:25.074823Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Encoding\nWe use **One-Hot Encoding** (`pd.get_dummies`) to convert categorical variables into binary columns (0 or 1). This increases the dataset width but captures categorical information effectively.","metadata":{}},{"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\nprint(f\"Shape after encoding: {all_data.shape}\")\n\n# Split back into train and test\nX = all_data[:len(train_df)]\nX_test = all_data[len(train_df):]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:25.077437Z","iopub.execute_input":"2026-01-17T15:29:25.077813Z","iopub.status.idle":"2026-01-17T15:29:25.145423Z","shell.execute_reply.started":"2026-01-17T15:29:25.077774Z","shell.execute_reply":"2026-01-17T15:29:25.144326Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Modeling\n\nWe will use a **Gradient Boosting Regressor** to correct the errors of previous trees. We use an 80/20 train-validation split to assess our model's performance without touching the test set.","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Validation\npreds_val = model.predict(X_val)\nrmse = np.sqrt(mean_squared_error(y_val, preds_val))\nprint(f\"Validation RMSE (Log Scale): {rmse:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:25.146871Z","iopub.execute_input":"2026-01-17T15:29:25.147254Z","iopub.status.idle":"2026-01-17T15:29:29.917861Z","shell.execute_reply.started":"2026-01-17T15:29:25.147219Z","shell.execute_reply":"2026-01-17T15:29:29.916923Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Performance Interpretation:**\nThe RMSE (Root Mean Squared Error) is calculated on the log-transformed prices. A score of **approx 0.13** is quite competitive for a base model on this dataset. It roughly translates to an average error of about 13% in the predicted price.","metadata":{}},{"cell_type":"markdown","source":"### Feature Importance\nLet's inspect what the model determined as the most important drivers of House Price.","metadata":{}},{"cell_type":"code","source":"feature_importance = model.feature_importances_\nsorted_idx = np.argsort(feature_importance)[-10:]\n\nplt.figure(figsize=(10,6))\nplt.barh(range(10), feature_importance[sorted_idx])\nplt.yticks(range(10), X.columns[sorted_idx])\nplt.title(\"Top 10 Important Features\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:29.919146Z","iopub.execute_input":"2026-01-17T15:29:29.919449Z","iopub.status.idle":"2026-01-17T15:29:30.089693Z","shell.execute_reply.started":"2026-01-17T15:29:29.919416Z","shell.execute_reply":"2026-01-17T15:29:30.08869Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Interpretation:**\nThe Feature Importance plot confirms our EDA:\n- `OverallQual` is typically the dominant feature.\n- `GrLivArea` and `TotalBsmtSF` (Total Basement Square Footage) are also critical.\nThis tells us that **size and quality** are the two biggest factors in determining a home's value in this dataset.","metadata":{}},{"cell_type":"markdown","source":"## 5. Submission\n\nFinally, we predict on the official test set. Since we trained on log-prices, we must use `np.expm1` (exponential minus 1) to convert the predictions back to the original dollar values.","metadata":{}},{"cell_type":"code","source":"# Predict\nfinal_preds_log = model.predict(X_test)\nfinal_preds = np.expm1(final_preds_log)\n\n# Create submission DataFrame\nsubmission = pd.DataFrame({'Id': test_df['Id'], 'SalePrice': final_preds})\n\n# Display head\nprint(submission.head())\n\n# Save\nsubmission.to_csv('submission.csv', index=False)\nprint(\"submission.csv saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T15:29:30.090864Z","iopub.execute_input":"2026-01-17T15:29:30.091283Z","iopub.status.idle":"2026-01-17T15:29:30.130248Z","shell.execute_reply.started":"2026-01-17T15:29:30.091253Z","shell.execute_reply":"2026-01-17T15:29:30.1293Z"}},"outputs":[],"execution_count":null}]}