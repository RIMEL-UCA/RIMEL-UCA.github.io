{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"},{"sourceId":14061329,"sourceType":"datasetVersion","datasetId":8949820}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#=========================================================\n#CELL 1 — IMPORTS\n#=========================================================\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom kaggle_evaluation.default_inference_server import DefaultInferenceServer\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:43:50.452875Z","iopub.execute_input":"2025-12-11T11:43:50.453154Z","iopub.status.idle":"2025-12-11T11:43:50.45848Z","shell.execute_reply.started":"2025-12-11T11:43:50.453132Z","shell.execute_reply":"2025-12-11T11:43:50.45745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/dataset/train_merged.csv\")\n\nTARGET = \"market_forward_excess_returns\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:43:50.460256Z","iopub.execute_input":"2025-12-11T11:43:50.460569Z","iopub.status.idle":"2025-12-11T11:43:50.866552Z","shell.execute_reply.started":"2025-12-11T11:43:50.460547Z","shell.execute_reply":"2025-12-11T11:43:50.865583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================\n# LOAD HULL DATA\n# ================================================\ntrain = pd.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/hull-tactical-market-prediction/test.csv\")\n\n# ================================================\n# LOAD DAILY BREADTH FEATURES (FROM YOUR DATASET)\n# ================================================\ndaily = pd.read_csv(\"/kaggle/input/dataset/sp500_breadth_features.csv\")\n\ndaily[\"date\"] = pd.to_datetime(daily[\"date\"])\ndaily = daily.sort_values(\"date\").reset_index(drop=True)\ndaily[\"date_id\"] = daily.index\n\nprint(\"Breadth file columns:\", daily.columns.tolist())\nprint(\"Min daily date_id:\", daily.date_id.min())\nprint(\"Max daily date_id:\", daily.date_id.max())\n\n\n# ================================================\n# MERGE USING date_id\n# ================================================\ntrain2 = train.merge(daily, on=\"date_id\", how=\"left\")\ntest2  = test.merge(daily, on=\"date_id\", how=\"left\")\n\ntrain2 = train2.fillna(method=\"ffill\").fillna(0)\ntest2  = test2.fillna(method=\"ffill\").fillna(0)\n\n# ================================================\n# SAVE TO WORKING FOLDER (ALLOWED)\n# ================================================\ntrain2.to_csv(\"/kaggle/working/train_merged.csv\", index=False)\ntest2.to_csv(\"/kaggle/working/test_merged.csv\", index=False)\n\nprint(\"Merged train shape:\", train2.shape)\nprint(\"Merged test shape:\", test2.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:44:00.659668Z","iopub.execute_input":"2025-12-11T11:44:00.659988Z","iopub.status.idle":"2025-12-11T11:44:02.159684Z","shell.execute_reply.started":"2025-12-11T11:44:00.659964Z","shell.execute_reply":"2025-12-11T11:44:02.157934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# FEATURE ENGINEERING ON MERGED TRAIN\n# ============================================================\n\ndf = train2.copy()\n\n# ---------------------------------------\n# Lagged breadth indicators\n# ---------------------------------------\nlag_cols = [\n    \"breadth_up\",\n    \"avg_ret_1d\",\n    \"dispersion_1d\",\n    \"avg_vol_20d\"\n]\n\nfor c in lag_cols:\n    if c not in df.columns:\n        raise ValueError(f\"Missing required feature column: {c}\")\n    df[c + \"_lag1\"] = df[c].shift(1)\n    df[c + \"_lag2\"] = df[c].shift(2)\n\n# ---------------------------------------\n# Momentum of dispersion\n# ---------------------------------------\ndf[\"dispersion_mom\"] = df[\"dispersion_1d\"].diff()\n\n# ---------------------------------------\n# Volatility regime detection\n# ---------------------------------------\nvol_ma = df[\"avg_vol_20d\"].rolling(60).mean()\ndf[\"regime\"] = (df[\"avg_vol_20d\"] > vol_ma).astype(int)\n\n# ---------------------------------------\n# Fill lag-related missing values\n# ---------------------------------------\ndf = df.fillna(0)\n\n# ---------------------------------------\n# Remove columns not used as features\n# ---------------------------------------\nexclude = [\n    \"date_id\",\n    \"date\",\n    \"forward_returns\",\n    \"risk_free_rate\",\n    \"market_forward_excess_returns\",  # target\n]\n\nfeature_cols = [c for c in df.columns if c not in exclude]\n\nprint(\"Number of features:\", len(feature_cols))\nprint(\"Sample features:\", feature_cols[:15])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:44:05.303152Z","iopub.execute_input":"2025-12-11T11:44:05.303512Z","iopub.status.idle":"2025-12-11T11:44:05.341737Z","shell.execute_reply.started":"2025-12-11T11:44:05.303486Z","shell.execute_reply":"2025-12-11T11:44:05.34069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tscv = TimeSeriesSplit(n_splits=8)\n\nmodels_lgb = []\nmodels_xgb = []\nmodels_cat = []\n\nfor fold, (tr, va) in enumerate(tscv.split(df)):\n    print(f\"Training Fold {fold+1}/8\")\n\n    X_tr, y_tr = df.iloc[tr][feature_cols], df.iloc[tr][TARGET]\n    X_va, y_va = df.iloc[va][feature_cols], df.iloc[va][TARGET]\n\n    # LightGBM\n    lgb_params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.01,\n        \"num_leaves\": 48,\n        \"feature_fraction\": 0.75,\n        \"bagging_fraction\": 0.75,\n        \"bagging_freq\": 5,\n        \"min_data_in_leaf\": 25,\n        \"verbose\": -1,\n    }\n\n    model_lgb = lgb.train(\n        lgb_params,\n        lgb.Dataset(X_tr, y_tr),\n        valid_sets=[lgb.Dataset(X_va, y_va)],\n        num_boost_round=3000,\n        callbacks=[lgb.early_stopping(200, verbose=False)]\n    )\n    models_lgb.append(model_lgb)\n\n    # XGBoost\n    model_xgb = XGBRegressor(\n        n_estimators=2000,\n        learning_rate=0.01,\n        max_depth=6,\n        subsample=0.8,\n        colsample_bytree=0.75,\n        objective=\"reg:squarederror\",\n        eval_metric=\"rmse\",\n        tree_method=\"hist\"\n    )\n    model_xgb.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n    models_xgb.append(model_xgb)\n\n    # CatBoost\n    model_cat = CatBoostRegressor(\n        iterations=2000,\n        learning_rate=0.01,\n        depth=6,\n        loss_function=\"RMSE\",\n        verbose=False\n    )\n    model_cat.fit(X_tr, y_tr, eval_set=(X_va, y_va), verbose=False)\n    models_cat.append(model_cat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:44:09.416273Z","iopub.execute_input":"2025-12-11T11:44:09.416655Z","iopub.status.idle":"2025-12-11T11:50:40.414238Z","shell.execute_reply.started":"2025-12-11T11:44:09.416623Z","shell.execute_reply":"2025-12-11T11:50:40.413129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ensemble_predict(df):\n    X = df[feature_cols].fillna(0)\n    pred_lgb = np.mean([m.predict(X) for m in models_lgb], axis=0)\n    pred_xgb = np.mean([m.predict(X) for m in models_xgb], axis=0)\n    return 0.6 * pred_lgb + 0.4 * pred_xgb\n\ndef ensemble_pred(X):\n    p_lgb = np.mean([m.predict(X) for m in models_lgb], axis=0)\n    p_xgb = np.mean([m.predict(X) for m in models_xgb], axis=0)\n    p_cat = np.mean([m.predict(X) for m in models_cat], axis=0)\n    return 0.5*p_lgb + 0.3*p_xgb + 0.2*p_cat\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:51:54.907954Z","iopub.execute_input":"2025-12-11T11:51:54.908285Z","iopub.status.idle":"2025-12-11T11:51:54.915392Z","shell.execute_reply.started":"2025-12-11T11:51:54.908259Z","shell.execute_reply":"2025-12-11T11:51:54.914429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nlag_cols = [\n    \"breadth_up\",\n    \"avg_ret_1d\",\n    \"dispersion_1d\",\n    \"avg_vol_20d\"\n]\n\n\nBASE = 0.10\nprev_alloc = BASE\n\ndef adaptive_alloc(raw_signal, regime):\n    global prev_alloc\n\n    # Noise clamp\n    raw_signal = np.clip(raw_signal, -0.02, 0.02)\n\n    # Stronger multiplier in calm regime\n    mult = 5.0 if regime == 0 else 3.0\n\n    adj = raw_signal * mult\n    alloc = BASE + adj\n\n    # Regime-based volatility dampening\n    if regime == 1:  # high volatility regime\n        alloc *= 0.7\n\n    # Smoothing\n    alpha = 0.15 if regime == 0 else 0.25  \n    alloc = alpha * alloc + (1 - alpha) * prev_alloc\n\n    # Bounds\n    alloc = np.clip(alloc, 0, 2)\n    prev_alloc = alloc\n\n    return alloc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:51:57.107967Z","iopub.execute_input":"2025-12-11T11:51:57.108299Z","iopub.status.idle":"2025-12-11T11:51:57.115306Z","shell.execute_reply.started":"2025-12-11T11:51:57.108274Z","shell.execute_reply":"2025-12-11T11:51:57.114432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def approx_sharpe(r):\n    m = np.mean(r)\n    s = np.std(r)\n    return 0 if s == 0 else m/s*np.sqrt(252)\n\nN = 300\ndf_val = df.iloc[-N:].copy()\n\nX_val = df_val[feature_cols].fillna(0)\npred_raw = ensemble_pred(X_val)\n\nallocs = []\nprev_alloc = BASE\n\nfor raw, reg in zip(pred_raw, df_val[\"regime\"]):\n    allocs.append(adaptive_alloc(raw, reg))\n\nallocs = np.array(allocs)\n\n# Strategy return approximation\nstrategy_returns = (\n    df_val[\"risk_free_rate\"] * (1 - allocs)\n    + allocs * df_val[\"forward_returns\"]\n)\n\nscore_local = approx_sharpe(strategy_returns)\n\nprint(\"=======================================\")\nprint(\" LOCAL SHARPE SCORE:\", score_local)\nprint(\"=======================================\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:51:58.149038Z","iopub.execute_input":"2025-12-11T11:51:58.149318Z","iopub.status.idle":"2025-12-11T11:51:58.419149Z","shell.execute_reply.started":"2025-12-11T11:51:58.1493Z","shell.execute_reply":"2025-12-11T11:51:58.418413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(test_df: pl.DataFrame) -> pl.DataFrame:\n    global last_alloc\n\n    pdf = test_df.to_pandas()\n\n    pdf2 = pdf.merge(daily, on=\"date_id\", how=\"left\").ffill().fillna(0)\n\n    # feature engineering\n    for c in lag_cols:\n        pdf2[c+\"_lag1\"] = pdf2[c].shift(1)\n        pdf2[c+\"_lag2\"] = pdf2[c].shift(2)\n\n    pdf2[\"dispersion_mom\"] = pdf2[\"dispersion_1d\"].diff()\n    vol_ma = pdf2[\"avg_vol_20d\"].rolling(60).mean()\n    pdf2[\"regime\"] = (pdf2[\"avg_vol_20d\"] > vol_ma).astype(int)\n\n    pdf2 = pdf2.fillna(0)\n\n    # ensure missing columns are added\n    for col in feature_cols:\n        if col not in pdf2:\n            pdf2[col] = 0\n    pdf2 = pdf2[feature_cols]\n\n    # FIXED: correct function name\n    raw_preds = ensemble_pred(pdf2)\n\n    # regime is NOT inside pdf2 anymore → must fetch from original engineered dataframe\n    regimes = pdf2[\"regime\"].values if \"regime\" in pdf2 else np.zeros(len(pdf2))\n\n    allocs = [adaptive_alloc(p, r) for p, r in zip(raw_preds, regimes)]\n\n    return test_df.with_columns(pl.Series(\"prediction\", allocs))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:52:02.268213Z","iopub.execute_input":"2025-12-11T11:52:02.268535Z","iopub.status.idle":"2025-12-11T11:52:02.276563Z","shell.execute_reply.started":"2025-12-11T11:52:02.268513Z","shell.execute_reply":"2025-12-11T11:52:02.275594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = DefaultInferenceServer(predict)\n\nimport os\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\"/kaggle/input/hull-tactical-market-prediction/\",)\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T11:52:06.108277Z","iopub.execute_input":"2025-12-11T11:52:06.109285Z","iopub.status.idle":"2025-12-11T11:52:08.007284Z","shell.execute_reply.started":"2025-12-11T11:52:06.109252Z","shell.execute_reply":"2025-12-11T11:52:08.005915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}