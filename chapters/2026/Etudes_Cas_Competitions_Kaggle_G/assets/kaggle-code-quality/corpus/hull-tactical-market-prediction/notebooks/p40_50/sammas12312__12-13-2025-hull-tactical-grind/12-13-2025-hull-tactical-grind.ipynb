{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14861981,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgbm\nimport kaggle_evaluation.default_inference_server\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\npd.set_option(\"display.max_columns\", None)\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-11T04:00:19.604379Z","iopub.execute_input":"2026-01-11T04:00:19.604748Z","iopub.status.idle":"2026-01-11T04:00:27.044949Z","shell.execute_reply.started":"2026-01-11T04:00:19.604708Z","shell.execute_reply":"2026-01-11T04:00:27.043951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv')\ndf_test = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv')\ndf_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T04:00:27.046749Z","iopub.execute_input":"2026-01-11T04:00:27.047477Z","iopub.status.idle":"2026-01-11T04:00:27.358525Z","shell.execute_reply.started":"2026-01-11T04:00:27.047444Z","shell.execute_reply":"2026-01-11T04:00:27.357768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Simply Throwing in LGBM model wasn't fit for high or low valitility setting, as we just simply listed out the allocation base and leveraged coefficient. Some improvements made in valitatility-normalized allocation\n","metadata":{}},{"cell_type":"code","source":"\n\n# ============================================================\n# Feature Engineering, kinda random tho\n# ============================================================\n\n# def func_rolling(df):\n#     \"\"\"\n#     Rolling features with relative normalization\n#     \"\"\"\n#     # Relative deviation instead of raw rolling mean\n#     roll_mean_20 = df['V7'].rolling(window=20, min_periods=5).mean()\n#     df['V7_rel_20'] = df['V7'] / (roll_mean_20 + 1e-6)\n\n#     # Rolling volatility proxy (for regime detection)\n#     df['V7_vol_60'] = df['V7'].rolling(window=60, min_periods=10).std()\n\n#     return df\n\ndef func_rolling(df): # fixed version\n    # Relative deviation with safe fallback\n    roll_mean_20 = df['V7'].rolling(window=20, min_periods=1).mean()\n    df['V7_rel_20'] = df['V7'] / (roll_mean_20 + 1e-6)\n\n    # Volatility proxy with safe fallback\n    roll_std_60 = df['V7'].rolling(window=60, min_periods=1).std()\n    df['V7_vol_60'] = roll_std_60.fillna(0.15)  # assume normal vol if unavailable\n\n    return df\n    \n#Training phase\n\nall_cols = df_train.columns\nnon_use_cols = [\n    'date_id',\n    'forward_returns',\n    'risk_free_rate',\n    'market_forward_excess_returns'\n]\n\nfeature_cols = [c for c in all_cols if c not in non_use_cols]\n\n# Apply feature engineering to training data\ndf_train = func_rolling(df_train)\n\n# Update feature list (important!)\nfeature_cols = [c for c in feature_cols if c in df_train.columns]\n\nX = df_train[feature_cols]\ny = df_train['market_forward_excess_returns']\n\nmodel_lgbm = lgbm.LGBMRegressor(\n    objective=\"regression\",\n    random_state=42,\n    n_estimators=100,\n    learning_rate=0.05,\n    num_leaves=31,\n    verbose=-1\n)\n\nmodel_lgbm.fit(X, y)\n\n\n# def predict(test_df):\n#     test_df = test_df.to_pandas()\n#     test_df = func_rolling(test_df)\n\n#     X_test = test_df[feature_cols]\n\n#     # ---- Prediction ----\n#     pred = model_lgbm.predict(X_test)[0]\n\n#     # ---- Prediction scaling (prevents spikes) ----\n#     pred_scaled = np.tanh(pred / 0.01)\n\n#     # ---- Volatility targeting ----\n#     # Use forward returns if available, else fallback to V7 volatility\n#     if 'forward_returns' in test_df.columns:\n#         realized_vol = (\n#             test_df['forward_returns']\n#             .rolling(60, min_periods=10)\n#             .std()\n#             .iloc[-1]\n#         )\n#     else:\n#         realized_vol = test_df['V7_vol_60'].iloc[-1]\n\n#     realized_vol = max(realized_vol, 1e-3)\n#     target_vol = 0.12\n#     vol_adj = target_vol / realized_vol\n\n#     # ---- Base allocation ----\n#     allocation = vol_adj * (0.7 + 5.0 * pred_scaled)\n\n#     # ---- High volatility regime throttle ----\n#     vol_median = test_df['V7_vol_60'].rolling(120, min_periods=20).median().iloc[-1]\n#     high_vol_regime = realized_vol > vol_median\n\n#     if high_vol_regime:\n#         allocation *= 0.7  # throttle exposure in high-vol regimes\n\n#     # ---- Final safety clip ----\n#     allocation = np.clip(allocation, 0.0, 2.0)\n\n#     print(float(allocation))\n#     return float(allocation)\n\n\n# fixed one\n# Voltaility-adjusted based on Market's Action\n\ndef predict(test_df):\n    test_df = test_df.to_pandas()\n    test_df = func_rolling(test_df)\n\n    X_test = test_df[feature_cols]\n\n\n    #Prediction\n    pred = model_lgbm.predict(X_test)[0]\n\n\n    pred_scaled = np.tanh(pred / 0.01)\n\n    realized_vol = test_df['V7_vol_60'].iloc[-1]\n\n    if not np.isfinite(realized_vol) or realized_vol <= 0:\n        realized_vol = 0.15  # hard fallback\n\n    target_vol = 0.12\n    vol_adj = target_vol / realized_vol\n\n    allocation = vol_adj * (0.7 + 5.0 * pred_scaled)\n\n    vol_median = test_df['V7_vol_60'].median()\n\n    if np.isfinite(vol_median) and realized_vol > vol_median:\n        allocation *= 0.7\n\n    allocation = np.nan_to_num(allocation, nan=1.0, posinf=2.0, neginf=0.0)\n    allocation = np.clip(allocation, 0.0, 2.0)\n\n    return float(allocation)\n\n\n# ============================================================\n# Kaggle Inference Server\n# ============================================================\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\"/kaggle/input/hull-tactical-market-prediction/\",)\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T04:00:27.359301Z","iopub.execute_input":"2026-01-11T04:00:27.359507Z","iopub.status.idle":"2026-01-11T04:00:28.788646Z","shell.execute_reply.started":"2026-01-11T04:00:27.359491Z","shell.execute_reply":"2026-01-11T04:00:28.78783Z"}},"outputs":[],"execution_count":null}]}