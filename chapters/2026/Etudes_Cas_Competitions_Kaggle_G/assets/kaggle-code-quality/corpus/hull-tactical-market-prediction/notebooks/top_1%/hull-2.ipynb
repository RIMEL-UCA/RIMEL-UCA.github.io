{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14861981,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hull Tactical - Market Prediction: Exploratory Data Analysis \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-12-12T13:40:24.819814Z","iopub.execute_input":"2025-12-12T13:40:24.820047Z","iopub.status.idle":"2025-12-12T13:40:27.126912Z","shell.execute_reply.started":"2025-12-12T13:40:24.820009Z","shell.execute_reply":"2025-12-12T13:40:27.125669Z"}}},{"cell_type":"markdown","source":"# 0. Load Packages and Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import jarque_bera\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', 200)\npd.set_option('display.precision', 6)\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:30.535134Z","iopub.execute_input":"2025-12-12T14:48:30.535773Z","iopub.status.idle":"2025-12-12T14:48:30.543286Z","shell.execute_reply.started":"2025-12-12T14:48:30.535738Z","shell.execute_reply":"2025-12-12T14:48:30.542347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = r\"/kaggle/input/hull-tactical-market-prediction/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:30.544907Z","iopub.execute_input":"2025-12-12T14:48:30.545152Z","iopub.status.idle":"2025-12-12T14:48:30.558696Z","shell.execute_reply.started":"2025-12-12T14:48:30.545132Z","shell.execute_reply":"2025-12-12T14:48:30.557765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\ntrain_df = pd.read_csv(f'{file_path}/train.csv')\ntest_df = pd.read_csv(f'{file_path}/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:30.559554Z","iopub.execute_input":"2025-12-12T14:48:30.559795Z","iopub.status.idle":"2025-12-12T14:48:30.725211Z","shell.execute_reply.started":"2025-12-12T14:48:30.559776Z","shell.execute_reply":"2025-12-12T14:48:30.724352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import yfinance as yf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:30.72612Z","iopub.execute_input":"2025-12-12T14:48:30.726441Z","iopub.status.idle":"2025-12-12T14:48:30.730665Z","shell.execute_reply.started":"2025-12-12T14:48:30.726413Z","shell.execute_reply":"2025-12-12T14:48:30.729743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sp500 = yf.Ticker('^GSPC')\ndf_sp500 = sp500.history(period=\"max\", start='1990-01-01')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:30.732915Z","iopub.execute_input":"2025-12-12T14:48:30.733207Z","iopub.status.idle":"2025-12-12T14:48:30.97929Z","shell.execute_reply.started":"2025-12-12T14:48:30.733185Z","shell.execute_reply":"2025-12-12T14:48:30.978459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sp500.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:30.980131Z","iopub.execute_input":"2025-12-12T14:48:30.980395Z","iopub.status.idle":"2025-12-12T14:48:30.985865Z","shell.execute_reply.started":"2025-12-12T14:48:30.980375Z","shell.execute_reply":"2025-12-12T14:48:30.985017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sp500['Close'].plot()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:30.986785Z","iopub.execute_input":"2025-12-12T14:48:30.987113Z","iopub.status.idle":"2025-12-12T14:48:31.567256Z","shell.execute_reply.started":"2025-12-12T14:48:30.987085Z","shell.execute_reply":"2025-12-12T14:48:31.566435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reconstruct_price(pct_changes, initial_price):\n    prices = [initial_price]\n    \n    for pct_change in pct_changes:\n        next_price = prices[-1] * (1 + pct_change)\n        prices.append(next_price)\n    \n    return prices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.568267Z","iopub.execute_input":"2025-12-12T14:48:31.569253Z","iopub.status.idle":"2025-12-12T14:48:31.573576Z","shell.execute_reply.started":"2025-12-12T14:48:31.569226Z","shell.execute_reply":"2025-12-12T14:48:31.572806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prices = reconstruct_price(train_df['forward_returns'].values, 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.5744Z","iopub.execute_input":"2025-12-12T14:48:31.574673Z","iopub.status.idle":"2025-12-12T14:48:31.591218Z","shell.execute_reply.started":"2025-12-12T14:48:31.574648Z","shell.execute_reply":"2025-12-12T14:48:31.590359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['price'] = prices[:-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.59223Z","iopub.execute_input":"2025-12-12T14:48:31.59274Z","iopub.status.idle":"2025-12-12T14:48:31.607699Z","shell.execute_reply.started":"2025-12-12T14:48:31.592713Z","shell.execute_reply":"2025-12-12T14:48:31.606964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['SP500'] = df_sp500['Close'].values[:len(train_df)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.608747Z","iopub.execute_input":"2025-12-12T14:48:31.609524Z","iopub.status.idle":"2025-12-12T14:48:31.623739Z","shell.execute_reply.started":"2025-12-12T14:48:31.6095Z","shell.execute_reply":"2025-12-12T14:48:31.622839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.624693Z","iopub.execute_input":"2025-12-12T14:48:31.624969Z","iopub.status.idle":"2025-12-12T14:48:31.63916Z","shell.execute_reply.started":"2025-12-12T14:48:31.624943Z","shell.execute_reply":"2025-12-12T14:48:31.638331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Data Overview and Statistics","metadata":{}},{"cell_type":"code","source":"# Basic information\nprint(f\"\\nDataset Shape: {train_df.shape}\")\nprint(f\"Number of rows: {train_df.shape[0]} | Number of columns: {train_df.shape[1]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.640145Z","iopub.execute_input":"2025-12-12T14:48:31.640458Z","iopub.status.idle":"2025-12-12T14:48:31.654699Z","shell.execute_reply.started":"2025-12-12T14:48:31.640437Z","shell.execute_reply":"2025-12-12T14:48:31.653815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.658292Z","iopub.execute_input":"2025-12-12T14:48:31.659027Z","iopub.status.idle":"2025-12-12T14:48:31.71905Z","shell.execute_reply.started":"2025-12-12T14:48:31.658997Z","shell.execute_reply":"2025-12-12T14:48:31.718221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data types\nprint(train_df.dtypes.value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.720019Z","iopub.execute_input":"2025-12-12T14:48:31.720273Z","iopub.status.idle":"2025-12-12T14:48:31.726053Z","shell.execute_reply.started":"2025-12-12T14:48:31.720254Z","shell.execute_reply":"2025-12-12T14:48:31.725185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Memory usage\nprint(f\"Total memory usage: {train_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.727018Z","iopub.execute_input":"2025-12-12T14:48:31.727487Z","iopub.status.idle":"2025-12-12T14:48:31.746461Z","shell.execute_reply.started":"2025-12-12T14:48:31.727465Z","shell.execute_reply":"2025-12-12T14:48:31.745572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic statistics\ntrain_df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.747392Z","iopub.execute_input":"2025-12-12T14:48:31.747649Z","iopub.status.idle":"2025-12-12T14:48:31.977339Z","shell.execute_reply.started":"2025-12-12T14:48:31.747623Z","shell.execute_reply":"2025-12-12T14:48:31.976457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# date_id - An identifier for a single trading day.\n# M* - Market Dynamics/Technical features.\n# E* - Macro Economic features.\n# I* - Interest Rate features.\n# P* - Price/Valuation features.\n# V* - Volatility features.\n# S* - Sentiment features.\n# MOM* - Momentum features.\n# D* - Dummy/Binary features.\n# forward_returns - The returns from buying the S&P 500 and selling it a day later. Train set only.\n# risk_free_rate - The federal funds rate. Train set only.\n# market_forward_excess_returns - Forward returns relative to expectations. Computed by subtracting the rolling five-year mean forward returns and winsorizing the result using a median absolute deviation (MAD) with a criterion of 4. Train set only.\n    \n\nfeature_categories = {\n    'Identifier': ['date_id'],\n    'Momentum_Features': [col for col in train_df.columns if col.startswith('MOM')],  # Momentum features\n    'Market_Features': [col for col in train_df.columns if (col.startswith('M') & ~col.startswith('MOM'))],  # Market Dynamics/Technical features\n    'Economic_Features': [col for col in train_df.columns if col.startswith('E')],  # Macro Economic features\n    'Interest_Features': [col for col in train_df.columns if col.startswith('I')],  # Interest Rate features\n    'Price_Features': [col for col in train_df.columns if col.startswith('P')],  # Price/Valuation features\n    'Volatility_Features': [col for col in train_df.columns if col.startswith('V')],  # Volatility features\n    'Sentiment_Features': [col for col in train_df.columns if col.startswith('S')],  # Sentiment features\n    'Dummy_Features': [col for col in train_df.columns if col.startswith('D')],  # Dummy/Binary features\n    'Target_Variables': ['forward_returns', 'risk_free_rate', 'market_forward_excess_returns'],\n    'Reference_Data': ['price', 'SP500']\n}\n\ntotal_feats = 0\nfor category, features in feature_categories.items():\n    available = [f for f in features if f in train_df.columns]\n    total_feats = total_feats + len(available)\n    print(f\"{category:25s}: {len(available):3d} features\")\n\nprint(f\"Total number of features: {total_feats}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.978794Z","iopub.execute_input":"2025-12-12T14:48:31.979128Z","iopub.status.idle":"2025-12-12T14:48:31.987933Z","shell.execute_reply.started":"2025-12-12T14:48:31.979099Z","shell.execute_reply":"2025-12-12T14:48:31.986978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.DataFrame(data = {'category': feature_categories.keys(), 'feats': feature_categories.values()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:31.988897Z","iopub.execute_input":"2025-12-12T14:48:31.98915Z","iopub.status.idle":"2025-12-12T14:48:32.013146Z","shell.execute_reply.started":"2025-12-12T14:48:31.989131Z","shell.execute_reply":"2025-12-12T14:48:32.012286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:32.014481Z","iopub.execute_input":"2025-12-12T14:48:32.014766Z","iopub.status.idle":"2025-12-12T14:48:32.083118Z","shell.execute_reply.started":"2025-12-12T14:48:32.014739Z","shell.execute_reply":"2025-12-12T14:48:32.082076Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Target Variable Analysis","metadata":{}},{"cell_type":"code","source":"target = 'forward_returns'\ntrain_df[target].describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:32.084104Z","iopub.execute_input":"2025-12-12T14:48:32.085066Z","iopub.status.idle":"2025-12-12T14:48:32.094109Z","shell.execute_reply.started":"2025-12-12T14:48:32.085039Z","shell.execute_reply":"2025-12-12T14:48:32.093269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Skewness:    {train_df[target].skew():.4f}\")\nprint(f\"Kurtosis:    {train_df[target].kurtosis():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:32.095056Z","iopub.execute_input":"2025-12-12T14:48:32.095381Z","iopub.status.idle":"2025-12-12T14:48:32.107848Z","shell.execute_reply.started":"2025-12-12T14:48:32.095355Z","shell.execute_reply":"2025-12-12T14:48:32.106948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Percentile analysis\npercentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\nfor p in percentiles:\n    print(f\"{p:2d}th percentile: {train_df[target].quantile(p/100):.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:32.108856Z","iopub.execute_input":"2025-12-12T14:48:32.10907Z","iopub.status.idle":"2025-12-12T14:48:32.131049Z","shell.execute_reply.started":"2025-12-12T14:48:32.109053Z","shell.execute_reply":"2025-12-12T14:48:32.130061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#Positive (True) vs Negative (False) returns\n\n(train_df[target] > 0).value_counts()/len(train_df[target])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:32.131982Z","iopub.execute_input":"2025-12-12T14:48:32.132256Z","iopub.status.idle":"2025-12-12T14:48:32.145533Z","shell.execute_reply.started":"2025-12-12T14:48:32.132236Z","shell.execute_reply":"2025-12-12T14:48:32.144654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Risk metrics\nprint(f\"   Daily Volatility:       {train_df[target].std():.6f}\")\nprint(f\"   Annualized Volatility:  {train_df[target].std() * np.sqrt(252):.4f}\")\nprint(f\"   Value at Risk (95%):    {np.percentile(train_df[target], 5):.6f}\")\nprint(f\"   Value at Risk (99%):    {np.percentile(train_df[target], 1):.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:32.146393Z","iopub.execute_input":"2025-12-12T14:48:32.146643Z","iopub.status.idle":"2025-12-12T14:48:32.162386Z","shell.execute_reply.started":"2025-12-12T14:48:32.146622Z","shell.execute_reply":"2025-12-12T14:48:32.161358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normality tests\nstat_jb, p_jb = jarque_bera(train_df[target])\nprint(f\"\\n Normality Test (Jarque-Bera):\")\nprint(f\"   Test Statistic: {stat_jb:.4f}\")\nprint(f\"   P-value:        {p_jb:.6f}\")\nprint(f\"   Normal?:        {'No (returns are NOT normally distributed)' if p_jb < 0.05 else 'Yes'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:32.163444Z","iopub.execute_input":"2025-12-12T14:48:32.163739Z","iopub.status.idle":"2025-12-12T14:48:32.177661Z","shell.execute_reply.started":"2025-12-12T14:48:32.163714Z","shell.execute_reply":"2025-12-12T14:48:32.176669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Detailed returns analysis\ndef analyze_returns(df):\n    \"\"\"Detailed analysis of forward returns\"\"\"\n    returns = df['forward_returns'].dropna()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"DETAILED RETURNS ANALYSIS\")\n    print(\"=\"*80)\n    \n    # Distribution statistics\n    print(f\"\\nPositive Returns: {(returns > 0).sum()} ({(returns > 0).sum()/len(returns)*100:.2f}%)\")\n    print(f\"Negative Returns: {(returns < 0).sum()} ({(returns < 0).sum()/len(returns)*100:.2f}%)\")\n    print(f\"Zero Returns: {(returns == 0).sum()} ({(returns == 0).sum()/len(returns)*100:.2f}%)\")\n    \n    # Percentiles\n    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n    print(\"\\nPercentiles:\")\n    for p in percentiles:\n        print(f\"{p}th: {np.percentile(returns, p):.6f}\")\n    \n    # Volatility\n    print(f\"\\nAnnualized Volatility (assuming daily data): {returns.std() * np.sqrt(252):.4f}\")\n    \n    # Sharpe Ratio (assuming risk-free rate column)\n    if 'risk_free_rate' in df.columns:\n        excess_returns = returns - df['risk_free_rate'].dropna().mean()\n        sharpe = excess_returns.mean() / excess_returns.std() * np.sqrt(252)\n        print(f\"Sharpe Ratio: {sharpe:.4f}\")\n\nanalyze_returns(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:32.178493Z","iopub.execute_input":"2025-12-12T14:48:32.178721Z","iopub.status.idle":"2025-12-12T14:48:32.201757Z","shell.execute_reply.started":"2025-12-12T14:48:32.178702Z","shell.execute_reply":"2025-12-12T14:48:32.200741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Histogram\naxes[0, 0].hist(train_df[target].dropna(), bins=100, edgecolor='black', alpha=0.7)\naxes[0, 0].axvline(train_df[target].mean(), color='red', linestyle='--', label=f'Mean: {train_df[target].mean():.6f}')\naxes[0, 0].axvline(train_df[target].median(), color='green', linestyle='--', label=f'Median: {train_df[target].median():.6f}')\naxes[0, 0].set_title('Forward Returns Distribution')\naxes[0, 0].set_xlabel('Returns')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].legend()\n\n# Box plot\naxes[0, 1].boxplot(train_df[target].dropna())\naxes[0, 1].set_title('Forward Returns - Box Plot')\naxes[0, 1].set_ylabel('Returns')\n\n# Q-Q plot\nstats.probplot(train_df[target].dropna(), dist=\"norm\", plot=axes[0, 2])\naxes[0, 2].set_title('Q-Q Plot (Normality Test)')\n\n# Time series\naxes[1, 0].plot(train_df['date_id'], train_df[target], linewidth=0.8, alpha=0.7)\naxes[1, 0].axhline(0, color='red', linestyle='--', alpha=0.5)\naxes[1, 0].set_title('Forward Returns Over Time')\naxes[1, 0].set_xlabel('Date ID')\naxes[1, 0].set_ylabel('Returns')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Rolling statistics\nwindow = 50\naxes[1, 1].plot(train_df['date_id'], train_df[target], label='Actual', alpha=0.3)\naxes[1, 1].plot(train_df['date_id'], train_df[target].rolling(window=window).mean(), \n                label=f'{window}-period MA', linewidth=2)\naxes[1, 1].plot(train_df['date_id'], train_df[target].rolling(window=window).std(), \n                label=f'{window}-period Std', linewidth=2)\naxes[1, 1].set_title('Forward Returns with Rolling Statistics')\naxes[1, 1].set_xlabel('Date ID')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Cumulative returns\naxes[1, 2].plot(train_df['date_id'], (1 + train_df[target]).cumprod() - 1)\naxes[1, 2].set_title('Cumulative Returns')\naxes[1, 2].set_xlabel('Date ID')\naxes[1, 2].set_ylabel('Cumulative Return')\naxes[1, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:32.202953Z","iopub.execute_input":"2025-12-12T14:48:32.203693Z","iopub.status.idle":"2025-12-12T14:48:33.908262Z","shell.execute_reply.started":"2025-12-12T14:48:32.203664Z","shell.execute_reply":"2025-12-12T14:48:33.907234Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Missing Data Analysis","metadata":{}},{"cell_type":"code","source":"# Analyze missing data patterns by feature group\nfor category, features in feature_categories.items():\n    if category not in ['Identifier', 'Target_Variables']:\n        available = [f for f in features if f in train_df.columns]\n        if available:\n            missing_pct = (train_df[available].isnull().sum().sum() / (len(train_df) * len(available))) * 100\n            print(f\"{category:25s}: {missing_pct:6.2f}% missing\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:33.909211Z","iopub.execute_input":"2025-12-12T14:48:33.909546Z","iopub.status.idle":"2025-12-12T14:48:33.927069Z","shell.execute_reply.started":"2025-12-12T14:48:33.909526Z","shell.execute_reply":"2025-12-12T14:48:33.926222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_stats = pd.DataFrame({\n    'Missing_Count': train_df.isnull().sum(),\n    'Missing_Percentage': (train_df.isnull().sum() / len(train_df)) * 100,\n    'Present_Count': train_df.notna().sum(),\n    'First_Valid_Index': train_df.apply(lambda x: x.first_valid_index()),\n    'Last_Valid_Index': train_df.apply(lambda x: x.last_valid_index())\n}).sort_values('Missing_Percentage', ascending=False)\n\n\n\nprint(\"Top 30 Columns with Missing Values\")\nmissing_stats[missing_stats['Missing_Count'] > 0].head(30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:33.927945Z","iopub.execute_input":"2025-12-12T14:48:33.928221Z","iopub.status.idle":"2025-12-12T14:48:33.978751Z","shell.execute_reply.started":"2025-12-12T14:48:33.928201Z","shell.execute_reply":"2025-12-12T14:48:33.977921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize missing data pattern\nfig, axes = plt.subplots(2, 1, figsize=(16, 12))\n\n# Bar plot of missing percentages\ntop_missing = missing_stats[missing_stats['Missing_Count'] > 0].head(40)\naxes[0].barh(range(len(top_missing)), top_missing['Missing_Percentage'], color='coral')\naxes[0].set_yticks(range(len(top_missing)))\naxes[0].set_yticklabels(top_missing.index, fontsize=8)\naxes[0].set_xlabel('Missing Percentage (%)')\naxes[0].set_title('Top 40 Features with Missing Values', fontsize=12, fontweight='bold')\naxes[0].grid(alpha=0.3, axis='x')\n\n# Missing data over time (heatmap style)\nfeature_sample = [col for col in train_df.columns if col.startswith(('E', 'M', 'P', 'S', 'V'))][:30]\nmissing_over_time = train_df[feature_sample].isnull().astype(int).T\naxes[1].imshow(missing_over_time, aspect='auto', cmap='RdYlGn_r', interpolation='nearest')\naxes[1].set_title('Missing Data Pattern Over Time (Sample of 30 Features)', fontsize=12, fontweight='bold')\naxes[1].set_xlabel('date_id (time)')\naxes[1].set_ylabel('Features')\naxes[1].set_yticks(range(len(feature_sample)))\naxes[1].set_yticklabels(feature_sample, fontsize=7)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:33.979656Z","iopub.execute_input":"2025-12-12T14:48:33.979882Z","iopub.status.idle":"2025-12-12T14:48:34.903749Z","shell.execute_reply.started":"2025-12-12T14:48:33.979863Z","shell.execute_reply":"2025-12-12T14:48:34.902854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Dummy/Binary Features Analysis","metadata":{}},{"cell_type":"code","source":"d_features = [col for col in train_df.columns if col.startswith('D')]\n\nfor col in d_features:\n    print(f\"\\n{col} distribution:\")\n    print(train_df[col].value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:34.904692Z","iopub.execute_input":"2025-12-12T14:48:34.904953Z","iopub.status.idle":"2025-12-12T14:48:34.919396Z","shell.execute_reply.started":"2025-12-12T14:48:34.904933Z","shell.execute_reply":"2025-12-12T14:48:34.9185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(d_features) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:34.920439Z","iopub.execute_input":"2025-12-12T14:48:34.920802Z","iopub.status.idle":"2025-12-12T14:48:34.935424Z","shell.execute_reply.started":"2025-12-12T14:48:34.920773Z","shell.execute_reply":"2025-12-12T14:48:34.93448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Analyze relationship with target\nd_target_analysis = []\nfor col in d_features:\n    for val in train_df[col].dropna().unique():\n        subset_returns = train_df[train_df[col] == val]['forward_returns']\n        d_target_analysis.append({\n            'Feature': col,\n            'Value': val,\n            'Count': len(subset_returns),\n            'Mean_Return': subset_returns.mean(),\n            'Std_Return': subset_returns.std(),\n            'Median_Return': subset_returns.median()\n        })\n\nd_analysis_df = pd.DataFrame(d_target_analysis)\nd_analysis_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:34.936517Z","iopub.execute_input":"2025-12-12T14:48:34.937105Z","iopub.status.idle":"2025-12-12T14:48:34.988486Z","shell.execute_reply.started":"2025-12-12T14:48:34.937077Z","shell.execute_reply":"2025-12-12T14:48:34.987616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfig, axes = plt.subplots(3, 3, figsize=(16, 12))\naxes = axes.ravel()\n\nfor idx, col in enumerate(d_features):\n    if idx < 9:\n        train_df.groupby(col)['forward_returns'].mean().plot(kind='bar', ax=axes[idx], color='steelblue')\n        axes[idx].set_title(f'{col} vs Mean Forward Returns', fontsize=10, fontweight='bold')\n        axes[idx].set_xlabel(col)\n        axes[idx].set_ylabel('Mean Forward Returns')\n        axes[idx].grid(alpha=0.3, axis='y')\n        axes[idx].axhline(0, color='red', linestyle='--', linewidth=1)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:34.9894Z","iopub.execute_input":"2025-12-12T14:48:34.990207Z","iopub.status.idle":"2025-12-12T14:48:36.519624Z","shell.execute_reply.started":"2025-12-12T14:48:34.990176Z","shell.execute_reply":"2025-12-12T14:48:36.518725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Market & Reference Data Analysis","metadata":{}},{"cell_type":"code","source":"# Analyze SP500 and price relationship\nfig, axes = plt.subplots(3, 2, figsize=(16, 15))\n\n# SP500 over time\naxes[0, 0].plot(train_df['date_id'], train_df['SP500'], linewidth=1)\naxes[0, 0].set_title('S&P 500 Index Over Time')\naxes[0, 0].set_xlabel('Date ID')\naxes[0, 0].set_ylabel('S&P 500')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Price over time\naxes[0, 1].plot(train_df['date_id'], train_df['price'], linewidth=1, color='green')\naxes[0, 1].set_title('Price Over Time')\naxes[0, 1].set_xlabel('Date ID')\naxes[0, 1].set_ylabel('Price')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Scatter: SP500 vs forward returns\naxes[1, 0].scatter(train_df['SP500'], train_df[target], alpha=0.3, s=10)\naxes[1, 0].set_title('S&P 500 vs Forward Returns')\naxes[1, 0].set_xlabel('S&P 500')\naxes[1, 0].set_ylabel('Forward Returns')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Risk-free rate over time\naxes[1, 1].plot(train_df['date_id'], train_df['risk_free_rate'], linewidth=1, color='orange')\naxes[1, 1].set_title('Risk-Free Rate Over Time')\naxes[1, 1].set_xlabel('Date ID')\naxes[1, 1].set_ylabel('Risk-Free Rate')\naxes[1, 1].grid(True, alpha=0.3)\n\ncorr_price_sp500 = train_df['price'].corr(train_df['SP500'])\n\n# Dual axis comparison\nax1 = axes[2, 0]\nax2 = ax1.twinx()\nax1.plot(train_df['date_id'], train_df['price'], color='green', label='Asset Price', linewidth=1.5)\nax2.plot(train_df['date_id'], train_df['SP500'], color='blue', label='S&P 500', linewidth=1.5, alpha=0.7)\nax1.set_xlabel('date_id')\nax1.set_ylabel('Asset Price', color='green')\nax2.set_ylabel('S&P 500', color='blue')\nax1.set_title('Asset Price vs S&P 500', fontsize=12, fontweight='bold')\nax1.grid(alpha=0.3)\n\n# Scatter plot\naxes[2, 1].scatter(train_df['SP500'], train_df['price'], alpha=0.5, s=10)\naxes[2, 1].set_title(f'Asset Price vs S&P 500 (Corr: {corr_price_sp500:.4f})', \n                     fontsize=12, fontweight='bold')\naxes[2, 1].set_xlabel('S&P 500')\naxes[2, 1].set_ylabel('Asset Price')\naxes[2, 1].grid(alpha=0.3)\n\n# Add regression line\nz = np.polyfit(train_df['SP500'].dropna(), train_df['price'].dropna(), 1)\np = np.poly1d(z)\naxes[2, 1].plot(train_df['SP500'], p(train_df['SP500']), \"r--\", linewidth=2, alpha=0.8)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:36.520582Z","iopub.execute_input":"2025-12-12T14:48:36.520836Z","iopub.status.idle":"2025-12-12T14:48:37.893872Z","shell.execute_reply.started":"2025-12-12T14:48:36.520817Z","shell.execute_reply":"2025-12-12T14:48:37.892793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation analysis\n\nreference_cols = ['price', 'SP500', 'risk_free_rate', 'market_forward_excess_returns']\nfor col in reference_cols:\n    if col in train_df.columns:\n        corr = train_df[col].corr(train_df[target])\n        print(f\"{col:35s}: {corr:7.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:37.894912Z","iopub.execute_input":"2025-12-12T14:48:37.895232Z","iopub.status.idle":"2025-12-12T14:48:37.904799Z","shell.execute_reply.started":"2025-12-12T14:48:37.895203Z","shell.execute_reply":"2025-12-12T14:48:37.903862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Feature Group Correlation Analysis","metadata":{}},{"cell_type":"code","source":"# Calculate correlations for each feature group\nall_correlations = []\n\nfor category, features in feature_categories.items():\n    if category not in ['Identifier', 'Target_Variables', 'Reference_Data']:\n        available = [f for f in features if f in train_df.columns]\n        if available:\n            # Calculate correlation with target, handling NaN\n            corrs = train_df[available].corrwith(train_df[target])\n            corrs = corrs.dropna()\n            \n            if len(corrs) > 0:\n                print(f\"\\n--- {category} ---\")\n                print(f\"Features with data: {len(corrs)}\")\n                print(f\"Top 5 positive correlations:\")\n                print(corrs.nlargest(5))\n                print(f\"Top 5 negative correlations:\")\n                print(corrs.nsmallest(5))\n                \n                # Store for overall analysis\n                for feat, corr_val in corrs.items():\n                    all_correlations.append({\n                        'Feature': feat,\n                        'Category': category,\n                        'Correlation': corr_val\n                    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:37.905745Z","iopub.execute_input":"2025-12-12T14:48:37.90606Z","iopub.status.idle":"2025-12-12T14:48:37.983176Z","shell.execute_reply.started":"2025-12-12T14:48:37.906034Z","shell.execute_reply":"2025-12-12T14:48:37.982298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create correlation dataframe\ncorr_df = pd.DataFrame(all_correlations).sort_values('Correlation', key=abs, ascending=False)\n\n# Visualize top correlations\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\n\n# Top positive correlations\ntop_positive = corr_df.nlargest(20, 'Correlation')\naxes[0].barh(range(len(top_positive)), top_positive['Correlation'].values)\naxes[0].set_yticks(range(len(top_positive)))\naxes[0].set_yticklabels(top_positive['Feature'].values, fontsize=8)\naxes[0].set_xlabel('Correlation')\naxes[0].set_title('Top 20 Positive Correlations with Forward Returns')\naxes[0].grid(True, alpha=0.3, axis='x')\n\n# Top negative correlations\ntop_negative = corr_df.nsmallest(20, 'Correlation')\naxes[1].barh(range(len(top_negative)), top_negative['Correlation'].values, color='red')\naxes[1].set_yticks(range(len(top_negative)))\naxes[1].set_yticklabels(top_negative['Feature'].values, fontsize=8)\naxes[1].set_xlabel('Correlation')\naxes[1].set_title('Top 20 Negative Correlations with Forward Returns')\naxes[1].grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:37.984087Z","iopub.execute_input":"2025-12-12T14:48:37.984344Z","iopub.status.idle":"2025-12-12T14:48:38.574994Z","shell.execute_reply.started":"2025-12-12T14:48:37.984319Z","shell.execute_reply":"2025-12-12T14:48:38.574068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Advanced correlation analysis for feature groups\ndef analyze_feature_group_correlation(df, prefix, target='forward_returns'):\n    \"\"\"Analyze correlation within a feature group\"\"\"\n    cols = [col for col in df.columns if col.startswith(prefix)]\n    \n    if len(cols) > 1:\n        # Correlation within group\n        group_corr = df[cols].corr()\n        \n        # Plot heatmap\n        plt.figure(figsize=(12, 10))\n        sns.heatmap(group_corr, cmap='coolwarm', center=0, \n                    square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n        plt.title(f'Correlation Matrix: {prefix} Features')\n        plt.tight_layout()\n        plt.show()\n        \n        # Find highly correlated pairs\n        corr_pairs = []\n        for i in range(len(group_corr.columns)):\n            for j in range(i+1, len(group_corr.columns)):\n                if abs(group_corr.iloc[i, j]) > 0.8:\n                    corr_pairs.append({\n                        'Feature1': group_corr.columns[i],\n                        'Feature2': group_corr.columns[j],\n                        'Correlation': group_corr.iloc[i, j]\n                    })\n        \n        if corr_pairs:\n            print(f\"\\nHighly correlated pairs in {prefix} features (|r| > 0.8):\")\n            print(pd.DataFrame(corr_pairs))\n\n# Run for each feature group\nfor prefix in ['E', 'M', 'P', 'S', 'V']:\n    analyze_feature_group_correlation(train_df, prefix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:38.575926Z","iopub.execute_input":"2025-12-12T14:48:38.576191Z","iopub.status.idle":"2025-12-12T14:48:40.867715Z","shell.execute_reply.started":"2025-12-12T14:48:38.576166Z","shell.execute_reply":"2025-12-12T14:48:40.866898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Feature Availability Analysis","metadata":{}},{"cell_type":"code","source":"# Determine when each feature group becomes available\navailability_info = []\n\nfor category, features in feature_categories.items():\n    if category not in ['Identifier', 'Target_Variables', 'Reference_Data', 'Dummy_Features']:\n        available = [f for f in features if f in train_df.columns]\n        if available:\n            # Find first date with non-null data\n            first_data = train_df[train_df[available].notna().any(axis=1)]['date_id'].min()\n            # Count rows with any data\n            rows_with_data = train_df[available].notna().any(axis=1).sum()\n            # Calculate completeness\n            completeness = (train_df[available].notna().sum().sum() / (len(train_df) * len(available))) * 100\n            \n            availability_info.append({\n                'Category': category,\n                'Num_Features': len(available),\n                'First_Data_Date': first_data,\n                'Rows_With_Data': rows_with_data,\n                'Overall_Completeness_%': completeness\n            })\n\navailability_df = pd.DataFrame(availability_info).sort_values('First_Data_Date')\navailability_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:40.872622Z","iopub.execute_input":"2025-12-12T14:48:40.872893Z","iopub.status.idle":"2025-12-12T14:48:40.925894Z","shell.execute_reply.started":"2025-12-12T14:48:40.872873Z","shell.execute_reply":"2025-12-12T14:48:40.924999Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Volatility Analysis","metadata":{}},{"cell_type":"code","source":"\n# Calculate rolling volatility\nwindows = [20, 50, 100]\nfig, axes = plt.subplots(len(windows), 1, figsize=(16, 12))\n\nfor idx, window in enumerate(windows):\n    rolling_vol = train_df[target].rolling(window=window).std()\n    axes[idx].plot(train_df['date_id'], rolling_vol, linewidth=1)\n    axes[idx].set_title(f'{window}-Period Rolling Volatility')\n    axes[idx].set_xlabel('Date ID')\n    axes[idx].set_ylabel('Volatility')\n    axes[idx].grid(True, alpha=0.3)\n    \n    print(f\"\\n{window}-period rolling volatility:\")\n    print(f\"  Mean: {rolling_vol.mean():.6f}\")\n    print(f\"  Std:  {rolling_vol.std():.6f}\")\n    print(f\"  Max:  {rolling_vol.max():.6f}\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:40.9267Z","iopub.execute_input":"2025-12-12T14:48:40.926929Z","iopub.status.idle":"2025-12-12T14:48:41.604702Z","shell.execute_reply.started":"2025-12-12T14:48:40.92691Z","shell.execute_reply":"2025-12-12T14:48:41.603853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9. Excess Returns & Risk-free Rate Analysis","metadata":{}},{"cell_type":"code","source":"\nprint(f\"Risk-Free Rate Statistics:\")\nprint(train_df['risk_free_rate'].describe())\n\nprint(f\"Market Forward Excess Returns Statistics:\")\nprint(train_df['market_forward_excess_returns'].describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:41.605572Z","iopub.execute_input":"2025-12-12T14:48:41.605817Z","iopub.status.idle":"2025-12-12T14:48:41.617282Z","shell.execute_reply.started":"2025-12-12T14:48:41.605798Z","shell.execute_reply":"2025-12-12T14:48:41.616474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Verify relationship\ncalculated_excess = train_df['forward_returns'] - train_df['risk_free_rate']\nprint(f\"Verification: market_forward_excess_returns = forward_returns - risk_free_rate\")\nprint(f\"  Max Difference: {abs(calculated_excess - train_df['market_forward_excess_returns']).max():.10f}\")\nprint(f\"   Relationship confirmed!\" if abs(calculated_excess - train_df['market_forward_excess_returns']).max() < 1e-6 else \"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:41.618213Z","iopub.execute_input":"2025-12-12T14:48:41.618543Z","iopub.status.idle":"2025-12-12T14:48:41.627403Z","shell.execute_reply.started":"2025-12-12T14:48:41.618516Z","shell.execute_reply":"2025-12-12T14:48:41.626581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Sharpe Ratio\nsharpe_ratio = train_df['market_forward_excess_returns'].mean() / train_df['market_forward_excess_returns'].std() * np.sqrt(252)\nprint(f\" Sharpe Ratio (annualized): {sharpe_ratio:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:41.628144Z","iopub.execute_input":"2025-12-12T14:48:41.628402Z","iopub.status.idle":"2025-12-12T14:48:41.644527Z","shell.execute_reply.started":"2025-12-12T14:48:41.628383Z","shell.execute_reply":"2025-12-12T14:48:41.643738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n# Risk-free rate over time\naxes[0, 0].plot(train_df['date_id'], train_df['risk_free_rate'] * 100, linewidth=1.2)\naxes[0, 0].set_title('Risk-Free Rate Over Time', fontsize=12, fontweight='bold')\naxes[0, 0].set_xlabel('date_id')\naxes[0, 0].set_ylabel('Risk-Free Rate (%)')\naxes[0, 0].grid(alpha=0.3)\n\n# Excess returns over time\naxes[0, 1].plot(train_df['date_id'], train_df['market_forward_excess_returns'], linewidth=0.8, alpha=0.7)\naxes[0, 1].axhline(0, color='red', linestyle='--', linewidth=1)\naxes[0, 1].set_title('Market Forward Excess Returns Over Time', fontsize=12, fontweight='bold')\naxes[0, 1].set_xlabel('date_id')\naxes[0, 1].set_ylabel('Excess Returns')\naxes[0, 1].grid(alpha=0.3)\n\n# Distribution comparison\naxes[1, 0].hist(train_df['forward_returns'].dropna(), bins=50, alpha=0.5, label='Forward Returns', color='blue')\naxes[1, 0].hist(train_df['market_forward_excess_returns'].dropna(), bins=50, alpha=0.5, \n                label='Excess Returns', color='orange')\naxes[1, 0].set_title('Returns Distribution Comparison', fontsize=12, fontweight='bold')\naxes[1, 0].set_xlabel('Returns')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].legend()\naxes[1, 0].grid(alpha=0.3)\n\n# Rolling Sharpe ratio\nrolling_sharpe = (train_df['market_forward_excess_returns'].rolling(window=60).mean() / \n                  train_df['market_forward_excess_returns'].rolling(window=60).std() * np.sqrt(252))\naxes[1, 1].plot(train_df['date_id'], rolling_sharpe, linewidth=1.2)\naxes[1, 1].axhline(0, color='red', linestyle='--', linewidth=1)\naxes[1, 1].set_title('60-Day Rolling Sharpe Ratio', fontsize=12, fontweight='bold')\naxes[1, 1].set_xlabel('date_id')\naxes[1, 1].set_ylabel('Sharpe Ratio')\naxes[1, 1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:41.645338Z","iopub.execute_input":"2025-12-12T14:48:41.645651Z","iopub.status.idle":"2025-12-12T14:48:42.754667Z","shell.execute_reply.started":"2025-12-12T14:48:41.645625Z","shell.execute_reply":"2025-12-12T14:48:42.753746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10. Train vs Test Comparison","metadata":{}},{"cell_type":"code","source":"print(f\"\\nTraining period: date_id {train_df['date_id'].min()} to {train_df['date_id'].max()}\")\nprint(f\"Test period:     date_id {test_df['date_id'].min()} to {test_df['date_id'].max()}\")\nprint(f\"\\nTime gap: {test_df['date_id'].min() - train_df['date_id'].max()} periods\")\n\n# Compare feature availability\nprint(\"\\n--- Feature Availability Comparison ---\")\ntrain_cols = set(train_df.columns)\ntest_cols = set(test_df.columns)\ncommon_cols = train_cols.intersection(test_cols)\ntrain_only = train_cols - test_cols\ntest_only = test_cols - train_cols\n\nprint(f\"Common features: {len(common_cols)}\")\nprint(f\"Train only: {len(train_only)} - {list(train_only)}\")\nprint(f\"Test only: {len(test_only)} - {list(test_only)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T14:48:42.755649Z","iopub.execute_input":"2025-12-12T14:48:42.755922Z","iopub.status.idle":"2025-12-12T14:48:42.763691Z","shell.execute_reply.started":"2025-12-12T14:48:42.755901Z","shell.execute_reply":"2025-12-12T14:48:42.762792Z"}},"outputs":[],"execution_count":null}]}