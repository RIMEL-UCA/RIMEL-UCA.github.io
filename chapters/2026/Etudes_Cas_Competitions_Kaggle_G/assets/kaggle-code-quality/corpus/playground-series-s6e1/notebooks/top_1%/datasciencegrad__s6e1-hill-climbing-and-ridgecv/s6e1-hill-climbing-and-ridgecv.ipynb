{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":14521741,"sourceType":"datasetVersion","datasetId":9192936}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":3711.816063,"end_time":"2026-01-07T12:16:52.88658","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2026-01-07T11:15:01.070517","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Required Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2026-01-07T11:15:11.556873Z","iopub.status.busy":"2026-01-07T11:15:11.556451Z","iopub.status.idle":"2026-01-07T12:16:52.163042Z","shell.execute_reply":"2026-01-07T12:16:52.162308Z"},"papermill":{"duration":3700.611252,"end_time":"2026-01-07T12:16:52.164764","exception":false,"start_time":"2026-01-07T11:15:11.553512","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Blending Function","metadata":{}},{"cell_type":"code","source":"def blend_submissions(weight_dict, output_path=\"submission.csv\", visualize=True):\n    \n    print(\"Starting Blending Pipeline\")\n    print(\"=\" * 70)\n    \n    dfs = []\n    stats = {}\n    \n    print(\"\\nLoading submissions...\")\n    for idx, (path, weight) in enumerate(weight_dict.items(), 1):\n        df = pd.read_csv(path)\n        \n        if df['exam_score'].isna().any():\n            print(f\"  Warning: Found {df['exam_score'].isna().sum()} NaN values in {Path(path).name}\")\n            df = df.dropna(subset=['exam_score'])\n        \n        if not np.isfinite(df['exam_score']).all():\n            print(f\"  Warning: Found infinite values in {Path(path).name}\")\n            df = df[np.isfinite(df['exam_score'])]\n        \n        df['weighted_pred'] = df['exam_score'] * weight\n        dfs.append(df)\n        \n        stats[f'Model_{idx}'] = {\n            'file': Path(path).name,\n            'weight': weight,\n            'mean': df['exam_score'].mean(),\n            'std': df['exam_score'].std()\n        }\n        print(f\"  Loaded {Path(path).name}: weight={weight}\")\n    \n    print(\"\\nMerging predictions...\")\n    merged = dfs[0][['id', 'weighted_pred']].copy()\n    \n    for i, df in enumerate(dfs[1:], 1):\n        merged = merged.merge(\n            df[['id', 'weighted_pred']], \n            on='id', \n            suffixes=('', f'_{i}')\n        )\n    \n    weight_cols = [c for c in merged.columns if 'weighted_pred' in c]\n    merged['exam_score'] = merged[weight_cols].sum(axis=1) / sum(weight_dict.values())\n    \n    merged = merged.dropna(subset=['exam_score'])\n    merged = merged[np.isfinite(merged['exam_score'])]\n    \n    for i, df in enumerate(dfs):\n        merged[f'model_{i+1}'] = df.set_index('id')['exam_score']\n    \n    result = merged[['id', 'exam_score']].copy()\n    \n    print(\"\\nStatistics:\")\n    print(\"-\" * 70)\n    total_weight = sum(weight_dict.values())\n    for name, s in stats.items():\n        pct = (s['weight'] / total_weight) * 100\n        print(f\"{name}: {s['file']}\")\n        print(f\"  Weight: {s['weight']:.2f} ({pct:.1f}%) | Mean: {s['mean']:.4f} | Std: {s['std']:.4f}\")\n    \n    print(f\"\\nBlended: Mean={result['exam_score'].mean():.4f}, Std={result['exam_score'].std():.4f}\")\n    print(\"-\" * 70)\n    \n    if visualize and len(dfs) > 1:\n        create_visualization(dfs, result, stats, total_weight)\n    \n    result.to_csv(output_path, index=False)\n    print(f\"\\nBlended submission saved: {output_path}\")\n    print(f\"Total samples: {len(result)}\")\n    \n    return merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_visualization(dfs, result, stats, total_weight):\n    \n    print(\"\\nGenerating visualization...\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    colors = plt.cm.Set2(np.linspace(0, 1, len(dfs)))\n    \n    # Distribution comparison\n    for i, df in enumerate(dfs, 1):\n        data = df['exam_score'].dropna()\n        data = data[np.isfinite(data)]\n        if len(data) > 0:\n            axes[0].hist(data, bins=50, alpha=0.4, \n                        color=colors[i-1], label=f'Model {i}', edgecolor='white')\n            data.plot.kde(ax=axes[0], color=colors[i-1], linewidth=2)\n    \n    result_data = result['exam_score'].dropna()\n    result_data = result_data[np.isfinite(result_data)]\n    \n    axes[0].hist(result_data, bins=50, alpha=0.6, \n                color='crimson', label='Blended', edgecolor='darkred', linewidth=1.5)\n    result_data.plot.kde(ax=axes[0], color='darkred', linewidth=2.5, linestyle='--')\n    \n    axes[0].set_xlabel('Exam Score', fontsize=11, fontweight='bold')\n    axes[0].set_ylabel('Density / Frequency', fontsize=11, fontweight='bold')\n    axes[0].set_title('Score Distribution: Models vs Ensemble', fontsize=12, fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n    \n    # Box plot comparison\n    box_data = []\n    for df in dfs:\n        data = df['exam_score'].dropna()\n        data = data[np.isfinite(data)]\n        box_data.append(data.values)\n    box_data.append(result_data.values)\n    \n    box_labels = [f'Model {i}' for i in range(1, len(dfs) + 1)] + ['Blended']\n    \n    bp = axes[1].boxplot(box_data, labels=box_labels, patch_artist=True,\n                        notch=True, widths=0.6)\n    \n    for patch, color in zip(bp['boxes'][:-1], colors):\n        patch.set_facecolor(color)\n        patch.set_alpha(0.7)\n    bp['boxes'][-1].set_facecolor('crimson')\n    bp['boxes'][-1].set_alpha(0.8)\n    \n    axes[1].set_ylabel('Exam Score', fontsize=11, fontweight='bold')\n    axes[1].set_title('Distribution Comparison (Box Plot)', fontsize=12, fontweight='bold')\n    axes[1].grid(alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('blending_analysis.png', dpi=200, bbox_inches='tight')\n    plt.show()\n    \n    print(\"Saved: blending_analysis.png\")","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## OOF Generation Function","metadata":{}},{"cell_type":"code","source":"def generate_oof(blended_df, output_path=\"oof_predictions.csv\", visualize=True):\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"Generating OOF Predictions\")\n    print(\"=\" * 70)\n    \n    model_cols = [c for c in blended_df.columns if c.startswith('model_')]\n    \n    if len(model_cols) == 0:\n        print(\"No individual model predictions found\")\n        return None\n    \n    oof = blended_df[['id', 'exam_score'] + model_cols].copy()\n    \n    if len(model_cols) > 1:\n        pred_std = oof[model_cols].std(axis=1)\n        oof['diversity'] = pred_std\n        \n        print(f\"\\nOOF Metrics:\")\n        print(f\"  Models included: {len(model_cols)}\")\n        print(f\"  Samples: {len(oof)}\")\n        print(f\"  Mean diversity: {pred_std.mean():.4f}\")\n        print(f\"  Agreement score: {(1 / (1 + pred_std.mean())):.4f}\")\n        \n        if visualize:\n            create_oof_visualization(oof, model_cols, pred_std)\n    \n    oof.to_csv(output_path, index=False)\n    print(f\"\\nOOF predictions saved: {output_path}\")\n    \n    return oof","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_oof_visualization(oof, model_cols, pred_std):\n    \n    print(\"\\nGenerating OOF visualization...\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    colors = plt.cm.Set2(np.linspace(0, 1, len(model_cols)))\n    \n    # Diversity distribution\n    pred_std_clean = pred_std.dropna()\n    pred_std_clean = pred_std_clean[np.isfinite(pred_std_clean)]\n    \n    axes[0].hist(pred_std_clean, bins=50, color='steelblue', edgecolor='navy', alpha=0.7)\n    axes[0].axvline(pred_std_clean.mean(), color='red', linestyle='--', linewidth=2,\n                   label=f'Mean: {pred_std_clean.mean():.4f}')\n    axes[0].set_xlabel('Standard Deviation Across Models', fontsize=11, fontweight='bold')\n    axes[0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n    axes[0].set_title('Prediction Diversity Distribution', fontsize=12, fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n    \n    # Model vs Ensemble scatter\n    oof_clean = oof.dropna(subset=['exam_score'] + model_cols)\n    \n    for i, col in enumerate(model_cols):\n        axes[1].scatter(oof_clean[col], oof_clean['exam_score'], \n                       alpha=0.4, s=20, color=colors[i], label=f'Model {i+1}')\n    \n    min_val = min(oof_clean['exam_score'].min(), min([oof_clean[col].min() for col in model_cols]))\n    max_val = max(oof_clean['exam_score'].max(), max([oof_clean[col].max() for col in model_cols]))\n    axes[1].plot([min_val, max_val], [min_val, max_val], \n                'r--', linewidth=2, label='Perfect Agreement')\n    \n    axes[1].set_xlabel('Individual Model Predictions', fontsize=11, fontweight='bold')\n    axes[1].set_ylabel('Ensemble Prediction', fontsize=11, fontweight='bold')\n    axes[1].set_title('Models vs Ensemble', fontsize=12, fontweight='bold')\n    axes[1].legend()\n    axes[1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('oof_analysis.png', dpi=200, bbox_inches='tight')\n    plt.show()\n    \n    print(\"Saved: oof_analysis.png\")","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Execute Pipeline","metadata":{}},{"cell_type":"code","source":"weight_dict = {\n    \"/kaggle/input/student-test-scores-vault/submission.csv\": 2.7,\n    \"/kaggle/input/student-test-scores-vault/submission (1).csv\": 0.1,\n}\n\nblended_df = blend_submissions(weight_dict, output_path=\"submission.csv\", visualize=True)\n\noof_df = generate_oof(blended_df, output_path=\"oof_predictions.csv\", visualize=True)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Pipeline Complete\")\nprint(\"=\" * 70)","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null}]}