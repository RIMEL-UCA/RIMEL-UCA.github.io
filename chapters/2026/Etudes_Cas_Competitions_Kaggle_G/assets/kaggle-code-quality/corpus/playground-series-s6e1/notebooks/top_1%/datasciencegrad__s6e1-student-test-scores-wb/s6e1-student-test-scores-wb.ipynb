{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":14539798,"sourceType":"datasetVersion","datasetId":9286678}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import mean_squared_error\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submissions Analysis","metadata":{}},{"cell_type":"code","source":"def analyze_submissions(file_paths):\n    \n    print(\"Submission Analysis\")\n    print(\"=\" * 70)\n    \n    dfs = []\n    analysis = []\n    \n    for idx, path in enumerate(file_paths, 1):\n        df = pd.read_csv(path)\n        filename = Path(path).name\n        \n        stats = {\n            'model': f'Model_{idx}',\n            'filename': filename,\n            'samples': len(df),\n            'missing': df['exam_score'].isna().sum(),\n            'duplicates': df['id'].duplicated().sum(),\n            'mean': df['exam_score'].mean(),\n            'std': df['exam_score'].std(),\n            'min': df['exam_score'].min(),\n            'max': df['exam_score'].max()\n        }\n        \n        analysis.append(stats)\n        dfs.append(df)\n        \n        print(f\"{stats['model']}: {filename}\")\n        print(f\"  Samples: {stats['samples']} | Mean: {stats['mean']:.6f} | Std: {stats['std']:.6f}\")\n    \n    analysis_df = pd.DataFrame(analysis)\n    print(\"=\" * 70)\n    \n    return dfs, analysis_df\n\n\ndef calculate_correlation(dfs):\n    \n    print(\"\\nPrediction Correlation Analysis\")\n    print(\"-\" * 70)\n    \n    merged = dfs[0][['id', 'exam_score']].copy()\n    merged.columns = ['id', 'model_1']\n    \n    for i, df in enumerate(dfs[1:], 2):\n        temp = df[['id', 'exam_score']].copy()\n        temp.columns = ['id', f'model_{i}']\n        merged = merged.merge(temp, on='id', how='inner')\n    \n    model_cols = [c for c in merged.columns if c.startswith('model_')]\n    corr_matrix = merged[model_cols].corr()\n    \n    print(\"\\nCorrelation Matrix:\")\n    print(corr_matrix.round(4))\n    \n    avg_corr = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].mean()\n    print(f\"\\nAverage Correlation: {avg_corr:.4f}\")\n    \n    if avg_corr > 0.95:\n        print(\"High correlation detected. Limited ensemble benefit expected.\")\n    elif avg_corr > 0.85:\n        print(\"Moderate correlation. Some ensemble benefit expected.\")\n    else:\n        print(\"Good diversity detected. Strong ensemble potential.\")\n    \n    return corr_matrix, merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Weight Optimization","metadata":{}},{"cell_type":"code","source":"def optimize_weights(merged_df, method='hill_climb', validation_split=0.2):\n    \n    print(f\"\\nWeight Optimization: {method.upper()}\")\n    print(\"=\" * 70)\n    \n    model_cols = [c for c in merged_df.columns if c.startswith('model_')]\n    n_models = len(model_cols)\n    \n    if validation_split > 0:\n        n_val = int(len(merged_df) * validation_split)\n        indices = np.random.permutation(len(merged_df))\n        train_df = merged_df.iloc[indices[n_val:]].copy()\n        val_df = merged_df.iloc[indices[:n_val]].copy()\n        print(f\"Train: {len(train_df)} | Validation: {len(val_df)}\")\n    else:\n        train_df = val_df = merged_df.copy()\n    \n    if method == 'equal':\n        weights = np.ones(n_models) / n_models\n        \n    elif method == 'hill_climb':\n        weights = hill_climb_optimization(train_df, val_df, model_cols)\n        \n    elif method == 'scipy':\n        weights = scipy_optimization(train_df, val_df, model_cols)\n        \n    else:\n        weights = np.ones(n_models) / n_models\n    \n    print(f\"\\nOptimized Weights: {weights}\")\n    return weights\n\n\ndef hill_climb_optimization(train_df, val_df, model_cols, iterations=1000):\n    \n    n_models = len(model_cols)\n    best_weights = np.ones(n_models) / n_models\n    target = val_df[model_cols].mean(axis=1)\n    best_score = mean_squared_error(target, val_df[model_cols].mean(axis=1))\n    \n    for _ in range(iterations):\n        new_weights = best_weights + np.random.randn(n_models) * 0.1\n        new_weights = np.maximum(new_weights, 0)\n        new_weights = new_weights / new_weights.sum()\n        \n        pred = (val_df[model_cols].values * new_weights).sum(axis=1)\n        score = mean_squared_error(target, pred)\n        \n        if score < best_score:\n            best_score = score\n            best_weights = new_weights\n    \n    print(f\"Validation MSE: {best_score:.6f}\")\n    return best_weights\n\n\ndef scipy_optimization(train_df, val_df, model_cols):\n    \n    n_models = len(model_cols)\n    target = val_df[model_cols].mean(axis=1)\n    \n    def objective(weights):\n        weights = weights / weights.sum()\n        pred = (val_df[model_cols].values * weights).sum(axis=1)\n        return mean_squared_error(target, pred)\n    \n    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n    bounds = [(0, 1) for _ in range(n_models)]\n    initial = np.ones(n_models) / n_models\n    \n    result = minimize(objective, initial, method='SLSQP', bounds=bounds, constraints=constraints)\n    optimal_weights = result.x / result.x.sum()\n    \n    print(f\"Validation MSE: {result.fun:.6f}\")\n    return optimal_weights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Blending","metadata":{}},{"cell_type":"code","source":"def blend_submissions(file_paths, weights, output_path=\"submission.csv\", visualize=True):\n    \n    print(\"\\nBlending Submissions\")\n    print(\"=\" * 70)\n    \n    dfs = []\n    \n    for idx, (path, weight) in enumerate(zip(file_paths, weights), 1):\n        df = pd.read_csv(path)\n        df = df.dropna(subset=['exam_score'])\n        df = df[np.isfinite(df['exam_score'])]\n        df['weighted_pred'] = df['exam_score'] * weight\n        dfs.append(df)\n        print(f\"Model {idx}: {Path(path).name} (weight: {weight:.4f})\")\n    \n    merged = dfs[0][['id', 'weighted_pred']].copy()\n    \n    for i, df in enumerate(dfs[1:], 1):\n        merged = merged.merge(df[['id', 'weighted_pred']], on='id', suffixes=('', f'_{i}'))\n    \n    weight_cols = [c for c in merged.columns if 'weighted_pred' in c]\n    merged['exam_score'] = merged[weight_cols].sum(axis=1)\n    \n    for i, df in enumerate(dfs):\n        temp = df[['id', 'exam_score']].copy()\n        temp.columns = ['id', f'model_{i+1}']\n        merged = merged.merge(temp, on='id', how='left')\n    \n    merged = merged.dropna()\n    result = merged[['id', 'exam_score']].copy()\n    \n    print(f\"\\nBlended Results:\")\n    print(f\"  Samples: {len(result)}\")\n    print(f\"  Mean: {result['exam_score'].mean():.6f}\")\n    print(f\"  Std: {result['exam_score'].std():.6f}\")\n    \n    if visualize:\n        visualize_blend(dfs, result, weights)\n    \n    result.to_csv(output_path, index=False)\n    print(f\"\\nSaved: {output_path}\")\n    \n    return merged","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_blend(dfs, result, weights):\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    colors = plt.cm.tab10(np.linspace(0, 1, len(dfs)))\n    \n    for i, df in enumerate(dfs, 1):\n        data = df['exam_score'].dropna()\n        axes[0].hist(data, bins=50, alpha=0.4, color=colors[i-1], \n                    label=f'Model {i} (w={weights[i-1]:.3f})')\n    \n    axes[0].hist(result['exam_score'], bins=50, alpha=0.6, \n                color='black', label='Blended', linewidth=2)\n    axes[0].set_xlabel('Exam Score')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Distribution Comparison')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n    \n    box_data = [df['exam_score'].dropna().values for df in dfs]\n    box_data.append(result['exam_score'].values)\n    labels = [f'M{i}\\n{weights[i-1]:.3f}' for i in range(1, len(dfs)+1)] + ['Blend']\n    \n    bp = axes[1].boxplot(box_data, labels=labels, patch_artist=True)\n    for patch, color in zip(bp['boxes'][:-1], colors):\n        patch.set_facecolor(color)\n        patch.set_alpha(0.6)\n    bp['boxes'][-1].set_facecolor('black')\n    bp['boxes'][-1].set_alpha(0.6)\n    \n    axes[1].set_ylabel('Exam Score')\n    axes[1].set_title('Box Plot Comparison')\n    axes[1].grid(alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.savefig('blending_analysis.png', dpi=200)\n    plt.show()","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Execute Pipeline","metadata":{}},{"cell_type":"code","source":"file_paths = [\n    \"/kaggle/input/s6e1-student-test-scores/8.54461_sub.csv\",\n    \"/kaggle/input/s6e1-student-test-scores/8.54462_sub.csv\",\n    \"/kaggle/input/s6e1-student-test-scores/8.54465_sub.csv\",\n    \"/kaggle/input/s6e1-student-test-scores/8.54466_sub.csv\",\n    \"/kaggle/input/s6e1-student-test-scores/8.54476_sub.csv\",\n]\n\ndfs, analysis_df = analyze_submissions(file_paths)\n\ncorr_matrix, merged_df = calculate_correlation(dfs)\n\nmethods = ['equal', 'hill_climb', 'scipy']\nall_weights = {}\n\nfor method in methods:\n    weights = optimize_weights(merged_df, method=method, validation_split=0.2)\n    all_weights[method] = weights\n\nchosen_method = 'hill_climb'\nfinal_weights = all_weights[chosen_method]\n\nblended_df = blend_submissions(file_paths, final_weights, output_path=\"submission.csv\", visualize=True)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Pipeline Complete\")\nprint(\"=\" * 70)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}