{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# S6E1 Doubly Censored Regression: XGBoost + Tobit + Bayes Optimal Prediction\r\n\r\n### ðŸ“‰ A Theoretical Approach to Bounded Targets\r\n\r\nThis notebook implements a **Doubly Censored Tobit Model** as a custom objective function for XGBoost.  \r\nUnlike standard regression (MSE) which treats the target limits (19.6 and 100.0) as arbitrary cutoffs, the Tobit model statistically handles them as **censored data**.\r\n\r\n**Key Features:**\r\n*   **Custom Objective**: Maximizes Tobit Log-Likelihood (Gradient & Hessian derived mathematically).\r\n*   **Bayes Optimal Prediction**: Converts the latent variable $z$ (raw output) into the expected value $E[y|x]$ considering the boundaries.\r\n*   **Robust Pipeline**: Incorporates **Ridge Stacking** (Stage 1) and **Category Mean Encoding** for a strong baseline.\r\n\r\n> **Note:** While standard MSE + Clipping often yields better raw RMSE on the Leaderboard due to the metric's nature, this approach generates a **statistically more natural distribution** at the boundaries. It serves as an excellent **diversity candidate** for your ensemble.\r\n\r\n*Inspired by [broccoli beef's discussion](https://www.kaggle.com/competitions/playground-series-s6e1/discussion/667296).*","metadata":{"_uuid":"5f538de2-7703-479e-b801-244ccb04c3a3","_cell_guid":"5635628b-70c6-45dc-85e6-2fdbcaaa5165","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 1. Setup & Configuration","metadata":{"_uuid":"dfc14817-5416-4124-a5fa-df0188928b9f","_cell_guid":"7be80855-f652-4da5-97f3-53dca7260fe3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\r\nimport numpy as np\r\nimport xgboost as xgb\r\nfrom sklearn.model_selection import KFold\r\nfrom sklearn.metrics import mean_squared_error\r\nfrom sklearn.linear_model import RidgeCV\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom scipy import stats\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport warnings\r\nimport gc\r\n\r\nwarnings.filterwarnings(\"ignore\")\r\nsns.set_style(\"whitegrid\")\r\n\r\nclass CFG:\r\n    SEED = 42\r\n    N_FOLDS = 10\r\n    TRAIN_PATH = \"/kaggle/input/playground-series-s6e1/train.csv\"\r\n    TEST_PATH = \"/kaggle/input/playground-series-s6e1/test.csv\"\r\n    SUB_PATH = \"/kaggle/input/playground-series-s6e1/sample_submission.csv\"\r\n    TARGET = \"exam_score\"\r\n    ID_COL = \"id\"\r\n    \r\n    # Tobit Parameters (Double Censoring)\r\n    Y_L = 19.6   # Lower bound (Min score)\r\n    Y_H = 100.0  # Upper bound (Max score)\r\n    SIGMA = 8.5  # Estimated noise standard deviation","metadata":{"_uuid":"d5c99760-5eda-4a56-8af4-8b79a9912017","_cell_guid":"b1c881ec-5353-4a3a-bdc8-6cbf8c70aec6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-18T12:51:26.649691Z","iopub.execute_input":"2026-01-18T12:51:26.650178Z","iopub.status.idle":"2026-01-18T12:51:28.361425Z","shell.execute_reply.started":"2026-01-18T12:51:26.650123Z","shell.execute_reply":"2026-01-18T12:51:28.360886Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Tobit Theory: Mathematical Core\r\n\r\n### Custom Objective\r\nWe define the Log-Likelihood for the doubly censored model and derive the **Gradient (1st derivative)** and **Hessian (2nd derivative)** required by XGBoost.\r\n\r\n### Bayes Optimal Prediction\r\nThe raw output of the Tobit model is a latent variable $z \\in (-\\infty, \\infty)$. To get the final prediction, we calculate the expected value:\r\n$$ E[y|z] = P(L) \\cdot y_L + P(H) \\cdot y_H + P(Obs) \\cdot E[y_{obs}] $$","metadata":{"_uuid":"d73a6ee2-b7ec-441e-93db-2e45778bbcab","_cell_guid":"cad64c3f-90fb-43a6-a90b-b02b226000ea","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def bayes_optimal_pred(z_raw, ymin, ymax, sigma):\r\n    \"\"\"\r\n    Converts latent Tobit output to expected value.\r\n    \"\"\"\r\n    ymin_ = (ymin - z_raw) / sigma\r\n    ymax_ = (ymax - z_raw) / sigma\r\n    \r\n    Phi_min = stats.norm.cdf(ymin_)\r\n    Phi_max = stats.norm.cdf(ymax_)\r\n    pdf_min = stats.norm.pdf(ymin_)\r\n    pdf_max = stats.norm.pdf(ymax_)\r\n    \r\n    return (\r\n        ymin * Phi_min +\r\n        ymax * (1 - Phi_max) +\r\n        z_raw * (Phi_max - Phi_min) -\r\n        sigma * (pdf_max - pdf_min)\r\n    )\r\n\r\ndef tobit_objective(y_pred, dtrain):\r\n    \"\"\"\r\n    Custom Gradient & Hessian for Doubly Censored Tobit Likelihood\r\n    \"\"\"\r\n    y_true = dtrain.get_label()\r\n    sigma = CFG.SIGMA\r\n    y_L, y_H = CFG.Y_L, CFG.Y_H\r\n    y_pred = y_pred.flatten()\r\n\r\n    z_L = (y_L - y_pred) / sigma\r\n    z_H = (y_H - y_pred) / sigma\r\n    z_y = (y_true - y_pred) / sigma\r\n    \r\n    Phi_L = stats.norm.cdf(z_L)\r\n    Phi_H = stats.norm.cdf(z_H)\r\n    phi_L = stats.norm.pdf(z_L)\r\n    phi_H = stats.norm.pdf(z_H)\r\n    \r\n    # Identify censored regions (with small buffer for numerical stability)\r\n    is_censored_low = y_true <= y_L + 1e-4\r\n    is_censored_high = y_true >= y_H - 1e-4\r\n    is_observed = ~(is_censored_low | is_censored_high)\r\n\r\n    grad = np.zeros_like(y_pred)\r\n    hess = np.ones_like(y_pred) / (sigma ** 2)\r\n    \r\n    # 1. Lower Censored\r\n    if is_censored_low.any():\r\n        denom = np.maximum(Phi_L[is_censored_low], 1e-10)\r\n        grad[is_censored_low] = (phi_L[is_censored_low] / denom) / sigma\r\n        term = z_L[is_censored_low] * phi_L[is_censored_low] / denom\r\n        hess[is_censored_low] = (term + (phi_L[is_censored_low] / denom)**2) / (sigma**2)\r\n        \r\n    # 2. Observed\r\n    if is_observed.any():\r\n        grad[is_observed] = -z_y[is_observed] / sigma\r\n        \r\n    # 3. Upper Censored\r\n    if is_censored_high.any():\r\n        denom = np.maximum(1 - Phi_H[is_censored_high], 1e-10)\r\n        grad[is_censored_high] = - (phi_H[is_censored_high] / denom) / sigma\r\n        term = -z_H[is_censored_high] * phi_H[is_censored_high] / denom\r\n        hess[is_censored_high] = (term + (phi_H[is_censored_high] / denom)**2) / (sigma**2)\r\n        \r\n    # Stability Clipping\r\n    hess = np.clip(hess, 1e-6, 1e2)\r\n    return grad, hess","metadata":{"_uuid":"d2325910-57a4-438b-8d2f-8280fb2f381b","_cell_guid":"537e665b-c143-4486-9a45-8edb59be773d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-18T12:51:28.362587Z","iopub.execute_input":"2026-01-18T12:51:28.362954Z","iopub.status.idle":"2026-01-18T12:51:28.371654Z","shell.execute_reply.started":"2026-01-18T12:51:28.362931Z","shell.execute_reply":"2026-01-18T12:51:28.371028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Feature Engineering\r\n\r\nIncorporating robust features inspired by top public kernels:\r\n- **Magic Formula**: Linear combination known to approximate the target.\r\n- **Polynomials & Logs**: Capturing non-linear trends.\r\n- **Category Mean Transformer (CMT)**: Target encoding that respects ordinality (Fold-safe).","metadata":{"_uuid":"1e4b50ff-7e22-4886-a72d-a7fe74b8087d","_cell_guid":"30c8e260-e0e3-4416-a4d1-76adc9249e87","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class CategoryMeanTransformer:\r\n    def __init__(self, cat_cols):\r\n        self.cat_cols = cat_cols\r\n        self.mappings_ = {}\r\n    def fit(self, X, y):\r\n        for col in self.cat_cols:\r\n            df_temp = pd.DataFrame({col: X[col], 'target': y})\r\n            group_means = df_temp.groupby(col)['target'].mean()\r\n            sorted_cats = group_means.sort_values().index\r\n            self.mappings_[col] = {cat: i for i, cat in enumerate(sorted_cats)}\r\n        return self\r\n    def transform(self, X):\r\n        X_copy = X.copy()\r\n        for col in self.cat_cols:\r\n            X_copy[col] = X_copy[col].map(self.mappings_[col]).fillna(-1).astype(int)\r\n        return X_copy\r\n\r\ndef fe_basic(df):\r\n    df = df.copy()\r\n    # Basic Expansions\r\n    df['study_sq'] = df['study_hours'] ** 2\r\n    df['attend_sq'] = df['class_attendance'] ** 2\r\n    df['sleep_sq'] = df['sleep_hours'] ** 2\r\n    df['log_study'] = np.log1p(df['study_hours'].clip(lower=0))\r\n    \r\n    # Efficiency & Ratios\r\n    df['efficiency'] = (df['study_hours'] * df['class_attendance']) / (df['sleep_hours'] + 1)\r\n    \r\n    # Binning\r\n    df['attend_bin'] = pd.cut(df['class_attendance'], bins=5, labels=False).astype(float)\r\n    \r\n    return df","metadata":{"_uuid":"74feec87-cb27-430c-aecb-4effe603761e","_cell_guid":"91d04d13-ec44-42df-83d9-03a430a8603a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-18T12:51:28.37239Z","iopub.execute_input":"2026-01-18T12:51:28.372597Z","iopub.status.idle":"2026-01-18T12:51:28.392324Z","shell.execute_reply.started":"2026-01-18T12:51:28.372579Z","shell.execute_reply":"2026-01-18T12:51:28.391715Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Data Loading & Preparation","metadata":{"_uuid":"e172c309-549e-47ca-88cd-02ee23c2d5ec","_cell_guid":"7f9b04cb-bc04-40b0-bc86-be95cb64592c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"train_df = pd.read_csv(CFG.TRAIN_PATH)\r\ntest_df = pd.read_csv(CFG.TEST_PATH)\r\nsubmission_df = pd.read_csv(CFG.SUB_PATH)\r\n\r\nCATS = train_df.select_dtypes(\"object\").columns.tolist()\r\nX_raw = fe_basic(train_df)\r\nX_test_raw = fe_basic(test_df)\r\ny = train_df[CFG.TARGET].values\r\nnum_features = [c for c in X_raw.columns if c not in CATS + [CFG.TARGET, CFG.ID_COL]]\r\n\r\nprint(f\"Data Loaded. Train: {X_raw.shape}, Test: {X_test_raw.shape}\")","metadata":{"_uuid":"367f8368-9be9-49ab-a286-f3413e2291e2","_cell_guid":"6a57f889-870c-4723-a01f-d32b759dda79","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-18T12:51:28.393901Z","iopub.execute_input":"2026-01-18T12:51:28.394213Z","iopub.status.idle":"2026-01-18T12:51:29.946094Z","shell.execute_reply.started":"2026-01-18T12:51:28.394186Z","shell.execute_reply":"2026-01-18T12:51:29.945228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Stage 1: RidgeCV Skeleton\r\n\r\nCreating a strong linear meta-feature using OOF predictions.  \r\n*Note: We apply CMT within each fold to prevent leakage.*","metadata":{"_uuid":"991362b3-fb6d-4de2-bc3f-5a359c1c5ca2","_cell_guid":"6bce6092-89e7-4fed-b37d-62c734c2fba2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Training Ridge Stage 1...\")\r\nkf = KFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.SEED)\r\noof_ridge = np.zeros(len(X_raw))\r\ntest_ridge_accum = np.zeros(len(X_test_raw))\r\n\r\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_raw, y), 1):\r\n    X_tr, X_val = X_raw.iloc[train_idx], X_raw.iloc[val_idx]\r\n    y_tr, y_val = y[train_idx], y[val_idx]\r\n    \r\n    # Fold-wise Encoding\r\n    cmt = CategoryMeanTransformer(CATS).fit(X_tr, y_tr)\r\n    scaler = StandardScaler()\r\n    \r\n    def transform_fold(df):\r\n        num = df[num_features].fillna(0)\r\n        cat = cmt.transform(df[CATS])\r\n        return scaler.transform(pd.concat([num, cat], axis=1)) if hasattr(scaler, 'mean_') else pd.concat([num, cat], axis=1)\r\n    \r\n    X_tr_p = transform_fold(X_tr)\r\n    scaler.fit(X_tr_p) # Fit scaler on train fold\r\n    X_tr_p = scaler.transform(X_tr_p)\r\n    X_val_p = transform_fold(X_val)\r\n    X_test_p = transform_fold(X_test_raw)\r\n    \r\n    # Ridge Training\r\n    ridge = RidgeCV(alphas=np.logspace(-3, 3, 10))\r\n    ridge.fit(X_tr_p, y_tr)\r\n    \r\n    oof_ridge[val_idx] = ridge.predict(X_val_p)\r\n    test_ridge_accum += ridge.predict(X_test_p) / CFG.N_FOLDS\r\n\r\nprint(\"Ridge Stage 1 Complete.\")","metadata":{"_uuid":"c401de72-06c2-4c76-90e2-af8f10c6e961","_cell_guid":"50fe427f-9c7d-45c3-a36f-ace056a8a7a5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-18T12:51:29.946913Z","iopub.execute_input":"2026-01-18T12:51:29.94712Z","iopub.status.idle":"2026-01-18T12:51:53.305874Z","shell.execute_reply.started":"2026-01-18T12:51:29.947102Z","shell.execute_reply":"2026-01-18T12:51:53.304841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Stage 2: XGBoost with Tobit Objective\r\n\r\nWe feed the Ridge OOF predictions into XGBoost and train using the custom `tobit_objective`.","metadata":{"_uuid":"c637cb1a-f956-4488-913b-198ce343a3cd","_cell_guid":"2643c950-4442-494d-8376-13030d0b8f8e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\"Starting XGBoost Tobit CV training...\")\r\n\r\n# Add Meta-Feature\r\ndrop_cols = [CFG.TARGET, CFG.ID_COL]\r\nX_raw['ridge_pred'] = oof_ridge\r\nX_test_raw['ridge_pred'] = test_ridge_accum\r\n\r\n# Categorical Setup for XGB\r\nfor df in [X_raw, X_test_raw]:\r\n    for col in CATS:\r\n        df[col] = df[col].astype(str).astype('category')\r\n\r\nxgb_params = {\r\n    'learning_rate': 0.005,\r\n    'max_depth': 9,\r\n    'subsample': 0.8,\r\n    'colsample_bytree': 0.6,\r\n    'tree_method': 'hist',\r\n    'device': 'cuda',\r\n    'enable_categorical': True,\r\n    'random_state': CFG.SEED\r\n}\r\n\r\noof_tobit_raw = np.zeros(len(X_raw))\r\ntest_tobit_accum_raw = np.zeros(len(X_test_raw))\r\nrmse_fold_list = []\r\n\r\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X_raw, y), 1):\r\n    X_tr, X_val = X_raw.iloc[tr_idx].copy(), X_raw.iloc[val_idx].copy()\r\n    y_tr, y_val = y[tr_idx], y[val_idx]\r\n    \r\n    X_tr_final = X_tr.drop(columns=drop_cols, errors='ignore')\r\n    X_val_final = X_val.drop(columns=drop_cols, errors='ignore')\r\n    X_test_final = X_test_raw.drop(columns=drop_cols, errors='ignore')\r\n    \r\n    dtrain = xgb.DMatrix(X_tr_final, label=y_tr, enable_categorical=True)\r\n    dval = xgb.DMatrix(X_val_final, label=y_val, enable_categorical=True)\r\n    dtest = xgb.DMatrix(X_test_final, enable_categorical=True)\r\n    \r\n    # Train with Custom Objective\r\n    model = xgb.train(params=xgb_params, dtrain=dtrain, num_boost_round=10000,\r\n                      obj=tobit_objective, evals=[(dval, 'val')],\r\n                      early_stopping_rounds=100, verbose_eval=1000)\r\n    \r\n    # Predict (Output is Latent Variable z)\r\n    oof_tobit_raw[val_idx] = model.predict(dval)\r\n    test_tobit_accum_raw += model.predict(dtest) / CFG.N_FOLDS\r\n    \r\n    # Evaluation (using Bayes Optimal for score check)\r\n    oof_tobit_bayes = bayes_optimal_pred(oof_tobit_raw[val_idx], CFG.Y_L, CFG.Y_H, CFG.SIGMA)\r\n    fold_rmse = np.sqrt(mean_squared_error(y_val, oof_tobit_bayes))\r\n    rmse_fold_list.append(fold_rmse)\r\n    \r\n    print(f\"Fold {fold} RMSE (Bayes): {fold_rmse:.5f}\")\r\n    gc.collect()","metadata":{"_uuid":"18f34042-8285-4b72-ab0d-a832c3502e01","_cell_guid":"168a1370-168a-4606-8860-e99b01104a71","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-18T12:51:53.306582Z","iopub.execute_input":"2026-01-18T12:51:53.306828Z","execution_failed":"2026-01-18T12:52:35.289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Bayes-Optimal Transformation & Results\r\n\r\nThe raw output from the Tobit model is the latent variable $z$.  \r\nWe convert this to the expected value using `bayes_optimal_pred` function, which statistically accounts for the censoring.","metadata":{"_uuid":"a97ff1d1-1007-42dd-8011-538a7bfa8348","_cell_guid":"bb183004-46e4-4a01-bcda-0c75f87a31ea","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Convert Latent z -> Expected y\r\noof_tobit_bayes = bayes_optimal_pred(oof_tobit_raw, CFG.Y_L, CFG.Y_H, CFG.SIGMA)\r\ntest_tobit_bayes = bayes_optimal_pred(test_tobit_accum_raw, CFG.Y_L, CFG.Y_H, CFG.SIGMA)\r\n\r\n# Clip for safety (though Bayes pred naturally stays within bounds mostly)\r\noof_tobit_bayes = np.clip(oof_tobit_bayes, CFG.Y_L, CFG.Y_H)\r\ntest_tobit_bayes = np.clip(test_tobit_bayes, CFG.Y_L, CFG.Y_H)\r\n\r\nfinal_oof_rmse = np.sqrt(mean_squared_error(y, oof_tobit_bayes))\r\nprint(f\"\\n=============================================\")\r\nprint(f\"FINAL OOF RMSE (Bayes-optimal): {final_oof_rmse:.5f}\")\r\nprint(f\"=============================================\")","metadata":{"_uuid":"e66c31d3-c26a-4b16-9dfe-7ec8d9ec5bd9","_cell_guid":"d2099fb3-3a38-4205-b6d8-a9377b1058ab","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2026-01-18T12:52:35.289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Visualizations: The Power of Tobit\r\n\r\nNotice how the **Residual Histogram** and **Prediction Distribution** behave.  \r\nTobit handles the peaks at 19.6 and 100.0 by design, creating a distinct distribution compared to standard regression.","metadata":{"_uuid":"c62e6647-071f-40af-80f5-6f13b65bd668","_cell_guid":"323c3697-28c2-409e-a54d-b974275276e6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(14, 10))\r\n\r\n# 1. Fold RMSE\r\naxes[0, 0].plot(range(1, CFG.N_FOLDS+1), rmse_fold_list, marker='o', color='tab:blue')\r\naxes[0, 0].set_xlabel(\"Fold\")\r\naxes[0, 0].set_ylabel(\"RMSE\")\r\naxes[0, 0].set_title(\"Fold-wise RMSE (Bayes-optimal)\")\r\n\r\n# 2. Predicted vs True\r\naxes[0, 1].scatter(y, oof_tobit_bayes, alpha=0.1, s=1, color='tab:orange')\r\naxes[0, 1].plot([CFG.Y_L, CFG.Y_H], [CFG.Y_L, CFG.Y_H], 'r--', label=\"y=x\")\r\naxes[0, 1].set_xlabel(\"True Score\")\r\naxes[0, 1].set_ylabel(\"Predicted\")\r\naxes[0, 1].set_title(\"OOF Prediction vs True Target\")\r\naxes[0, 1].legend()\r\n\r\n# 3. Residuals\r\naxes[1, 0].hist(oof_tobit_bayes - y, bins=100, color='tab:green', alpha=0.7)\r\naxes[1, 0].set_title(\"Residual Distribution\")\r\naxes[1, 0].set_xlabel(\"Error\")\r\n\r\n# 4. Test Predictions Dist\r\naxes[1, 1].hist(test_tobit_bayes, bins=100, color='tab:purple', alpha=0.7, label='Test Preds')\r\naxes[1, 1].axvline(CFG.Y_L, color='red', linestyle=':', label='Min (19.6)')\r\naxes[1, 1].axvline(CFG.Y_H, color='red', linestyle=':', label='Max (100.0)')\r\naxes[1, 1].set_title(\"Test Predictions Distribution\")\r\naxes[1, 1].legend()\r\n\r\nplt.tight_layout()\r\nplt.show()","metadata":{"_uuid":"90063ec8-a9eb-4b63-a197-ee10ef1830e5","_cell_guid":"62704d7e-8389-4a17-a86b-23b89b88a962","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2026-01-18T12:52:35.289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Submission\r\n\r\nSaving the OOF and Submission files.","metadata":{"_uuid":"9ada23ad-fa65-43b9-8490-8f90823ca9dc","_cell_guid":"0b1f3eed-7369-44a1-baea-f5391ca6179a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Save OOF\r\noof_df = pd.DataFrame({\r\n    CFG.ID_COL: train_df[CFG.ID_COL],\r\n    CFG.TARGET: oof_tobit_bayes\r\n})\r\noof_df.to_csv(\"oof_predictions.csv\", index=False)\r\n\r\n# Save Submission\r\nsubmission_df[CFG.TARGET] = test_tobit_bayes\r\nsubmission_df.to_csv(\"submission.csv\", index=False)\r\n\r\nprint(\"âœ… Files saved successfully.\")","metadata":{"_uuid":"83d02561-0421-4451-b8a0-4c341b4c295c","_cell_guid":"fb0b232b-e750-4b6e-867b-12c9683dcc89","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2026-01-18T12:52:35.289Z"}},"outputs":[],"execution_count":null}]}