{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":292302717,"sourceType":"kernelVersion"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook we let Optuna optimize a few hyperparameters of an XGBoost model that will be used to make a submission to the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/competitions/home-data-for-ml-course/data).\n\n# Data Preparation\n\nThe following code cell checks that the required inputs are available.","metadata":{}},{"cell_type":"code","source":"# Load helpful packages\nimport numpy as np\nimport pandas as pd\nimport ames_housing_utils as utils\n\n# List data files\nutils.list_input()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:36:09.04887Z","iopub.execute_input":"2026-01-18T06:36:09.049102Z","iopub.status.idle":"2026-01-18T06:36:09.403246Z","shell.execute_reply.started":"2026-01-18T06:36:09.049078Z","shell.execute_reply":"2026-01-18T06:36:09.402455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can now load the training set and convert it into the `DMatrix` that is expected by the [XGBoost cross validation function](https://xgboost.readthedocs.io/en/latest/r_docs/R-package/docs/reference/xgb.cv.html). We enable support for categorical data.","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# Load and display training set\nX_train, y_train = utils.load_train()\ndisplay(pd.concat([X_train, y_train], axis=1))\n\n# Convert training set into DMatrix\ndtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:36:09.404353Z","iopub.execute_input":"2026-01-18T06:36:09.40484Z","iopub.status.idle":"2026-01-18T06:36:10.750235Z","shell.execute_reply.started":"2026-01-18T06:36:09.4048Z","shell.execute_reply":"2026-01-18T06:36:10.749579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optuna Study\n\nFirst we define the objectives of the optimization:\n\n0. **Mean Absolute Error (MAE)** for accuracy\n1. **Standard Error (SE)** for robustness\n\nWe'll let Optuna search for the best tree constraints (`max_depth`, `min_child_weight`) and stochasticity (`subsample`, `colsample_bytree`) given a fixed `learning_rate`. We'll rely on early stopping to find the best number of trees.","metadata":{}},{"cell_type":"code","source":"FIXED_PARAMS = {\n        \"objective\": \"reg:absoluteerror\",\n        \"tree_method\": \"hist\",  # required for categorical support\n        \"learning_rate\": 0.05\n}\n\ndef objective(trial):\n    params = {\n        **FIXED_PARAMS,\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.01),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, step=0.01),\n    }\n\n    cv_results = xgb.cv(\n        params=params,\n        dtrain=dtrain,\n        num_boost_round=5000,\n        nfold=5,\n        early_stopping_rounds=50,\n        metrics=\"mae\",\n        seed=42\n    )\n    trial.set_user_attr(\"num_boost_round\", cv_results.shape[0])\n\n    last_round = cv_results.iloc[-1]\n    return last_round['test-mae-mean'], last_round['test-mae-std'] / (5**0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:36:10.750999Z","iopub.execute_input":"2026-01-18T06:36:10.751232Z","iopub.status.idle":"2026-01-18T06:36:10.758241Z","shell.execute_reply.started":"2026-01-18T06:36:10.751209Z","shell.execute_reply":"2026-01-18T06:36:10.757293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we can perform the actual study. For a warm start, we enqueue the result of previous tuning efforts.","metadata":{}},{"cell_type":"code","source":"import optuna\n\nstudy = optuna.create_study(directions=[\"minimize\", \"minimize\"])\nstudy.enqueue_trial({\n    'max_depth': 5,\n    'min_child_weight': 18,\n    'subsample': 0.64,\n    'colsample_bytree': 0.57\n})\n\n# Prepare warm start\n%time study.optimize(objective, n_trials=50, show_progress_bar=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:36:10.759377Z","iopub.execute_input":"2026-01-18T06:36:10.75974Z","iopub.status.idle":"2026-01-18T06:37:08.668394Z","shell.execute_reply.started":"2026-01-18T06:36:10.759711Z","shell.execute_reply":"2026-01-18T06:37:08.667321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we can display a scatter plot with the Pareto front of best trials.","metadata":{}},{"cell_type":"code","source":"import plotly.io as pio\npio.renderers.default = \"iframe\"\n\noptuna.visualization.plot_pareto_front(study).show()","metadata":{"execution":{"iopub.status.busy":"2026-01-18T06:37:08.669466Z","iopub.execute_input":"2026-01-18T06:37:08.669809Z","iopub.status.idle":"2026-01-18T06:37:11.377955Z","shell.execute_reply.started":"2026-01-18T06:37:08.669783Z","shell.execute_reply":"2026-01-18T06:37:11.377137Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally, let's select the trial with the lowest $MAE + 2 \\times SE$ as the best trial and print the relevant properties.","metadata":{}},{"cell_type":"code","source":"best_trial = min(study.best_trials, key=lambda t: t.values[0] + (2 * t.values[1]))\nprint(f'Best trial: {best_trial.number}')\nprint(f'Best score: {best_trial.values[0]:.2f} +/- {best_trial.values[1]:.2f}')\n\nbest_params = best_trial.params\nprint(f'Best params: {best_params}')\n\nbest_iteration = best_trial.user_attrs['num_boost_round']\nprint(f'Best iteration: {best_iteration}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:37:11.380155Z","iopub.execute_input":"2026-01-18T06:37:11.38063Z","iopub.status.idle":"2026-01-18T06:37:11.387006Z","shell.execute_reply.started":"2026-01-18T06:37:11.380602Z","shell.execute_reply":"2026-01-18T06:37:11.386123Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Refit and Predict\n\nNow we can refit the XGBoost model to the full training set using the best parameters found in the study.","metadata":{}},{"cell_type":"code","source":"model = xgb.train(\n    params={**FIXED_PARAMS, **best_params},\n    dtrain=dtrain,\n    num_boost_round=best_iteration\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:37:11.388238Z","iopub.execute_input":"2026-01-18T06:37:11.388981Z","iopub.status.idle":"2026-01-18T06:37:13.311394Z","shell.execute_reply.started":"2026-01-18T06:37:11.388943Z","shell.execute_reply":"2026-01-18T06:37:13.310526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We conclude this notebook by preparing a submission to the competition.","metadata":{}},{"cell_type":"code","source":"# Define test matrix\nX_test, test_ids = utils.load_test()\ndtest = xgb.DMatrix(X_test, enable_categorical=True)\n\n# Make and save predictions\npreds = model.predict(dtest)\nutils.save_preds(test_ids, preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T06:37:13.312351Z","iopub.execute_input":"2026-01-18T06:37:13.312821Z","iopub.status.idle":"2026-01-18T06:37:13.440724Z","shell.execute_reply.started":"2026-01-18T06:37:13.312794Z","shell.execute_reply":"2026-01-18T06:37:13.43992Z"}},"outputs":[],"execution_count":null}]}