{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Top 5% Custom Neural Network solution\nHey! Welcome to the start of your Kaggle journey! Exciting!\n\nThroughout this notebook I tried to simplify as much as I could and make sure everything is straightforward.\n\nBy no means is this notebook perfect, I'm learning just as you are. \n\nAs further exercises, here are a few ideas to get you going inside this notebook:\n- The onehot encoding can be further optimized\n- Evaluation can be enhanced with Cross-Validation using KFold or other technique\n- Tweak and experiment with the Custom Multilayer Perceptron (neural network) hyperparameters such as:\n    - Amount of neurons per hidden layer\n    - Amount of hidden layers\n    - Regularization (eg. nn.Dropout)\n    - Different optimizer (eg. Adam, AdamW, RMSProp)\n\nHope you'll learn something from this notebook!\n\nGood luck! ðŸ˜„","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch import nn\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\n\ninput_dir = '/kaggle/input/titanic/'\ntarget = 'Survived'\n\nif torch.cuda.is_available():\n    device = 'cuda:0' # first GPU available\nelse:\n    device = 'cpu'\nprint(f'Using device: {device}')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:29.050282Z","iopub.execute_input":"2026-01-15T18:49:29.050752Z","iopub.status.idle":"2026-01-15T18:49:35.798509Z","shell.execute_reply.started":"2026-01-15T18:49:29.050723Z","shell.execute_reply":"2026-01-15T18:49:35.797871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing\nIn this custom function a couple of things are done:\n\n- Fill NaN/null values with '0'\n- Numerical Features get normalized\n- Categorical Features get onehot encoded","metadata":{}},{"cell_type":"code","source":"# use global scaler to preserve fit for test transform\nscaler = StandardScaler()\nonehot = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n\ndef prepare(df, scaler=scaler, onehot=onehot, test_set=False):\n    df = df.drop(columns=['PassengerId', target], errors='ignore')\n\n    numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n\n    df[numerical_features] = df[numerical_features].fillna(0)\n\n    # apply normalization to features (x - x.mean()) / x.std()\n    # for test set we only transform the data, otherwise we leak data and cheat = overly optimistic (inaccurate) evaluation\n    if test_set:\n        df[numerical_features] = scaler.transform(df[numerical_features])\n    else:\n        df[numerical_features] = scaler.fit_transform(df[numerical_features])\n\n    categorical_features = df.select_dtypes(exclude=np.number).columns.tolist()\n\n    if test_set:\n        encoded_features = onehot.transform(df[categorical_features])\n    else:\n        encoded_features = onehot.fit_transform(df[categorical_features])\n\n    # get names of new onehot columns, drop existing columns and replace with onehot columns\n    new_cols = onehot.get_feature_names_out()\n\n    encoded_df = pd.DataFrame(encoded_features, columns=new_cols, index=df.index)\n    \n    df = df.drop(columns=categorical_features)\n    df = pd.concat([df, encoded_df], axis=1)\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:35.799979Z","iopub.execute_input":"2026-01-15T18:49:35.800333Z","iopub.status.idle":"2026-01-15T18:49:35.806931Z","shell.execute_reply.started":"2026-01-15T18:49:35.800312Z","shell.execute_reply":"2026-01-15T18:49:35.806242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prepare train/valid splits","metadata":{}},{"cell_type":"code","source":"train_csv = Path(input_dir) / 'train.csv'\ndf_train = pd.read_csv(train_csv)\n\nX_train, X_valid, y_train, y_valid = train_test_split(df_train, df_train[target], test_size=0.2, random_state=42)\n\nprint(X_train.shape, X_valid.shape)\n\nX_train = prepare(X_train)\nX_valid = prepare(X_valid, test_set=True) # test_set=True for validation so we don't leak scaler info into validation set\n\nprint(f'X_train shape: {X_train.shape}')\nprint(f'y_train shape: {y_train.shape}')\nprint(f'X_valid shape: {X_valid.shape}')\nprint(f'y_valid shape: {y_valid.shape}')\nX_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:35.807597Z","iopub.execute_input":"2026-01-15T18:49:35.807857Z","iopub.status.idle":"2026-01-15T18:49:35.924958Z","shell.execute_reply.started":"2026-01-15T18:49:35.807838Z","shell.execute_reply":"2026-01-15T18:49:35.924203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prepare data for neural network","metadata":{}},{"cell_type":"code","source":"# unsqueeze(1) inserts a new axis at index 1. y_train shape (712,) -> (712, 1)\n# y_train becomes a column vector after unsqueeze, column vectors are needed for output layer of our custom MLP (inference): nn.Linear(64, 1)\nX_train = torch.tensor(X_train.values, dtype=torch.float32)\ny_train = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\nX_valid = torch.tensor(X_valid.values, dtype=torch.float32)\ny_valid = torch.tensor(y_valid.values, dtype=torch.float32).unsqueeze(1)\n\ntrain_ds = TensorDataset(X_train, y_train)\nvalid_ds = TensorDataset(X_valid, y_valid)\n\nbatch_size = 64\n\npin_memory = True if 'cuda' in device else False\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=pin_memory) # pin_memory speeds up data transfer from CPU to GPU\nvalid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, pin_memory=pin_memory) # we don't shuffle test/valid sets for reproducibility reasons","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:35.925795Z","iopub.execute_input":"2026-01-15T18:49:35.926411Z","iopub.status.idle":"2026-01-15T18:49:35.969634Z","shell.execute_reply.started":"2026-01-15T18:49:35.926383Z","shell.execute_reply":"2026-01-15T18:49:35.968798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Neural Network","metadata":{}},{"cell_type":"code","source":"class CustomMLP(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, 64),\n            nn.LeakyReLU(0.1),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:35.970512Z","iopub.execute_input":"2026-01-15T18:49:35.970786Z","iopub.status.idle":"2026-01-15T18:49:35.976436Z","shell.execute_reply.started":"2026-01-15T18:49:35.97076Z","shell.execute_reply":"2026-01-15T18:49:35.97582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"net = CustomMLP(X_train.shape[1]) # initialize with X_train input dimension","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:35.977214Z","iopub.execute_input":"2026-01-15T18:49:35.9777Z","iopub.status.idle":"2026-01-15T18:49:35.997467Z","shell.execute_reply.started":"2026-01-15T18:49:35.97767Z","shell.execute_reply":"2026-01-15T18:49:35.996841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(net, dataloader, num_epochs, lr, lr_period, lr_decay, momentum, device):\n    net = net.to(device)\n\n    loss = nn.BCELoss()\n    optim = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n\n    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=lr_period, gamma=lr_decay)\n    \n    for epoch in range(num_epochs):\n        net.train()\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            optim.zero_grad()\n            preds = net(X)\n            l = loss(preds, y)\n            l.backward()\n            optim.step()\n        scheduler.step()\n        if (epoch+1) % 5 == 0:\n            print(f'Epoch {epoch + 1} Loss: {l.item():.6f}')       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:35.999519Z","iopub.execute_input":"2026-01-15T18:49:35.999727Z","iopub.status.idle":"2026-01-15T18:49:36.00577Z","shell.execute_reply.started":"2026-01-15T18:49:35.999711Z","shell.execute_reply":"2026-01-15T18:49:36.005237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## X_train training","metadata":{}},{"cell_type":"code","source":"train(\n    net,\n    train_loader,\n    num_epochs=100,\n    lr=0.1,\n    momentum=0.9,\n    device=device,\n    lr_period=5,\n    lr_decay=0.99\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:36.006345Z","iopub.execute_input":"2026-01-15T18:49:36.006559Z","iopub.status.idle":"2026-01-15T18:49:41.488966Z","shell.execute_reply.started":"2026-01-15T18:49:36.006543Z","shell.execute_reply":"2026-01-15T18:49:41.488332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Network Accuracy Evaluation","metadata":{}},{"cell_type":"code","source":"def eval_acc(net, dataloader, device):\n    net.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            preds_raw = net(X)\n            preds = (preds_raw > 0.5).float() # convert probabilities to prediction, 0 or 1\n            correct += (preds == y).sum().item()\n            total += y.size(0)\n\n    return correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:41.489681Z","iopub.execute_input":"2026-01-15T18:49:41.49004Z","iopub.status.idle":"2026-01-15T18:49:41.494517Z","shell.execute_reply.started":"2026-01-15T18:49:41.490022Z","shell.execute_reply":"2026-01-15T18:49:41.493747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = eval_acc(net, valid_loader, device)\nprint(f'Validation accuracy: {acc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:41.495212Z","iopub.execute_input":"2026-01-15T18:49:41.495397Z","iopub.status.idle":"2026-01-15T18:49:41.554925Z","shell.execute_reply.started":"2026-01-15T18:49:41.495383Z","shell.execute_reply":"2026-01-15T18:49:41.554168Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Retrain Network on full dataset","metadata":{}},{"cell_type":"code","source":"X_train_full = prepare(df_train)\ny_train_full = df_train[target]\n\nX_train_full = torch.tensor(X_train_full.values, dtype=torch.float32)\ny_train_full = torch.tensor(y_train_full.values, dtype=torch.float32).unsqueeze(1)\n\nfull_trainset = TensorDataset(X_train_full, y_train_full)\nfull_loader = DataLoader(\n    full_trainset,\n    batch_size=batch_size,\n    shuffle=True,\n    pin_memory=True\n)\n\nnet = CustomMLP(X_train_full.shape[1]) # recreate model to reset pretrained weights and use X_train_full input dimension\n\ntrain(\n    net,\n    full_loader,\n    num_epochs=100,\n    lr=0.1,\n    momentum=0.9,\n    device=device,\n    lr_period=5,\n    lr_decay=0.99\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:41.555718Z","iopub.execute_input":"2026-01-15T18:49:41.556006Z","iopub.status.idle":"2026-01-15T18:49:44.6573Z","shell.execute_reply.started":"2026-01-15T18:49:41.555988Z","shell.execute_reply":"2026-01-15T18:49:44.656621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Make predictions and create submissions.csv","metadata":{}},{"cell_type":"code","source":"test_csv = Path(input_dir) / 'test.csv'\ndf_test = pd.read_csv(test_csv)\n\nX_test = prepare(df_test, test_set=True)\n\nX_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n\nwith torch.no_grad():\n    preds = net(X_test)\n    preds = (preds > 0.5).int().squeeze().cpu().numpy()\n\npassenger_ids = df_test['PassengerId']\n\nsubmission = pd.DataFrame({\n    'PassengerId': passenger_ids,\n    'Survived': preds\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:49:44.658079Z","iopub.execute_input":"2026-01-15T18:49:44.658291Z","iopub.status.idle":"2026-01-15T18:49:44.694677Z","shell.execute_reply.started":"2026-01-15T18:49:44.658274Z","shell.execute_reply":"2026-01-15T18:49:44.694043Z"}},"outputs":[],"execution_count":null}]}