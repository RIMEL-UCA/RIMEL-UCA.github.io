{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions_barcelona as fb\n",
    "#from functions_barcelona import posant_accents,utmToLatLng,\\\n",
    "      #              ped_to_angles,setmana_a_angles,mes_a_angles,cause_to_angles\n",
    "import datetime\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import date, datetime, timedelta\n",
    "#from functions_barcelona import getting_daily_weather,getting_next_day\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "import webbrowser\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accidents-causes-gu-bcn\n",
      "2020_ACCIDENTS_CAUSES_GU_BCN.csv CSV\n",
      "2019_ACCIDENTS_CAUSES_GU_BCN CSV\n",
      "2018_ACCIDENTS_CAUSES_GU_BCN CSV\n",
      "2017_ACCIDENTS_CAUSES_GU_BCN CSV\n",
      "2016_ACCIDENTS_CAUSES_GU_BCN_.csv CSV\n",
      "2015_ACCIDENTS_CAUSES_GU_BCN_.csv CSV\n",
      "2014_ACCIDENTS_CAUSES_GU_BCN_.csv CSV\n",
      "2013_ACCIDENTS_CAUSES_GU_BCN_.csv CSV\n",
      "2012_ACCIDENTS_CAUSES_GU_BCN_.csv CSV\n",
      "2011_ACCIDENTS_CAUSES_GU_BCN_.csv CSV\n",
      "2010_ACCIDENTS_CAUSES_GU_BCN_.csv CSV\n",
      "accidents-persones-gu-bcn\n",
      "2020_ACCIDENTS_PERSONES_GU_BCN.csv CSV\n",
      "2019_ACCIDENTS_PERSONES_GU_BCN.csv CSV\n",
      "2018_ACCIDENTS_PERSONES_GU_BCN CSV\n",
      "2017_ACCIDENTS_PERSONES_GU_BCN CSV\n",
      "2016_ACCIDENTS_PERSONES_GU_BCN_.csv CSV\n",
      "2015_ACCIDENTS_PERSONES_GU_BCN_.csv CSV\n",
      "2014_ACCIDENTS_PERSONES_GU_BCN_.csv CSV\n",
      "2013_ACCIDENTS_PERSONES_GU_BCN_.csv CSV\n",
      "2012_ACCIDENTS_PERSONES_GU_BCN_.csv CSV\n",
      "2011_ACCIDENTS_PERSONES_GU_BCN_.csv CSV\n",
      "2010_ACCIDENTS_PERSONES_GU_BCN_.csv CSV\n",
      "accidents-tipus-gu-bcn\n",
      "2020_ACCIDENTS_TIPUS_GU_BCN.csv CSV\n",
      "2019_ACCIDENTS_TIPUS_GU_BCN.csv CSV\n",
      "2018_ACCIDENTS_TIPUS_GU_BCN CSV\n",
      "2017_ACCIDENTS_TIPUS_GU_BCN CSV\n",
      "2016_ACCIDENTS_TIPUS_GU_BCN.csv CSV\n",
      "2015_ACCIDENTS_TIPUS_GU_BCN_.csv CSV\n",
      "2014_ACCIDENTS_TIPUS_GU_BCN_.csv CSV\n",
      "2013_ACCIDENTS_TIPUS_GU_BCN_.csv CSV\n",
      "2012_ACCIDENTS_TIPUS_GU_BCN_.csv CSV\n",
      "2011_ACCIDENTS_TIPUS_GU_BCN_.csv CSV\n",
      "2010_ACCIDENTS_TIPUS_GU_BCN_.csv CSV\n",
      "accidents-vehicles-gu-bcn\n",
      "2020_ACCIDENTS_VEHICLES_GU_BCN.csv CSV\n",
      "2019_ACCIDENTS_VEHICLES_GU_BCN.csv CSV\n",
      "2018_ACCIDENTS_VEHICLES_GU_BCN CSV\n",
      "2017_ACCIDENTS_VEHICLES_GU_BCN_.csv CSV\n",
      "2016_ACCIDENTS_VEHICLES_GU_BCN_.csv CSV\n",
      "2015_ACCIDENTS_VEHICLES_GU_BCN_.csv CSV\n",
      "2014_ACCIDENTS_VEHICLES_GU_BCN_.csv CSV\n",
      "2013_ACCIDENTS_VEHICLES_GU_BCN_.csv CSV\n",
      "2012_ACCIDENTS_VEHICLES_GU_BCN_.csv CSV\n",
      "2011_ACCIDENTS_VEHICLES_GU_BCN_.csv CSV\n",
      "2010_ACCIDENTS_VEHICLES_GU_BCN_.csv CSV\n",
      "accidents-gu-bcn\n",
      "2020_ACCIDENTS_GU_BCN.csv CSV\n",
      "2019_ACCIDENTS_GU_BCN.CSV CSV\n",
      "2018_accidents_gu_bcn.csv CSV\n",
      "2017_ACCIDENTS_GU_BCN.csv CSV\n",
      "2016_ACCIDENTS_GU_BCN.csv CSV\n",
      "2015_ACCIDENTS_GU_BCN_.csv CSV\n",
      "2014_ACCIDENTS_GU_BCN_.csv CSV\n",
      "2013_ACCIDENTS_GU_BCN_.csv CSV\n",
      "2012_ACCIDENTS_GU_BCN_.csv CSV\n",
      "2011_ACCIDENTS_GU_BCN_.csv CSV\n",
      "2010_ACCIDENTS_GU_BCN_.csv CSV\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('https://opendata-ajuntament.barcelona.cat/data/api/3/action/package_search?rows=1000')\n",
    "response= r.json()\n",
    "\n",
    "\n",
    "def concatenating_dataframes(filter1):\n",
    "    \n",
    "    files=response['result']['results']\n",
    "    for num, file in enumerate(files):\n",
    "        \n",
    "        if ('accidents-' +filter1+'gu') in file['name']:\n",
    "            print(file['name'])\n",
    "            filter_list=[]\n",
    "            \n",
    "            for fitxer in file['resources']:\n",
    "\n",
    "                if fitxer['format']=='CSV':\n",
    "\n",
    "                    print(fitxer['name'],fitxer['format'])\n",
    "                    #print(fitxer['url'])\n",
    "                    try:\n",
    "                        filter_list.append(pd.read_csv(fitxer['url']))\n",
    "                    except:\n",
    "                        try:\n",
    "                            filter_list.append(pd.read_csv(fitxer['url'],encoding='ISO-8859-15'))\n",
    "                        except:\n",
    "\n",
    "                            filter_list.append(pd.read_csv(fitxer['url'],sep=';',encoding='ISO-8859-1'))\n",
    "\n",
    "            column_names=filter_list[0].columns\n",
    "            data=pd.DataFrame()\n",
    "            for df in filter_list:\n",
    "                df=pd.DataFrame.from_records(df.values)\n",
    "                data=pd.concat([data,df])\n",
    "                #print(data.shape)\n",
    "            data.columns=column_names\n",
    "            data=data.reset_index()\n",
    "    try:\n",
    "        return data\n",
    "    except:\n",
    "        print('NO FILE WITH THAT FILTER')\n",
    "    print('CONCATENATING DONE')\n",
    "#causes_df= concatenating_dataframes('causes-')\n",
    "\n",
    "noms=['causes-', 'persones-','tipus-', 'vehicles-','']\n",
    "dict_noms={}\n",
    "for nom in noms:\n",
    "    dict_noms[nom[:-1]]=concatenating_dataframes(nom)\n",
    "\n",
    "pickle.dump( dict_noms, open( \"../data/dataframes_dict.pkl\", \"wb\" ) )\n",
    "#favorite_color = pickle.load( open( \"dataframes_dict.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning ACCIDENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordenada_UTM_Y\n",
      "Coordenada_UTM_X\n",
      "Number of nulls: 0\n",
      "I dropped Codi_districte\n",
      "(102995, 18)\n",
      "Accidents:  (102974, 19)\n",
      "(103121, 2)\n",
      "causes:  (102954, 2) Total:  (102974, 20)\n",
      "people:  (128436, 4) Total:  (102974, 36)\n",
      "type:  (102958, 2) Total:  (102974, 37)\n",
      "vehicles:  (93642, 6) Total:  (102974, 42)\n",
      "DONE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_noms = pickle.load( open( \"../data/dataframes_dict.pkl\", \"rb\" ) )\n",
    "acc= dict_noms[''].copy()\n",
    "\n",
    "#acc.columns\n",
    "index_to_drop=acc[acc.isnull().sum(axis=1)>10].index\n",
    "acc=acc.drop(index_to_drop)\n",
    "acc['Nom_carrer']=acc['Nom_carrer'].fillna('Casetes')\n",
    "for column in acc:\n",
    "    if 'Coor' in column:\n",
    "        print(column)\n",
    "        acc[column]=[str(coor).replace(',','.') for coor in acc[column]]\n",
    "        acc[column]= acc[column].astype(float)\n",
    "for column in acc:\n",
    "    acc[column]=acc[column].apply(fb.posant_accents)\n",
    "old_column_names=acc.columns\n",
    "acc=acc.drop(['Num_postal_caption','Longitud','Latitud','index','Codi_barri', 'Dia_setmana','Mes_any',\n",
    "             'Descripcio_torn', 'Descripcio_tipus_dia'],axis=1)\n",
    "print('Number of nulls:',acc.isnull().sum().sum())\n",
    "\n",
    "acc['Codi_districte']=[x if x!='Desconegut' else -1 for x in acc['Codi_districte']]\n",
    "acc['Codi_carrer']=acc['Codi_carrer'].astype(int)\n",
    "#acc['Codi_barri']=[str(x)[-2:].replace('-','') for x in acc.Codi_barri]\n",
    "tuple_features=[('Nom_carrer','Codi_carrer'),('Nom_districte','Codi_districte')]\n",
    "for tup in tuple_features:\n",
    "    if acc[tup[0]].nunique()==acc[tup[1]].nunique():\n",
    "        acc=acc.drop(tup[1],axis=1)\n",
    "        print(f\"I dropped {tup[1]}\")\n",
    "\n",
    "print(acc.shape)\n",
    "\n",
    "acc.columns=['num_incident', 'district', 'neighborhood', 'street_code',\n",
    "       'street_name', 'weekday', 'year', 'month', 'day', 'hour', 'ped_cause',\n",
    "       'num_deaths', 'num_minorly_injured', 'num_severly_injured',\n",
    "       'num_victims', 'num_vehicles', 'utm_y',\n",
    "       'utm_x']\n",
    "##Fixing utm_x and utm_y that are mixed in some cases. Replacing nulls(-1) with the mean\n",
    "acc['utm_x']=[x[0] if len(str(x[0]).split('.')[0])==7 else x[1]  if len(str(x[1]).split('.')[0])==7 else round(acc.utm_x.mean(),2) for x in zip(acc.utm_x,acc.utm_y)]\n",
    "acc['utm_y']=[x[1] if len(str(x[1]).split('.')[0])==6 else x[0]  if len(str(x[0]).split('.')[0])==6 else round(acc.utm_y.mean(),2) for x in zip(acc.utm_x,acc.utm_y)]\n",
    "\n",
    "\n",
    "##Translating to English\n",
    "\n",
    "acc['ped_cause']=acc['ped_cause'].apply(fb.ped_to_angles)\n",
    "acc['weekday']=acc['weekday'].apply(fb.setmana_a_angles)\n",
    "acc['month']=acc['month'].apply(fb.mes_a_angles)\n",
    "acc['num_incident']=[x.strip() for x in acc['num_incident']]\n",
    "for col in ['year','day','hour','num_deaths','num_minorly_injured','num_severly_injured','num_victims','num_vehicles']:\n",
    "    acc[col]=acc[col].astype(int)\n",
    "##eliminatong duplictaes that have 2 ped cause: I am losing the second one.\n",
    "index_to_drop= acc[acc['num_incident'].duplicated()].index\n",
    "acc=acc.drop(index_to_drop).reset_index()\n",
    "acc.to_csv('../data/accidents_only2020.csv')\n",
    "print('Accidents: ', acc.shape)\n",
    "\n",
    "##adding causes\n",
    "causes= dict_noms['causes'].reset_index().copy()\n",
    "\n",
    "\n",
    "columns_to_keep=['Descripcio_causa_mediata', 'Numero_expedient']\n",
    "\n",
    "causes=causes[columns_to_keep]\n",
    "causes.columns=['cause','num_incident']\n",
    "causes['num_incident']=[x.strip() for x in causes['num_incident']]\n",
    "causes['cause']=causes.cause.apply(fb.posant_accents).apply(fb.cause_to_angles)\n",
    "print(causes.shape)\n",
    "causes=causes.drop_duplicates('num_incident')\n",
    "\n",
    "\n",
    "causes['cause']=causes['cause'].fillna('unknown')\n",
    "causes.to_csv('../data/causes2020.csv')\n",
    "total= pd.merge(acc,causes, how='left',on='num_incident')\n",
    "\n",
    "total['cause']=total.cause.fillna('unknown')\n",
    "\n",
    "print('causes: ', causes.shape, 'Total: ',total.shape)\n",
    "\n",
    "\n",
    "\n",
    "##people\n",
    "\n",
    "people= dict_noms['persones'].reset_index().copy()\n",
    "\n",
    "columns_to_add=['Numero_Expedient','Desc_Tipus_vehicle_implicat', 'Descripcio_sexe', 'Edat',\n",
    "                'Descripcio_tipus_persona', 'Descripcio_Lloc_atropellament_vianat',\n",
    "                'Descripcio_Motiu_desplaçament_vianant',\n",
    "                'Descripcio_Motiu_desplaçament_conductor', 'Descripcio_victimitzacio',]\n",
    "people.columns=[fb.posant_accents(col) for col in people.columns]\n",
    "anual_dict={}\n",
    "for any_ in sorted(people['NK_ Any'].unique()):\n",
    "    if any_ in [2010,2011,2012,2013]:\n",
    "        df=people[people['NK_ Any']==any_].copy()\n",
    "        df.rename(columns={'Edat':'Descripcio_tipus_persona_','Descripcio_tipus_persona':'Edat_',\\\n",
    "        'Descripcio_Motiu_desplaçament_conductor':'Coordenada_UTM_Y_',\n",
    "        'Descripcio_Motiu_desplaçament_vianant': 'Coordenada_UTM_X_',\n",
    "        'Descripcio_Lloc_atropellament_vianat':'Descripcio_victimitzacio_' },inplace=True)\n",
    "        df=df.drop(df.columns[-5:],axis=1)\n",
    "        #columnes=[]\n",
    "        df.columns=[col if col[-1]!='_' else col[:-1] for col in df.columns]\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "    elif any_ in [2014,2015]:\n",
    "        df=people[people['NK_ Any']==any_].copy()\n",
    "        df.rename(columns={'Descripcio_Motiu_desplaçament_vianant':'Coordenada_UTM_Y_','Descripcio_Lloc_atropellament_vianat':'Coordenada_UTM_X_',\\\n",
    "        'Descripcio_tipus_persona':'Descripcio_victimitzacio_',\n",
    "        'Descripcio_sexe': 'Descripcio_tipus_persona_',\n",
    "        'Desc_Tipus_vehicle_implicat':'Descripcio_sexe_',\n",
    "         'Descripcio_causa_vianant':  'Desc_Tipus_vehicle_implicat_'   },inplace=True)\n",
    "        df=df.drop(df.columns[-6:],axis=1)\n",
    "        df.columns=[col if col[-1]!='_' else col[:-1] for col in df.columns]\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "    elif any_ in [2016,2017,2018]:\n",
    "        df=people[people['NK_ Any']==any_].copy()\n",
    "        df.rename(columns={'Descripcio_Motiu_desplaçament_vianant':'Descripcio_victimitzacio_','Descripcio_Lloc_atropellament_vianat':'Descripcio_situacio',\n",
    "        },inplace=True)\n",
    "        df=df.drop(df.columns[-6:],axis=1)\n",
    "        df.columns=[col if col[-1]!='_' else col[:-1] for col in df.columns]\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "    elif any_ in [2019,2020]:\n",
    "        df=people[people['NK_ Any']==any_].copy()\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "\n",
    "mapping_columns={'Numero_Expedient':'num_incident',\n",
    "             'Desc_Tipus_vehicle_implicat':'vehicle',\n",
    "             'Descripcio_sexe':'gender',\n",
    "             'Edat':'age',\n",
    "            'Descripcio_tipus_persona':'people_role',\n",
    "             'Descripcio_Lloc_atropellament_vianat':'run_over_location',\n",
    "             'Descripcio_victimitzacio': 'level_injuries',\n",
    "              'Descripcio_Motiu_desplaçament_vianant':'peds_activity',\n",
    "            'Descripcio_Motiu_desplaçament_conductor':'drivers_activity'}\n",
    "people=pd.DataFrame()\n",
    "for key in anual_dict.keys():\n",
    "    #print(key, anual_dict[key].shape)\n",
    "    people= pd.concat([people,anual_dict[key].rename(columns=mapping_columns)])\n",
    "    #print(people.shape)\n",
    "    ##too many nulls.\n",
    "for col in people:\n",
    "    if people[col].isnull().sum()>10000:\n",
    "        people=people.drop(col,axis=1)\n",
    "people=people.drop('level_injuries',axis=1)\n",
    "people['num_incident']=[x.strip() for x in people['num_incident']]\n",
    "people['age']=people.age.astype(str)\n",
    "people['age']=[edat if edat!='Desconegut' else '-1' for edat in people.age]\n",
    "people['vehicle']=people.vehicle.map(fb.map_vehicles)\n",
    "people['gender']=people.gender.map({'Home':'M','Dona':\"F\",'Desconegut':'unknown'})\n",
    "people['people_role']=people.people_role.map({'Conductor':'driver','Passatger':'pass','Vianant':'ped','Desconegut':'unknown'})\n",
    "#compressed_people= people.groupby('num_incident').agg(lambda x : ','.join(x)).reset_index()\n",
    "people.to_csv('../data/people2020.csv')\n",
    "##Fixing people\n",
    "misc_vehicles=people.vehicle.value_counts()[people.vehicle.value_counts()<1000].index\n",
    "people['vehicles']=[veh if veh not in misc_vehicles else 'misc_vehicle' for veh in people.vehicle]\n",
    "\n",
    "if 'vehicle' in list(people.columns):\n",
    "    people=people.drop('vehicle',axis=1)\n",
    "people=people.applymap(lambda x: np.nan if x in ['Unknown','-1'] else x)\n",
    "people['gender']=people.gender.map({'M':0,'F':1})\n",
    "people['age']=people['age'].astype(float)\n",
    "people=people.set_index('num_incident')\n",
    "people2=pd.get_dummies(people)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "people2 = pd.DataFrame(scaler.fit_transform(people2), columns = people2.columns)\n",
    "\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "people2 = pd.DataFrame(imputer.fit_transform(people2),columns = people2.columns)\n",
    "\n",
    "\n",
    "people2=pd.DataFrame(scaler.inverse_transform(people2), columns = people2.columns)\n",
    "people2['gender_100/1']=[100 if gen<0.5 else 1 for gen in people2.gender]\n",
    "people2['age_driver']=people2.age*people2.people_role_driver\n",
    "people2['gender_driver']=people2['gender_100/1']*people2.people_role_driver\n",
    "if 'gender' in list(people2.columns):\n",
    "    people2=people2.drop('gender',axis=1)\n",
    "people2=people2.set_index(people.index)\n",
    "people2=people2.reset_index().groupby('num_incident').sum()\n",
    "people2['age_driver']=[x[0]/x[1] if x[1]!=0 else 0 for x in zip(people2['age_driver'],people2['people_role_driver'])]\n",
    "people2['gender_driver_male']=[int(str(gen)[0]) if len(str(gen))==3 else 0 for gen in people2.gender_driver.astype(int)]\n",
    "people2['gender_driver_female']=[int(str(gen)[2]) if len(str(gen))==3 else gen for gen in people2.gender_driver.astype(int)]\n",
    "people2=people2.drop(['age','gender_100/1','gender_driver'],axis=1)\n",
    "\n",
    "total= pd.merge(total,people2, how='left',on='num_incident')\n",
    "total['age_driver']=total.age_driver.fillna(total.age_driver.mean())\n",
    "total=total.fillna(0)\n",
    "#total=total.fillna(-1)\n",
    "total3=total.copy()\n",
    "\n",
    "total.sample(5)\n",
    "print('people: ', people.shape, 'Total: ',total.shape)\n",
    "\n",
    "##TYPE\n",
    "tipus=dict_noms['tipus'].copy()\n",
    "type_accident_map={'Atropellament': 'run_over',\n",
    "         'Col.lisio lateral': 'lateral_collision',\n",
    "        'Xoc contra element estatic': 'crash_into_stationary',\n",
    "      'Abast': 'rear-end_collision',\n",
    "       'Col.lisio frontal':'frontal_collision',\n",
    "      'Col.lisio fronto-lateral':'frontal-lateral_collision',\n",
    "      'Caiguda (dues rodes)':'fall--motorcycle',\n",
    "      'Abast multiple':'multiple_rear-end_collision',\n",
    "      'Caiguda interior vehicle':'fall_inside_vehicle',\n",
    "      'Altres':'Other_types',\n",
    "      'Bolcada (mes de dues rodes)':'overturning',\n",
    "      'Desconegut':'unknown',\n",
    "      'Sortida de via amb xoc o col.lisio':'run-off_with_crash_or_collision',\n",
    "      'Encalç':'rear-end_collision',\n",
    "      'Sortida de via amb bolcada':'run-off_with_overturning',\n",
    "      'Xoc amb animal a la calçada':'crash_into_animal_on_road',\n",
    "      'Resta sortides de via':'run-off_not_included_previously'}\n",
    "\n",
    "tipus['Descripcio_tipus_accident']=tipus['Descripcio_tipus_accident'].apply(fb.posant_accents).map(type_accident_map)\n",
    "tipus=tipus[['Numero_expedient','Descripcio_tipus_accident',]].copy()\n",
    "tipus.columns=['num_incident','accident_type']\n",
    "tipus['num_incident']=[x.strip() for x in tipus.num_incident]\n",
    "tipus=tipus.dropna()\n",
    "misc_type=list(tipus.accident_type.value_counts()[tipus.accident_type.value_counts()<350].index)\n",
    "misc_type.append('Other_types')\n",
    "tipus['accident_type']=[\"misc_type\" if x in misc_type else 'collision' if 'collision' in x else 'crash' if 'crash' in x else x for x in tipus.accident_type]\n",
    "\n",
    "tipus=tipus.groupby('num_incident')['accident_type'].agg(lambda x: ','.join(x)).reset_index()\n",
    "type_list=tipus.accident_type.value_counts().index\n",
    "tipus['accident_type']=[fb.organizing_types(x,type_list) for x in  tipus.accident_type]\n",
    "#tipus2=tipus.copy()\n",
    "total= pd.merge(total,tipus, how='left',on='num_incident')\n",
    "total.fillna('misc_type',inplace=True)\n",
    "total4=total.copy()\n",
    "tipus.to_csv('../data/types2020.csv')\n",
    "\n",
    "print('type: ', tipus.shape, 'Total: ',total.shape)\n",
    "\n",
    "##VEHICLE***Model no surt a tots els anys\n",
    "\n",
    "vehicles= dict_noms['vehicles'].copy()\n",
    "columns_to_add=['Numero_expedient', 'Descripcio_model', 'Descripcio_marca',\n",
    "       'Descripcio_color', 'Descripcio_carnet', 'Antiguitat_carnet' ]\n",
    "\n",
    "anual_dict={}\n",
    "\n",
    "for any_ in sorted(vehicles['NK_Any'].unique()):\n",
    "    if any_ in [2010,2011,2013,2014,2015,2016,2017]:\n",
    "        df=vehicles[vehicles['NK_Any']==any_].copy()\n",
    "        df.rename(columns={'Descripcio_tipus_vehicle':'Descripcio_model_','Descripcio_model':'Descripcio_marca_',\\\n",
    "        'Descripcio_marca':'Descripcio_color_',\n",
    "        'Descripcio_color': 'Descripcio_carnet_',\n",
    "        'Descripcio_carnet':'Antiguitat_carnet_' },inplace=True)\n",
    "        df=df.drop(df.columns[-5:],axis=1)\n",
    "        df.columns=[col if col[-1]!='_' else col[:-1] for col in df.columns]\n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "    elif any_ in [2018,2019,2020]:\n",
    "        df=vehicles[vehicles['NK_Any']==any_].copy()        \n",
    "        columnes=[col for col in df.columns if col in columns_to_add]\n",
    "        df=df[columnes].copy()\n",
    "        anual_dict[any_]=df\n",
    "\n",
    "vehicles=pd.DataFrame()\n",
    "for key in anual_dict.keys():\n",
    "    vehicles=pd.concat([vehicles,anual_dict[key]],)\n",
    "    \n",
    "\n",
    "vehicles.columns=['num_incident', 'vehicle_model', 'vehicle_brand',\n",
    "       'vehicle_color', 'type_license', 'seniority_license']\n",
    "\n",
    "vehicles=vehicles.dropna()\n",
    "vehicles['num_incident']=[x.strip() for x in vehicles.num_incident]\n",
    "vehicles2=vehicles.copy()\n",
    "vehicles=vehicles.groupby('num_incident').agg(lambda x: ','.join(x)).reset_index()\n",
    "vehicles.to_csv('../data/vehicles2020.csv')\n",
    "total= pd.merge(total,vehicles, how='left',on='num_incident')\n",
    "##I will imput the most often\n",
    "total=total.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
    "\n",
    "total.to_csv('../data/accidents2020.csv')\n",
    "#total.fillna(-1,inplace=True)\n",
    "print('vehicles: ', vehicles.shape, 'Total: ',total.shape)\n",
    "print('DONE')\n",
    "url = \"https://www.youtube.com/watch?v=Udt-9J8nzGE\"\n",
    "webbrowser.open(url,new=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##REPEAT THE PROCESS FOR EACH YEAR----I have to limit the number of calls therefore I have to do it by year\n",
    "\n",
    "# import datetime\n",
    "# start_date = datetime.date(2020, 1, 1)\n",
    "# end_date = datetime.date(2020, 12, 31)\n",
    "# delta = datetime.timedelta(days=1)\n",
    "# df_total = []\n",
    "# while start_date <= end_date:\n",
    "#     print(start_date)\n",
    "#     df= getting_daily_weather(str(start_date))\n",
    "#     df_total.append(df)\n",
    "#     start_date += delta\n",
    "# pd.concat(df_total).to_csv('./data/weather2020.csv')\n",
    "weather_df=pd.DataFrame()\n",
    "\n",
    "##Combining every year\n",
    "\n",
    "for file in os.listdir('../data/'):\n",
    "    if 'weather' in file:\n",
    "        #print(file)#,pd.read_csv('../data/'+file).shape,pd.read_csv('../data/'+file).columns)\n",
    "        weather_df=pd.concat([weather_df,pd.read_csv('../data/'+file)])\n",
    "weather_df=weather_df.sort_values('time')\n",
    "weather_df=weather_df.reset_index(drop=True)\n",
    "for col in weather_df:\n",
    "    if 'Unnamed' in col:\n",
    "        weather_df=weather_df.drop(col,axis=1)\n",
    "weather_df['time']=pd.to_datetime(weather_df.time)\n",
    "weather_df=weather_df.drop_duplicates('time')\n",
    "weather_df=weather_df.loc[:,weather_df.isnull().sum()<25]\n",
    "calendar_df=pd.DataFrame(pd.date_range(\"2010-01-01\", \"2021-01-01\", freq=\"H\"),columns=['calendar_date'])[:-1]\n",
    "weather_df=pd.concat([calendar_df,weather_df],axis=1)\n",
    "weather_df=weather_df.fillna(method='ffill')\n",
    "weather_df.columns=['weather_'+col if 'calendar' not in col else col for col in weather_df.columns]\n",
    "weather_df['weather_icon']=['partly-cloudy' if col.startswith('partly-cloudy') else 'clear' if col.startswith('clear') else 'rare' if\\\n",
    "                            col in ['fog','snow','sleet','wind'] else col for col in weather_df.weather_icon]\n",
    "weather_df['weather_deadly_temperature']=[1 if temp>55 and temp<70 else 0 for temp in weather_df.weather_temperature]\n",
    "weather_only=weather_df.copy()\n",
    "\n",
    "\n",
    "#MERGING WEATHER AND TOTAL\n",
    "\n",
    "total['month']=total['month'].apply(fb.mes_english_number)\n",
    "date_columns = ['month', 'day', 'year','hour']\n",
    "date = total[date_columns]\n",
    "dates = []\n",
    "datetimes=[]\n",
    "for i in date.itertuples():\n",
    "    dates.append(i[1]+'/' +str(int(i[2]))+'/'+str(int(i[3])))\n",
    "    datetimes.append(i[1]+'/' +str(int(i[2]))+'/'+str(int(i[3]))+' '+str(int(i[4]))+':00:00')\n",
    "\n",
    "total['dates']=dates\n",
    "total['datetimes']=datetimes\n",
    "total['dates']=pd.to_datetime(total.dates)\n",
    "total['datetimes']=pd.to_datetime(total.datetimes)\n",
    "total=pd.merge(total,weather_df,how='left', right_on='calendar_date',left_on='datetimes')\n",
    "total=total.drop(columns=['weather_time','calendar_date','weather_apparentTemperature'])\n",
    "total.to_csv('../data/weather_accidents_2010_20.csv')\n",
    "webbrowser.open(url,new=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
