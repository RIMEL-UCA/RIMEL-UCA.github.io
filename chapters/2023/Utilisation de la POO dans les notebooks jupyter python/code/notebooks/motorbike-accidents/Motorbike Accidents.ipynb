{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from fbprophet import Prophet\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'datasets/motorbike_ambulance_calls.csv'\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorbike_data = (\n",
    "    pd.read_csv(\n",
    "        DATASET_PATH,\n",
    "        parse_dates=['date'],\n",
    "        dayfirst=False,\n",
    "    )\n",
    "    .set_index('index')\n",
    ")\n",
    "motorbike_data.info()\n",
    "motorbike_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datetime_feature(data):\n",
    "    return (\n",
    "        data\n",
    "        .assign(datetime=lambda x: (\n",
    "            x.apply(\n",
    "                lambda y: (\n",
    "                    y['date'].replace(hour=y['hr'])\n",
    "                ), \n",
    "                axis='columns'\n",
    "            )\n",
    "        ))\n",
    "    )\n",
    "\n",
    "def convert_datetime_to_unix_timestamp(data):\n",
    "    return (\n",
    "        data\n",
    "        .assign(datetime=lambda x: x['datetime'].astype(np.int64) // 10**9)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = (\n",
    "    motorbike_data\n",
    "    .pipe(add_datetime_feature)\n",
    "    .pipe(convert_datetime_to_unix_timestamp)\n",
    ")\n",
    "\n",
    "analysis_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'cnt'\n",
    "numerical_features = {\n",
    "    'temp', 'atemp', 'hum', 'windspeed',\n",
    "    'yr', 'mnth', 'hr', 'datetime'\n",
    "}\n",
    "numerical_and_target_features = numerical_features | set([target_feature])\n",
    "categorical_features = {\n",
    "    'season', 'holiday', 'weekday', 'weathersit', 'workingday'\n",
    "}\n",
    "leftout_features = (\n",
    "    set(motorbike_data.columns) \n",
    "    - set([target_feature])\n",
    "    - numerical_features\n",
    "    - categorical_features\n",
    ")\n",
    "leftout_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    analysis_data\n",
    "    .reindex(columns=numerical_and_target_features)\n",
    "    .describe()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are already within domains.\n",
    "`windspeed` has to be scaled to [0, 1] interval, ordinal and categorical features have to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    analysis_data\n",
    "    .isnull()\n",
    "    .any()\n",
    "    .any()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Pearson correlation coefficient pairwise to see if there is a linear dependency between features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    analysis_data\n",
    "    .reindex(columns=numerical_and_target_features)\n",
    "    .corr()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max correlation for each feature:')\n",
    "(\n",
    "    analysis_data\n",
    "    .reindex(columns=numerical_and_target_features)\n",
    "    .corr()\n",
    "    .pipe(lambda x: x.subtract(np.diag([1.0] * len(x.columns))))\n",
    "    .apply(\n",
    "        lambda x: pd.Series({\n",
    "            'feature': x.abs().idxmax(), \n",
    "            'corr': x[x.abs().idxmax()]\n",
    "        }), \n",
    "        axis='columns'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong linear dependencies are not observed between the target variable and the numerical features. The target variable `cnt` is correlated most with `temp` feature. At the same time, features `temp` and `atemp` are strongly correlated, so the model should not use both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    (\n",
    "        analysis_data\n",
    "        .reindex(columns=numerical_and_target_features)\n",
    "        .drop(columns='temp')\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features: workingday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    x='hr',\n",
    "    y='cnt',\n",
    "    kind='box',\n",
    "    hue='workingday',\n",
    "    data=analysis_data,\n",
    "    aspect=2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, on working days more accidents happen in the rush hours - 7.00 - 8.00 and 16.00 - 20.00. On non-working days, the accidents are distributed closer to the middle of the day.\n",
    "\n",
    "For both working and non-working days, the dependency between `hr` and `cnt` doesn't seem to be linear.\n",
    "\n",
    "**Observation 1: motorbike accidents are distributed differently over time depending on workingday**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features: season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    x='hr',\n",
    "    y='cnt',\n",
    "    kind='box',\n",
    "    hue='workingday',\n",
    "    row='season',\n",
    "    data=analysis_data,\n",
    "    aspect=2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features: weathersit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    x='hr',\n",
    "    y='cnt',\n",
    "    kind='box',\n",
    "    hue='workingday',\n",
    "    row='weathersit',\n",
    "    data=analysis_data,\n",
    "    aspect=2\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    x='weathersit',\n",
    "    y='cnt',\n",
    "    data=analysis_data,\n",
    "    aspect=2\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    analysis_data\n",
    "    .groupby('weathersit')\n",
    "    ['cnt']\n",
    "    .sum()\n",
    "    .plot(kind='bar', title='Number of accidents by weathersit')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical feature: weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    x='weekday',\n",
    "    y='cnt',\n",
    "    kind='box',\n",
    "    hue='workingday',\n",
    "    data=analysis_data,\n",
    "    aspect=2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = motorbike_data.drop(columns=target_feature), motorbike_data[target_feature]\n",
    "\n",
    "# No time machine: use 'past' data for training, use 'future' data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, shuffle=False\n",
    ")\n",
    "\n",
    "assert X_train.index.max() < X_test.index.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_numerical_features = list(numerical_features - {'temp', 'datetime'})\n",
    "print('Numerical features:', model_numerical_features)\n",
    "model_categorical_features = list(categorical_features)\n",
    "print('Categorical features:', model_categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        # In case serving data has missing values\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        # Need to scale windspeed\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        # In case serving data has missing values\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(categories='auto', \n",
    "                                 sparse=False, \n",
    "                                 handle_unknown='ignore'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pipeline = Pipeline(steps=[\n",
    "    (\n",
    "        'features',\n",
    "        ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\n",
    "                    'numerical',\n",
    "                    numerical_transformer, \n",
    "                    model_numerical_features\n",
    "                ),\n",
    "                (\n",
    "                    'categorical', \n",
    "                    categorical_transformer, \n",
    "                    model_categorical_features\n",
    "                )\n",
    "            ],\n",
    "            remainder='drop'\n",
    "        )\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline(model, use_grid_search=True, **grid_search_params):\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('features', features_pipeline),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    if use_grid_search:\n",
    "        grid_search_params = {\n",
    "            'cv': TimeSeriesSplit(n_splits=5),\n",
    "\n",
    "            **grid_search_params\n",
    "        }\n",
    "        return GridSearchCV(pipeline, **grid_search_params)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(model_name, true_values, prediction):\n",
    "    rmse = math.sqrt(\n",
    "        mean_squared_error(true_values, prediction)\n",
    "    )\n",
    "    print(f'{model_name} RMSE: ', rmse)\n",
    "    sns.relplot(\n",
    "        x=model_name,\n",
    "        y='true values',\n",
    "        data=pd.DataFrame({\n",
    "            model_name: prediction,\n",
    "            'true values': true_values\n",
    "        })\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, model, **grid_search_params):\n",
    "    pipeline = build_pipeline(\n",
    "        model,\n",
    "        use_grid_search='param_grid' in grid_search_params,\n",
    "        **grid_search_params\n",
    "    )\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    evaluate_prediction(model_name, y_test, pipeline.predict(X_test))\n",
    "    if hasattr(pipeline, 'best_params_'):\n",
    "        print('Best params: ', pipeline.best_params_)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted before, there are no strong linear relationships in the data, so linear regression won't work well here. Anyway let's fit it just to have a baseline and see if the pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = evaluate_model('linear regression', LinearRegression())\n",
    "print('R2 score: ', linear_regression.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple non-linear relations like working/non-working day and rush hours should be well-captured by a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "decision_tree = evaluate_model(\n",
    "    'decision tree', \n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid={\n",
    "        # Already found the best params\n",
    "        'model__max_depth': [20]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree has a drawback: it easily overfits, especially on small datasets like the one we have here. To reduce overfitting we can use random forest and limit max_depth more aggresively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = evaluate_model(\n",
    "    'random forest',\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid={\n",
    "        # Already found the best params\n",
    "        'model__n_estimators': [40],\n",
    "        'model__max_depth': [20]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_data_for_prophet(X, y):\n",
    "    df = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                (\n",
    "                    X\n",
    "                    .pipe(add_datetime_feature)\n",
    "                    ['datetime']\n",
    "                    .rename('ds')\n",
    "                ),\n",
    "                y.rename('y')\n",
    "            ],\n",
    "            axis='columns',\n",
    "            sort=False\n",
    "        )\n",
    "    )\n",
    "    holidays = (\n",
    "        X\n",
    "        .groupby('date', as_index=False)\n",
    "        .first()\n",
    "        .assign(holiday=lambda x: np.where(\n",
    "            x['holiday'] == 1,\n",
    "            'holiday',\n",
    "            np.where(\n",
    "                x['workingday'] == 0,\n",
    "                'weekend',\n",
    "                None\n",
    "            )\n",
    "        ))\n",
    "        [lambda x: x['workingday'] == 0]\n",
    "        [['date', 'holiday']]\n",
    "        .rename(columns={\n",
    "            'date': 'ds'\n",
    "        })\n",
    "    )\n",
    "    return df, holidays\n",
    "\n",
    "(\n",
    "    X_train\n",
    "    .pipe(_transform_data_for_prophet, y=y_train)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProphetRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, **prophet_args):\n",
    "        self.prophet_args = prophet_args\n",
    "        self.model = None\n",
    "        self.last_prediction = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df, holidays = _transform_data_for_prophet(X, y)\n",
    "        \n",
    "        prophet_args = self.prophet_args.copy()\n",
    "        if prophet_args.pop('use_holidays', None):\n",
    "            prophet_args['holidays'] = holidays\n",
    "            \n",
    "        quarterly_seasonality = prophet_args.pop('quarterly_seasonality', None)\n",
    "        montly_seasonality = prophet_args.pop('montly_seasonality', None)\n",
    "        hourly_seasonality = prophet_args.pop('hourly_seasonality', None)\n",
    "        \n",
    "        self.model = Prophet(**prophet_args)\n",
    "        \n",
    "        if quarterly_seasonality:\n",
    "            self.model.add_seasonality(\n",
    "                name='quarterly',\n",
    "                period=365.25 / 4,\n",
    "                fourier_order=5\n",
    "            )\n",
    "        if montly_seasonality:\n",
    "            self.model.add_seasonality(\n",
    "                name='montly',\n",
    "                period=30.5,\n",
    "                fourier_order=5\n",
    "            )\n",
    "        if hourly_seasonality:\n",
    "            self.model.add_seasonality(\n",
    "                name='hourly',\n",
    "                period=24,\n",
    "                fourier_order=5\n",
    "            )\n",
    "            \n",
    "        self.model.fit(df)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.model:\n",
    "            raise RuntimeError('Neet to train first')\n",
    "        future = (\n",
    "            X\n",
    "            .pipe(add_datetime_feature)\n",
    "            [['datetime']]\n",
    "            .rename(columns={\n",
    "                'datetime': 'ds'\n",
    "            })\n",
    "        )\n",
    "        self.last_prediction = self.model.predict(future)\n",
    "        yhat = self.last_prediction['yhat'].copy()\n",
    "        yhat.index = X.index\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet1 = ProphetRegressor(\n",
    "    yearly_seasonality=True,\n",
    "    quarterly_seasonality=True,\n",
    "    # Montly seasonality actually makes it worse.\n",
    "#     montly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=True,\n",
    "    hourly_seasonality=True,\n",
    "    use_holidays=True\n",
    ")\n",
    "\n",
    "prophet1.fit(X_train, y_train)\n",
    "evaluate_prediction('prophet 1', y_test, prophet1.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet1.model.plot(prophet1.last_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prophet1.model.plot_components(prophet1.last_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_working = (\n",
    "    X_train\n",
    "    [X_train['workingday'] == 1]\n",
    ")\n",
    "\n",
    "y_train_working = (\n",
    "    y_train\n",
    "    [X_train['workingday'] == 1]\n",
    ")\n",
    "\n",
    "assert (X_train_working.index == y_train_working.index).all()\n",
    "\n",
    "X_test_working = (\n",
    "    X_test\n",
    "    [X_test['workingday'] == 1]\n",
    ")\n",
    "\n",
    "y_test_working = (\n",
    "    y_test\n",
    "    [X_test['workingday'] == 1]\n",
    ")\n",
    "\n",
    "assert (X_test_working.index == y_test_working.index).all()\n",
    "\n",
    "\n",
    "X_train_nonworking = (\n",
    "    X_train\n",
    "    [X_train['workingday'] == 0]\n",
    ")\n",
    "\n",
    "y_train_nonworking = (\n",
    "    y_train\n",
    "    [X_train['workingday'] == 0]\n",
    ")\n",
    "\n",
    "assert (X_train_nonworking.index == y_train_nonworking.index).all()\n",
    "\n",
    "X_test_nonworking = (\n",
    "    X_test\n",
    "    [X_test['workingday'] == 0]\n",
    ")\n",
    "y_test_nonworking = (\n",
    "    y_test\n",
    "    [X_test['workingday'] == 0]\n",
    ")\n",
    "\n",
    "assert (X_test_nonworking.index == y_test_nonworking.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.concat([X_train_working, y_train_working], axis=1)\n",
    "    .pipe(add_datetime_feature)\n",
    "    [['datetime', 'cnt']]\n",
    "    [lambda x: (x['datetime'] >= '2011-06-01') & (x['datetime'] <= '2011-07-01')]\n",
    "    .plot(\n",
    "        x='datetime',\n",
    "        y='cnt',\n",
    "        figsize=(15, 4),\n",
    "        xticks=pd.date_range(f'2011-06-01', '2011-07-01')\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    pd.concat([X_test_working, y_test_working], axis=1)\n",
    "    .pipe(add_datetime_feature)\n",
    "    [['datetime', 'cnt']]\n",
    "    [lambda x: (x['datetime'] >= '2012-06-01') & (x['datetime'] <= '2012-07-01')]\n",
    "    .plot(\n",
    "        x='datetime',\n",
    "        y='cnt',\n",
    "        figsize=(15, 4),\n",
    "        xticks=pd.date_range('2012-06-01', '2012-07-01')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_working_days = ProphetRegressor(\n",
    "    yearly_seasonality=True,\n",
    "    quarterly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=True,\n",
    "    hourly_seasonality=True,\n",
    ")\n",
    "\n",
    "prophet_working_days.fit(X_train_working, y_train_working)\n",
    "\n",
    "prophet_working_days_prediction = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            X_test_working, \n",
    "            y_test_working,\n",
    "            prophet_working_days.predict(X_test_working)\n",
    "        ], \n",
    "        axis=1\n",
    "    )\n",
    "    .pipe(add_datetime_feature)\n",
    "    [['datetime', 'cnt', 'yhat']]\n",
    ")\n",
    "\n",
    "evaluate_prediction(\n",
    "    'prophet working days', \n",
    "    y_test_working, \n",
    "    prophet_working_days_prediction['yhat']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots(figsize=(13, 10))\n",
    "\n",
    "(\n",
    "    prophet_working_days_prediction\n",
    "    .plot(\n",
    "        ax=ax,\n",
    "        x='datetime',\n",
    "        y='cnt',\n",
    "        label='cnt',\n",
    "        color='green',\n",
    "        alpha=0.7\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    prophet_working_days_prediction\n",
    "    .plot(\n",
    "        ax=ax,\n",
    "        x='datetime',\n",
    "        y='yhat',\n",
    "        label='yaht',\n",
    "        color='blue',\n",
    "        alpha=0.7\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProphetTransformer(ProphetRegressor, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, **prophet_args):\n",
    "        super().__init__(**prophet_args)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self.predict(X).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeveragingRandomForestRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self, **model_args):\n",
    "        self.model_args = model_args\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        self.model = RandomForestRegressor(**self.model_args)\n",
    "        self.model.fit(X[:, 1:], X[:, 0])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not self.model:\n",
    "            raise RuntimeError('Neet to train first')\n",
    "        prediction = self.model.predict(X[:, 1:])\n",
    "        return (X[:, 0] - prediction).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leveraging_transformer = Pipeline(steps=[\n",
    "    (\n",
    "        'features',\n",
    "        ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\n",
    "                    'prophet',\n",
    "                    ProphetTransformer(\n",
    "                        yearly_seasonality=True,\n",
    "                        quarterly_seasonality=True,\n",
    "                        weekly_seasonality=True,\n",
    "                        daily_seasonality=True,\n",
    "                        hourly_seasonality=True,\n",
    "                        use_holidays=True\n",
    "                    ),\n",
    "                    ['date', 'hr', 'holiday', 'workingday']\n",
    "                ),\n",
    "                (\n",
    "                    'numerical',\n",
    "                    numerical_transformer, \n",
    "                    model_numerical_features\n",
    "                ),\n",
    "                (\n",
    "                    'categorical', \n",
    "                    categorical_transformer, \n",
    "                    model_categorical_features\n",
    "                )\n",
    "            ],\n",
    "            remainder='drop'\n",
    "        )\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leveraging_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('leveraging_transformer', leveraging_transformer),\n",
    "        (\n",
    "            'leveraging_random_forest', \n",
    "            LeveragingRandomForestRegressor(\n",
    "                n_estimators=40,\n",
    "                max_depth=20\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leveraging_pipeline.fit(X_train, y_train)\n",
    "leveraging_pipeline_prediction = leveraging_pipeline.predict(X_test)\n",
    "evaluate_prediction(\n",
    "    'leveraging pipeline', \n",
    "    y_test, \n",
    "    leveraging_pipeline_prediction.reshape(1, -1)[0, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
