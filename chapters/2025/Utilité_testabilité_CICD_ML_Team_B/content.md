---
layout: default
title : Utilit√© et testabilit√© des pipelines CI/CD dans le Machine Learning
date:   2025-01
---

## Authors

Nous sommes trois √©tudiants en M2 Informatique ou en derni√®re ann√©e √† Polytech Nice-Sophia sp√©cialis√©s en SSE (Substainable Software Engineering):
* [Melek Farhat](https://github.com/farhatmelek) &lt;melek.farhat@etu.unice.fr&gt;
* [Adjo Emmanuelle Adote](https://github.com/EmmanuelleAD) &lt;adjo-emmanuelle.adote@etu.unice.fr&gt;
* [Anna Di Placido](https://github.com/Annadip5) &lt;anna.di-placido@etu.unice.fr&gt;

## I. Contexte de la recherche

L‚Äô√©mergence croissante du MLOps et de ses outils, tels que KitOps, MLFlow et ClearML, r√©pond √† un besoin grandissant d‚Äôautomatisation et de fiabilit√© dans la gestion des mod√®les de Machine Learning. Dans nos contextes professionnels actuels, o√π l‚ÄôIA joue un r√¥le de plus en plus central dans les applications quotidiennes, il devient crucial de garantir que les mod√®les sont non seulement performants, mais aussi d√©ploy√©s dans les meilleures conditions possibles. En effet, l‚Äôint√©gration de l‚ÄôIA dans les syst√®mes et services touche de plus en plus d‚Äôaspects de notre vie quotidienne, ce qui en fait une tendance incontournable. Pour cette raison, assurer la qualit√©, la fiabilit√© et la performance des mod√®les √† travers des pipelines robustes et bien structur√©s devient essentiel.

Cependant, le d√©veloppement et l‚Äôint√©gration des mod√®les de Machine Learning comportent des d√©fis uniques, notamment en termes de tests. Tester un mod√®le de Machine Learning est d√©j√† une t√¢che complexe, voire parfois impossible, en raison de la variabilit√© des donn√©es, des biais possibles et de la non-lin√©arit√© des comportements du mod√®le. D√®s lors, une question se pose : comment tester efficacement un outil de MLOps qui facilite le d√©ploiement, la gestion et la surveillance de ces mod√®les ? Les tests d‚Äôun outil MLOps sont eux-m√™mes complexes car ils n√©cessitent de garantir que les pipelines qu‚Äôils orchestrent fonctionnent comme pr√©vu dans des environnements o√π l‚Äôincertitude des mod√®les de Machine Learning est omnipr√©sente.

Ce sujet correspond parfaitement √† nos objectifs d‚Äôapprentissage, notamment gr√¢ce √† sa focalisation sur les aspects CI/CD des pipelines d‚Äôint√©gration et de d√©ploiement. En nous familiarisant avec des outils et processus d‚Äôautomatisation du cycle de vie des mod√®les, nous serons mieux pr√©par√©s √† g√©rer les d√©fis techniques li√©s au d√©ploiement d‚Äôapplications intelligentes et fiables. De plus, cette th√©matique nous permet d'approfondir nos comp√©tences op√©rationnelles, un domaine cl√© dans la gestion des applications en production. La ma√Ætrise de l'automatisation des tests, des d√©ploiements et de la gestion des flux de travail dans un environnement ML est de plus en plus demand√©e dans l‚Äôindustrie, car elle garantit une meilleure qualit√© et une plus grande efficacit√© dans la mise en production des solutions d‚ÄôIA. Ainsi, ce sujet nous offre une opportunit√© unique de nous pr√©parer √† un r√¥le cl√© dans la transformation num√©rique des entreprises, en nous √©quipant des comp√©tences n√©cessaires pour d√©ployer des mod√®les de Machine Learning dans des conditions optimales.


## II. Questions

### üîç Question principale 
***
#### *Comment l‚Äôefficacit√© et la fiabilit√© des outils MLOps sont-elles assur√©es √† travers les pipelines ?*
(Cas de **KitOps**, **MLFlow**, **ClearML**, **MetaFlow**)
<br></br>

Cette question est int√©ressante car elle soul√®ve des **enjeux cruciaux** li√©s √† l'**automatisation** et √† la **gestion des mod√®les de Machine Learning**, qui sont des domaines en pleine **√©volution**. En particulier, comprendre comment **assurer une mise en production fiable et efficace** des **mod√®les ML** est essentiel pour les **entreprises** qui cherchent √† tirer parti des **capacit√©s de l'IA** tout en maintenant des **standards de qualit√© √©lev√©s**.


### ‚ùì Sous-questions 
***

1. *Qu‚Äôest ce qui differencie les mlops & devops  ?*

2. *Que font les outils choisis ?*

3. *Que font les WorkFlows CI/CD*

4. *En quoi consiste le testing de ces outils via github actions ?*


## III. Collecte d'informations

Pour r√©pondre aux questions de recherche, nous avons d√©fini des zones sp√©cifiques pour orienter nos investigations et identifier les ressources pertinentes :

- **Articles scientifiques et publications en ligne** seront utilis√©es pour explorer et comprendre les distinctions fondamentales entre MLOps et DevOps. Les articles offriront un cadre th√©orique solide et des exemples concrets d'applications et de retours d'exp√©rience.
- **Documentation** des outils s√©lectionn√©s sera au c≈ìur de notre analyse pour d√©tailler les fonctionnalit√©s, les cas d'utilisation et les limitations sp√©cifiques de ces outils. Elle constitue une source fiable pour comprendre leur r√¥le dans le cycle de vie des mod√®les de Machine Learning.
- **Workflows CI/CD GitHub et pipelines** existants pour des outils de MLOps seront analys√©s. Ces workflows offriront un aper√ßu pratique des strat√©gies de test et d'int√©gration continues mises en ≈ìuvre pour garantir la fiabilit√© des pipelines orchestr√©s par les outils MLOps.


### Outils et m√©thodologie
Outils MLOps cibl√©s : [KitOps](https://github.com/jozu-ai/kitops), [MLFlow](https://github.com/mlflow/mlflow), [ClearML](https://github.com/clearml/clearml), [Metaflow](https://github.com/Netflix/metaflow). 

Ces outils ont √©t√© s√©lectionn√©s en raison de leur popularit√© et de leurs fonctionnalit√©s avanc√©es dans la gestion des pipelines de Machine Learning.


## IV. Hypoth√®ses, Exp√©riences, Result Analysis and Conclusion
--- 
1. Il s'agit ici d'**√©noncer sous forme d'hypoth√®ses** ce que vous allez chercher √† d√©montrer. Vous devez d√©finir vos hypoth√®ses de fa√ßon √† pouvoir les _mesurer/v√©rifier facilement._ Bien s√ªr, votre hypoth√®se devrait √™tre construite de mani√®re √† _vous aider √† r√©pondre √† votre question initiale_. Explicitez ces diff√©rents points.
2. Vous **explicitez les exp√©rimentations que vous allez mener** pour v√©rifier si vos hypoth√®ses sont vraies ou fausses. Il y a forc√©ment des choix, des limites, explicitez-les.

     :bulb: Structurez cette partie √† votre convenance : 
     Par exemples : 
        Pour Hypoth√®se 1 => 
            Nous ferons les Exp√©riences suivantes pour la d√©montrer
        Pour Hypoth√®se 2 => Exp√©riences 
        
        ou Vous pr√©sentez l'ensemble des hypoth√®ses puis vous expliquer comment les exp√©riences pr√©vues permettront de d√©montrer vos hypoth√®ses.

1. Pr√©sentation des r√©sultats
2. Interpr√©tation/Analyses des r√©sultats en fonction de vos hypoth√®ses
3. Construction d‚Äôune conclusion 

     :bulb:  Vos r√©sultats et donc votre analyse sont n√©cessairement limit√©s. Pr√©ciser bien ces limites : par exemple, jeux de donn√©es insuffisants, analyse r√©duite √† quelques crit√®res, d√©pendance aux projets analys√©s, ...

--- 
<br></br>

## üîç Hypoth√®ses
Voici les hypoth√®ses formul√©es en lien avec nos questions :

#### üü¢ Hypoth√®se 1 
   L'efficacit√© des outils MLOps est li√©e √† leur capacit√© √† automatiser les √©tapes des pipelines. Cela inclut l'int√©gration, le d√©ploiement, la surveillance et la gestion des mod√®les.

#### üü£ Hypoth√®se 2
   La fiabilit√© des outils MLOps d√©pend de leur robustesse face aux erreurs, de leur gestion des d√©pendances et de leur testabilit√©. Les outils peuvent offrir des fonctionnalit√©s sp√©cifiques (versionnement, suivi) qui renforcent cette fiabilit√©.

#### üî¥ Hypoth√®se 3  
   Les pipelines bien con√ßus permettent une meilleure collaboration entre les √©quipes, r√©duisant ainsi les risques d'erreurs manuelles.

---

#### üü† Hypoth√®se 4  
   MLOps se concentre davantage sur la gestion des donn√©es, le suivi des mod√®les et leur d√©ploiement, tandis que DevOps est centr√© sur la gestion des infrastructures et le d√©ploiement de logiciels traditionnels.

#### üü° Hypoth√®se 5 
   Les pipelines MLOps n√©cessitent des √©tapes suppl√©mentaires comme la pr√©paration des donn√©es, l'entrainement des mod√®les et la surveillance des performances apr√®s le d√©ploiement.

---

#### üü£ Hypoth√®se 6 
   Les outils offrent des interfaces unifi√©es pour g√©rer la complexit√© des workflows ML, tout en facilitant leur int√©gration dans des pipelines CI/CD.

#### üü¢ Hypoth√®se 7   
   Les workflows permettent de d√©tecter rapidement les erreurs gr√¢ce √† des tests automatis√©s, en r√©duisant le temps de mise en production.

#### üü£ Hypoth√®se 8 
   Les workflows CI/CD via GitHub Actions incluent des tests sp√©cifiques au machine learning, tels que des simulations d‚Äôentra√Ænement de mod√®les, la validation des logs g√©n√©r√©s et des tests de mont√©e en charge pour garantir la robustesse des mod√®les avant leur d√©ploiement.
   


<br></br>

## üî¨ Exp√©riences
Pour valider nos hypoth√®ses, nous m√®nerons les exp√©rimentations suivantes :

#### üü¢ MLOps vs DevOps
Identification des similitudes et des diff√©rences en comparant des points cl√©s (tests, validation, gestion des infrastructures, etc.) √† partir d'articles scientifiques et de documentation.

#### üü£ Analyse des fonctionnalit√©s des outils MLOps  
Analyse des principales fonctionnalit√©s fournies par les outils s√©lectionn√©s (KitOps, MLFlow, ClearML, MetaFlow) pour √©valuer leur ad√©quation avec les besoins du projet et d√©terminer dans quelle mesure les tests r√©alis√©s couvrent ces fonctionnalit√©s.

#### üî¥ Tests GitHub Actions  
Analyses des types de tests utilis√©s dans les workflows CI/CD, leur utilit√©, leur fr√©quence, et leur r√©partition par phase.  
Identification des m√©thodes de gestion des mod√®les et des donn√©es dans ces tests.



## ‚ö†Ô∏è Limites
Cependant, il est important de prendre en compte certaines limitations qui peuvent influencer les r√©sultats et les conclusions de nos exp√©rimentations, telles que :

#### üü† Limitation li√©e √† la s√©lection des outils 
Cette √©tude est bas√©e sur une s√©lection sp√©cifique d'outils (KitOps, MLFlow, ClearML, MetaFlow). Par cons√©quent, les conclusions tir√©es ne peuvent pas √™tre g√©n√©ralis√©es √† l'ensemble des solutions MLOps disponibles, car chaque outil peut pr√©senter des caract√©ristiques uniques.

#### üü° Disponibilit√© des informations
Certaines documentations ou fonctionnalit√©s des outils pourraient √™tre incompl√®tes ou difficiles √† √©valuer sans acc√®s direct √† des cas r√©els d‚Äôutilisation.

#### üü£ √âvolutivit√© des outils MLOps  
Les outils √©voluent rapidement, ce qui pourrait rendre certaines conclusions obsol√®tes √† court terme.

#### üî¥ Expertise limit√©e en Machine Learning  
L‚Äôanalyse des pipelines MLOps pourrait √™tre influenc√©e par une connaissance limit√©e dans certains aspects avanc√©s du Machine Learning.


## IV. R√©sultats et Analyses

## üìä ClearML

### Fonctionnement 
***
ClearML est une plateforme open source con√ßue pour g√©rer les workflows de machine learning (ML) de bout en bout. Elle permet de suivre les exp√©riences, d‚Äôorchestrer les pipelines, de g√©rer les donn√©es et de faciliter la collaboration entre les √©quipes. Voici une description d√©taill√©e de son fonctionnement :

1. Suivi des Exp√©riences (Experiment Tracking)

ClearML simplifie la gestion des workflows de machine learning gr√¢ce √† une orchestration automatis√©e et une tra√ßabilit√© compl√®te. Une fois les pipelines d√©finis par les utilisateurs, ses agents r√©partissent les t√¢ches sur des ressources locales ou cloud, tout en assurant un suivi exhaustif des param√®tres, m√©triques et d√©pendances. Compatible avec des frameworks comme TensorFlow, PyTorch ou Scikit-learn, ClearML garantit une int√©gration fluide dans les environnements existants, y compris avec Docker, Kubernetes ou des outils CI/CD. Son tableau de bord centralis√© permet de visualiser et de surveiller en temps r√©el l‚Äô√©tat des t√¢ches, offrant ainsi une meilleure compr√©hension et optimisation des workflows, tout en garantissant leur reproductibilit√©.

2. Gestion des Pipelines (Pipeline Orchestration)

Pour g√©rer les pipelines de machine learning, ClearML offre un module de gestion des t√¢ches. Cependant, il est important de noter que ClearML ne propose pas de pipelines pr√©configur√©s, les utilisateurs doivent les d√©finir eux-m√™mes. Chaque √©tape, qu‚Äôil s‚Äôagisse du pr√©traitement des donn√©es, de l‚Äôentra√Ænement ou de l‚Äô√©valuation, est repr√©sent√©e par une t√¢che distincte. Les d√©pendances entre les t√¢ches doivent √™tre configur√©es manuellement pour assurer leur ex√©cution dans le bon ordre. En outre, l‚Äôenregistrement des versions des scripts et des d√©pendances assure une reproductibilit√© totale des workflows.

3. Gestion des Donn√©es (Data Management)

La gestion des donn√©es est une fonctionnalit√© cl√© de ClearML. L‚Äôoutil suit les donn√©es d‚Äôentra√Ænement, de validation et de test tout au long du pipeline. Il offre une tra√ßabilit√© compl√®te en versionnant les datasets et en enregistrant les modifications apport√©es, comme l‚Äôajout ou le filtrage de donn√©es. En cas de mise √† jour des donn√©es, les pipelines associ√©s sont √©galement modifi√©s pour refl√©ter ces changements.

4. Collaboration et D√©ploiement

ClearML facilite √©galement la collaboration et le d√©ploiement des mod√®les. Les √©quipes peuvent partager facilement leurs exp√©riences, configurations de pipelines et datasets gr√¢ce √† un tableau de bord collaboratif. Les projets peuvent √™tre suivis en temps r√©el, avec des notifications en cas de succ√®s ou d‚Äô√©chec des t√¢ches. Pour le d√©ploiement, ClearML propose des outils pour exporter les mod√®les et les int√©grer dans des environnements de production. Il permet √©galement de surveiller les mod√®les d√©ploy√©s et de les r√©entra√Æner en fonction des besoins op√©rationnels.


### Analyses
***

Dans le cadre de notre √©tude, nous avons examin√© les workflows associ√©s √† l'outil ClearML, qui permet de g√©rer les exp√©rimentations et de faciliter l'ex√©cution de workflows dans des projets de machine learning. Cependant, lors de notre analyse, nous avons constat√© que le workflow configur√© dans le d√©p√¥t GitHub √©tait sp√©cifiquement destin√© √† l'ex√©cution de CodeQL. CodeQL est un outil de d√©tection de vuln√©rabilit√©s et d'erreurs dans le code source. Son objectif principal est d'am√©liorer la s√©curit√© et la qualit√© des projets en identifiant des failles de s√©curit√© potentielles dans le code h√©berg√© sur GitHub.

Le workflow de ClearML, tel qu'il est configur√©, est planifi√© pour s'ex√©cuter de mani√®re p√©riodique. En effet, le processus est d√©clench√© automatiquement chaque semaine √† 03h45. Toutefois, il est important de noter que ce workflow est limit√© dans sa port√©e. En effet, il se concentre uniquement sur l'ex√©cution de l'analyse statique du code via CodeQL, sans int√©grer des √©tapes fondamentales pour un workflow CI/CD complet.

Un autre point pr√©occupant concerne le manque de progression du workflow apr√®s son premier lancement. En effet, aucune √©volution n‚Äôa √©t√© observ√©e depuis le premier push, comme illustr√© sur la figure 1 On observe seulement deux commits li√©s au workflow : le premier concerne la configuration initiale des pipelines GitHub Actions, et le second met √† jour la version de CodeQL. Ce comportement soul√®ve des questions sur l'efficacit√© op√©rationnelle du workflow et sur sa capacit√© √† s'adapter √† un processus de d√©veloppement dynamique. Malgr√© la maturit√© du projet, comme le montre la figure 2 avec la date du premier push, La fr√©quence d'ex√©cution du workflow est √©galement un point faible. En √©tant d√©fini de mani√®re p√©riodique, √† un horaire fixe chaque semaine, comme le montre la figure 3, il ne permet pas de r√©agir en temps r√©el aux modifications du code. 


![Alt text](./assets/images/Clearml/clearmlcommits.png)
<div style="text-align: center;">  
  <p><em>Figure 1 : Commits li√©s au workflow depuis le premier push</em></p>
</div>

![Alt text](./assets/images/Clearml/fistcommitdate.png)
<div style="text-align: center;">  
  <p><em>Figure 2 : Date du premier push du projet et indicateurs de maturit√©</em></p>
</div>

![Alt](./assets/images/Clearml/start.png)
<div style="text-align: center;">
  <p><em>Figure 3 : Planification p√©riodique du workflow GitHub Actions</em></p>
</div>

### Conclusion
***
Le workflow actuellement configur√© pr√©sente des limitations significatives, non seulement dans le cadre d‚Äôun environnement de d√©veloppement ML (machine leaning), mais √©galement par rapport √† des workflows classiques. En se limitant uniquement √† l‚Äôanalyse statique du code avec CodeQL, il ignore des √©tapes fondamentales d‚Äôun pipeline CI/CD complet, telles que l‚Äôex√©cution de tests unitaires, de tests d‚Äôint√©gration ou le d√©ploiement automatis√©. Ces omissions compromettent sa capacit√© √† garantir la qualit√©, la s√©curit√© et la fiabilit√© du code.

En outre, la planification p√©riodique du workflow, bien qu‚Äôautomatis√©e, est peu flexible et inadapt√©e √† des processus dynamiques. Contrairement aux workflows modernes qui s‚Äôappuient sur des d√©clencheurs en temps r√©el (comme un push ou une pull request), ce workflow limite la d√©tection et la r√©solution rapide des erreurs ou vuln√©rabilit√©s introduites dans le code.

Le manque d'√©volution du workflow depuis son premier lancement souligne √©galement une incapacit√© √† s‚Äôadapter aux besoins changeants du projet, refl√©tant un d√©ficit par rapport aux standards actuels. Ces limitations le placent non seulement en retrait par rapport aux bonnes pratiques modernes, mais aussi derri√®re les workflows classiques qui incluent g√©n√©ralement une couverture plus large des t√¢ches critiques.
<br></br>

## ‚öôÔ∏è KitOps
### Fonctionnement 
***
KitOps est une solution de packaging, versioning et partage sp√©cifiquement con√ßue pour les projets d'intelligence artificielle (IA) et de machine learning (ML). Il s'int√®gre facilement avec les outils d√©j√† utilis√©s dans les environnements de d√©veloppement et DevOps. En tant qu'outil de gestion d'actifs, KitOps permet aux √©quipes d'IA/ML d'optimiser le stockage, la gestion des versions, et la distribution de leurs projets tout au long de leur cycle de vie.

1. Versioning et Gestion des D√©pendances

Chaque ModelKit cr√©√© avec KitOps est tagu√© avec une version sp√©cifique, ce qui permet aux √©quipes de savoir exactement quelle version des mod√®les et des datasets est utilis√©e ensemble. Cela r√©duit les risques li√©s √† l'incompatibilit√© des versions dans les projets d'IA/ML complexes, o√π plusieurs composants doivent √™tre √©troitement coupl√©s.

Au c≈ìur de KitOps se trouve le concept de ModelKit, une unit√© de packaging qui inclut tous les √©l√©ments n√©cessaires pour reproduire un projet IA/ML localement ou pour le d√©ployer en production. Un ModelKit peut contenir des mod√®les, des ensembles de donn√©es, des configurations et du code source. Chaque ModelKit est immuable, c'est-√†-dire que son contenu ne peut pas √™tre modifi√© apr√®s sa cr√©ation. Cela permet aux entreprises de suivre et auditer les actifs √† chaque √©tape du cycle de vie du projet.

2. Fonctionnalit√© de D√©ploiement

KitOps facilite √©galement le d√©ploiement de projets IA/ML. En utilisant une commande simple, il est possible de cr√©er un conteneur ex√©cutable √† partir d'un ModelKit. Ce conteneur peut √™tre d√©ploy√© sur des environnements Kubernetes ou √™tre ex√©cut√© localement √† l'aide de Docker. KitOps g√©n√®re des configurations de d√©ploiement pr√™tes pour Kubernetes, ce qui simplifie le processus d'int√©gration dans un environnement de production √† grande √©chelle. Cela permet une gestion fluide des d√©ploiements, que ce soit en mode d√©veloppement ou en production.

3. Unpacking S√©lectif et Flexibilit√©

Une autre fonctionnalit√© importante de KitOps est le unpacking s√©lectif des ModelKits. Cette fonctionnalit√© permet aux utilisateurs de d√©compresser uniquement les composants n√©cessaires pour une t√¢che sp√©cifique, √©conomisant ainsi du temps et de l'espace de stockage. Par exemple, un membre de l‚Äô√©quipe peut choisir de ne r√©cup√©rer que le mod√®le, tandis qu'un autre peut se concentrer uniquement sur les ensembles de donn√©es ou le code, selon ses besoins.

4. Automatisation et Int√©gration dans les Pipelines CI/CD

KitOps est con√ßu pour s'int√©grer facilement dans des workflows CI/CD, permettant aux √©quipes d'automatiser la gestion des ModelKits dans des environnements de d√©veloppement et de production. Des outils comme GitHub Actions peuvent √™tre utilis√©s pour automatiser le packaging ou le d√©ploiement des ModelKits. Cela permet de r√©duire les erreurs humaines, d‚Äôacc√©l√©rer les cycles de d√©veloppement et de tester les mod√®les de mani√®re plus efficace.

### Analyses
***

Lors de notre √©tude de l'outil KtiOps, nous avons observ√© que ses pipelines sont structur√©s en plusieurs fichiers YAML, chacun d√©di√© √† des t√¢ches sp√©cifiques. Ces pipelines, con√ßus avec GitHub Actions. Voici une description d√©taill√©e des principaux fichiers de pipeline identifi√©s :


1. Pipeline build-deploy-docs.yaml

Ce pipeline a pour objectif principal de g√©rer automatiquement la construction et le d√©ploiement de documents statiques sur GitHub Pages. Il est sp√©cifiquement con√ßu pour la documentation du projet.

- D√©clencheurs : mises √† jour du dossier docs sur la branche main ou ex√©cutions manuelles.

- √âtapes principales :
  - Configuration de l'environnement pour la documentation.
  - Installation des d√©pendances.
  - Construction des fichiers statiques.
  - D√©ploiement fiable sur GitHub Pages.

Ce pipeline offre un flux automatis√© pour maintenir √† jour la documentation, garantissant une disponibilit√© rapide et efficace pour les utilisateurs.

2.  Pipeline build-devmode.yaml

Ce pipeline cible l'interface utilisateur du mode d√©veloppement d'un projet frontend, en automatisant son processus de construction.
- Objectifs :
V√©rifier que les modifications dans le dossier frontend/dev-mode ne causent pas d'erreurs de compilation.
S'assurer que le projet peut √™tre construit avec succ√®s.

- D√©clencheurs : pull requests ou ex√©cutions manuelles.

- √âtapes principales :
  - Compilation du code.
  - Retour rapide sur les √©ventuelles erreurs de configuration ou de d√©pendances.

Bien qu'il ne g√©n√®re pas encore d'artifacts ni n'ex√©cute de tests, ce pipeline joue un r√¥le essentiel dans le workflow CI/CD. Il pr√©pare le projet pour les √©tapes ult√©rieures, comme les tests ou le d√©ploiement.

3. Pipeline next-container-build.yaml

Ce pipeline est d√©di√© √† la construction et √† la publication d'images Docker pour le projet.

- Objectifs :
  - Garantir un processus automatis√© et reproductible pour la cr√©ation des images.
  - Publier directement sur GitHub Container Registry.

4. Pipeline pr.yaml:

Ce workflow GitHub Actions est con√ßu pour assurer sur la qualit√© et la conformit√© du code. Il commence par l'installation des d√©pendances n√©cessaires pour Go, Node.js et pnpm, suivie d'une v√©rification des d√©pendances avec go mod tidy pour garantir leur coh√©rence. Ensuite, des √©tapes de linting et de formatage sont effectu√©es pour s'assurer que le code respecte les normes de style et inclut les en-t√™tes de licence requis. Le workflow compile √©galement le projet pour v√©rifier qu'il peut √™tre construit avec succ√®s, ex√©cute les tests unitaires d√©finis dans les fichiers *_test.go pour garantir la fiabilit√© du code, et effectuer des v√©rifications suppl√©mentaires, telles que la validation des inclusions dans la documentation et la d√©tection des espaces inutiles dans les fichiers. Ces √©tapes permettent d‚Äôassurer une int√©gration fluide et de maintenir un haut niveau de qualit√© avant la fusion des modifications dans la branche principale.

#### Les tests
***
Ces tests jouent un r√¥le central dans la validation et l‚Äôassurance qualit√© d‚Äôun outil MLOps, d√©di√© √† la gestion et au packaging d‚Äôartefacts de machine learning. Ces artefacts, appel√©s ModelKits, regroupent des mod√®les, fichiers, et configurations n√©cessaires pour ex√©cuter ou d√©ployer des workflows ML. L‚Äôobjectif principal de ces tests est de garantir la portabilit√©, la reproductibilit√© et la robustesse de l‚Äôoutil √† travers des sc√©narios vari√©s.

- Tests de KitOps pour la Gestion des Modelkits:

  Les tests v√©rifient plusieurs aspects du processus de gestion des modelkits. Un modelkit repr√©sente g√©n√©ralement un ensemble de fichiers n√©cessaires √† l'ex√©cution d'un mod√®le ou d'un pipeline de machine learning. Cela peut inclure des scripts, des fichiers de configuration, des datasets, etc. KitOps permet de g√©rer ces √©l√©ments en pack/unpack, garantissant ainsi une gestion efficace des d√©pendances et des configurations dans un environnement de machine learning.

  Les tests valident plusieurs op√©rations essentielles :

  - Cr√©ation et organisation des fichiers : Les tests s'assurent que les fichiers n√©cessaires au bon fonctionnement du modelkit sont correctement g√©n√©r√©s et rang√©s dans les r√©pertoires attendus.
  ou dans cahque models on peut trouver des fichiers qu'on veut igoner et des fichier qu'n veut inclure et donc ces tests valident que tous ces fichiers sont correctement pris en charge et manipul√©s.

  - Gestion des erreurs de packing : Si un modelkit contient des erreurs, le test v√©rifie que celles-ci sont bien captur√©es et que les messages d'erreur attendus sont g√©n√©r√©s.

  - Pack et unpack : Le test garantit que les outils KitOps sont capables de packager (Cette op√©ration consiste √† regrouper tous les fichiers d√©finis dans un modelkit dans un format compress√©, avec une gestion des erreurs en cas de probl√®me) un modelkit, de lister son contenu, puis de le d√©compresser (unpack) dans un autre r√©pertoire tout en respectant les r√®gles de gestion des fichiers (inclus/exclus).

- Tests de Pack/Unpack 

  Ces tests se concentrent sur le processus de cr√©ation, d'organisation, de packaging et de d√©packaging des modelkits dans un environnement de gestion de fichiers. Contrairement aux tests des r√©f√©rences des modelkits, cette s√©rie met l'accent sur la v√©rification de la reproductibilit√© et de l'int√©grit√© des contenus tout au long du cycle pack -> unpack -> validation

  Principales validations effectu√©es :
    - Cr√©ation et organisation des fichiers
      Ces tests simulent un environnement o√π diff√©rents fichiers sont cr√©√©s dans un r√©pertoire temporaire. Les fichiers √† inclure ou exclure sont d√©termin√©s par deux fichiers de configuration Kitfile (sp√©cifie les fichiers √† inclure et Kitignore d√©finit les fichiers √† exclure.)
      Cela permet de v√©rifier que le syst√®me respecte correctement les r√®gles d√©finies pour inclure ou ignorer certains fichiers lors du regroupement.

    - Gestion du Cycle Pack-Unpack :
      Pack : Processus consistant √† regrouper les fichiers s√©lectionn√©s (selon les r√®gles de Kitfile et Kitignore) dans un format compress√©.

      Unpack : Processus permettant de d√©compresser ces fichiers dans un nouveau r√©pertoire, en respectant les m√™mes r√®gles d‚Äôinclusion/exclusion.

      Les tests v√©rifient que les fichiers extraits apr√®s Unpack sont identiques √† ceux s√©lectionn√©s lors de l‚Äô√©tape Pack, garantissant ainsi une int√©grit√© et une coh√©rence dans le cycle. 

    - Reproductibilit√© du Pack
      ces tests verifient que le processus de compression des fichiers (Pack) doit toujours donner exactement le m√™me r√©sultat si on utilise les m√™mes fichiers d'entr√©e. Peu importe si des d√©tails comme les dates de modification des fichiers changent, le fichier compress√© final doit √™tre identique √† chaque fois qu'on refait l'op√©ration.

      Cela est tr√®s important pour s'assurer que le syst√®me fonctionne de mani√®re fiable et constante, surtout quand on a besoin que les fichiers compress√©s soient facilement transportables ou partag√©s sans surprises.

    Ces tests diff√®rent des tests effectu√©s dans modelkit-refs_test.go. Alors que ces derniers s'int√©ressaient √† des sc√©narios plus complexes impliquant des interactions entre plusieurs "modelkits" et des cha√Ænes d‚Äôop√©rations, les tests de pack-unpack_test.go se concentrent sur des cas isol√©s et directs, limit√©s √† un seul "modelkit". Cela simplifie l‚Äôanalyse et permet de valider des fonctionnalit√©s sp√©cifiques de gestion des fichiers.

    - test de suppression de model kits :

      Les tests de suppression des "model kits" ont pour but de v√©rifier que le syst√®me fonctionne correctement lorsqu'un kit doit √™tre supprim√©. Ces tests s'assurent qu'un kit, lorsqu'il est li√© √† un tag sp√©cifique, est bien supprim√© apr√®s avoir √©t√© ajout√© avec ce tag. Ils v√©rifient √©galement que la suppression d'un kit fonctionne lorsque celui-ci est identifi√© par son digest unique, en s'assurant que le kit est retir√© correctement de la liste. Si un kit n'a pas de tag, ces tests confirment que la suppression se fait correctement en utilisant seulement le digest, sans tenir compte des tags. Les tests √©valuent aussi la situation o√π un kit est associ√© √† plusieurs tags, en garantissant que seule l'association avec le tag sp√©cifi√© est supprim√©e, tandis que le kit reste dans le syst√®me avec les autres tags. De plus, lorsqu'un kit est supprim√© par son digest, cela entra√Æne la suppression de tous les tags associ√©s √† ce kit, assurant ainsi qu'il n'y a plus de r√©f√©rences √† ce kit dans le syst√®me. Un autre test v√©rifie que, lorsque plusieurs kits sont cr√©√©s avec des fichiers distincts (et donc des digests uniques), la commande de suppression supprime bien tous les kits marqu√©s pour retrait, sauf celui qui est sp√©cifi√©. Enfin, ces tests garantissent que la suppression de tous les kits et de leurs tags associ√©s fonctionne correctement, assurant que tout est bien retir√© du syst√®me, sans exception. Ces tests couvrent donc une large gamme de situations pour s'assurer de la bonne gestion et suppression des "model kits" et de leurs tags.

L'analyse des contributions des d√©veloppeurs met en √©vidence une forte activit√© au sein de l'√©quipe de d√©veloppement. Cette observation est montr√©e par les r√©sultats obtenus via un script automatis√©, une m√©thode essentielle pour surmonter la complexit√© d'un comptage manuel des contributions. On note une implication significative de plusieurs d√©veloppeurs, notamment dans le dossier des workflows (WF) illustr√©e par la Figure XX, et dans les tests comme le montre √©galement le Figure XX et XX. Ces derni√®res refl√©tent une √©volution constante des efforts d√©di√©s √† l'assurance qualit√©. Ce constat est particuli√®rement rassurant pour une application ayant moins d'un an d'existence, comme en t√©moigne la Figure XX indiquant la date du premier commit. Ces contributions actives et r√©guli√®res peuvent garantir une qualit√© accrue de l'application livr√©e.


![Alt](./assets/images/kitops/nombreDeCommit.png)
<div style="text-align: center;">
  <p><em>Figure XX : Contributions des d√©veloppeurs dans le dossier des workflows.</em></p>
</div>


![Alt](./assets/images/kitops/dateLastCommit.png)
<div style="text-align: center;">
  <p><em>Figure XX : Date du dernier push dans le dossier des tests.</em></p>
</div>


![Alt](./assets/images/kitops/nombreDeCommitTests.png)
<div style="text-align: center;">
  <p><em>Figure XX : Contributions des d√©veloppeurs dans le dossier des tests.</em></p>
</div>

![Alt](./assets/images/kitops/dateFirstCommit.png)
<div style="text-align: center;">
  <p><em>Figure XX : Date du premier push du projet.</em></p>
</div>


### Conclusion
***
KitOps se distingue comme un outil efficace pour la gestion, le packaging, et le d√©ploiement des artefacts de machine learning, gr√¢ce √† ses fonctionnalit√©s adapt√©es aux besoins des projets IA/ML. Bien que ses tests ne couvrent pas l'int√©gralit√© des sc√©narios possibles, ils assurent une couverture suffisante pour garantir le bon fonctionnement des fonctionnalit√©s principales de l'outil. Ces tests, bien que vari√©s, reposent sur les principes classiques des applications, tels que les tests unitaires.

L'accent est mis sur des aspects essentiels comme la gestion des fichiers, le packaging/d√©packaging des ModelKits, et la reproductibilit√© des processus. KitOps offre ainsi une base solide pour garantir la portabilit√© et la robustesse des artefacts dans divers environnements.

<br></br>

## üß™ MLFlow

### Fonctionnement
***
MLFlow est une plateforme MLOps qui r√©pond aux principaux besoins des workflows de machine learning. Ses fonctionnalit√©s cl√©s incluent:
- **Suivi des exp√©riences :** permet d'enregistrer et de suivre les diff√©rentes exp√©rimentations effectu√©es dans un projet de machine learning. Cela inclut les param√®tres utilis√©s, les donn√©es d'entr√©e, les m√©triques obtenus, et les artefacts produits.  Ce suivi facilite la comparaison et la reproductibilit√© des mod√®les.
- **D√©ploiement des mod√®les :** facilite le packaging et le d√©veloppement des mod√®les ML, permettant de les servir comme des API REST. Il prend en charge divers environnements de d√©ploiement, notamment les services cloud et les clusters Kubernetes [(MLflow Overview)](https://mlflow.org/docs/latest/introduction/index.html).
- **Gestion de cycle de vie des mod√®les :** la plateforme propose des outils pour versionner, valider et g√©rer les mod√®les tout au long de leur cycle de vie, de la phase de test √† celle de production.
- **Int√©gration avec des frameworks populaires :** MLFlow s'int√®gre avec TensorFlow, PyTorch, Keras et d'autres frameworks de deep learning, offrant des options de journalisation automatique et manuelle. Cette int√©gration facilite l'enregistrement des m√©triques, des artefacts, et des mod√®les produits par ces frameworks.
<br></br>


Ces fonctionnalit√©s sont rendues possibles gr√¢ce √† quatre composants principaux : 
- **MLFlow Tracking :** fournit une API et une interface utilisateur pour enregistrer les exp√©rimentations. Les d√©veloppeurs peuvent suivre les param√®tres, les m√©triques et les artefacts g√©n√©r√©s, ainsi que comparer et rechercher ces exp√©rimentations dans une base centralis√©e.
- **MLFlow Models :** offre un format standardis√© pour empaqueter les mod√®les, en incluant leurs d√©pendances et les donn√©es n√©cessaires √† leur ex√©cution. Il permet d'√©valuer les mod√®les sur diff√©rentes plateformes sans avoir √† r√©√©crire le code.
- **MLFlow Projects :** gr√¢ce √† un format YAML, il permet de structurer les projets de mani√®re reproductible. Chaque projet d√©finit ses d√©pendances, les scripts √† ex√©cuter, et les param√®tres n√©cessaires, facilitant ainsi le partage et l'ex√©cution automatis√©e.
- **MLFlow Model Registry :** agit comme un hub collaboratif pour g√©rer les mod√®les. Il prend en charge leur versionnement, la gestion des √©tats (test√©, valid√©, d√©ploy√©), et le suivi des transitions entre ces √©tats.

![Alt text](./assets/images/mlflow/structure/mlflowdeployhelp.png)
<div style="text-align: center;">  
  <p><em>Figure XX.</em> Utilisation de MLFlow</p>
</div>
<br></br>

En compl√©ment, MLFlow propose des **recettes** pour acc√©l√©rer les d√©veloppements avec des **mod√®les pr√©configur√©s** , des outils d'**√©valuation** pour g√©n√©rer automatiquement des **m√©triques** et des **visualisations**, ainsi que des fonctionnalit√©s de **d√©ploiement** coh√©rentes et √©volutives, avec prise en charge des **environnements Docker**.

### Analyses
***
L'approche CI/CD du repository github de MLFlow combine [CircleCI](https://github.com/mlflow/mlflow/blob/master/.circleci/config.yml), [DevContainer](https://github.com/mlflow/mlflow/tree/master/.devcontainer), [Github Actions](https://github.com/mlflow/mlflow/tree/master/.github), une configuration qui offre des avantages uniques pour l'automatisation des workflows et la gestion des environnements :

#### CircleCI
***
Utilis√© pour sa puissance et sa flexibilit√© dans l'ex√©cution des pipelines CI/CD, il permet de g√©rer des tests complexes et des d√©ploiements scalables.
<br></br>
*Ci-dessous les jobs lanc√©s dans circleCI :*
<table style="width: 100%;">
  <tr>
    <td style="width: 50%; text-align: center;">
      <img src="./assets/images/mlflow/circleci/jobs.png" style="width: 100%; max-width: 600px;">
      <p><em>Figure XX.</em> circleCI jobs</p>
    </td>
    <td style="width: 50%; font-size: 0.85em;">
      <div>
       <p>
          <strong>windows</strong>: Ce job cr√©e un environnement Windows pour ex√©cuter des tests ou des √©tapes sp√©cifiques √† la plateforme Windows. Il effectue un simple checkout du code source et est utilis√© principalement pour tester des fonctionnalit√©s sur une machine virtuelle Windows.
        </p>
        <p>
          <strong>build_doc_r</strong>: Ce job utilise une machine virtuelle de base pour v√©rifier la Pull Request, puis g√©n√®re la documentation API en utilisant un script d√©di√©. Si des fichiers ont √©t√© modifi√©s, il √©choue pour garantir l'int√©grit√© de la documentation.
        </p>
        <p>
          <strong>build_doc</strong>: Ce job s'ex√©cute dans un environnement Docker avec Python et Node.js. Il installe les d√©pendances n√©cessaires (Java, Pandoc, Python), puis g√©n√®re la documentation √† l'aide de yarn et de scripts Python, et enfin stocke les artefacts g√©n√©r√©s.
        </p>      
      </div>
    </td>
  </tr>
</table>

**mlflow-recipes :** Ce job ex√©cute des **recettes MLflow** pour tester une **r√©gression r√©elle** en utilisant des **notebooks Jupyter**. Il met √† jour les **sous-modules Git** et **installe les d√©pendances** n√©cessaires. Ensuite, il **ex√©cute les recettes** et les notebooks, convertissant les r√©sultats en diff√©rents formats. Ce processus permet de **tester et valider les performances du mod√®le** sur des **donn√©es concr√®tes**, en sauvegardant les **artefacts g√©n√©r√©s** pour assurer la reproductibilit√© et la tra√ßabilit√© des r√©sultats.

*Voici un diagramme qui d√©crit le fonctionnement du workflow circleCI :* 
![Alt text](./assets/images/mlflow/circleci/workflow_diagram.svg)
<div style="text-align: center;">  
  <p><em>Figure XX.</em> Workflow CircleCI : Construction de Documentation et Ex√©cution MLFlow</p>
</div>
<br></br>


Le sous-module `examples/recipes` dans le d√©p√¥t GitHub de MLflow est principalement utilis√© pour fournir des exemples pratiques de l'utilisation de MLflow Recipes. Selon la [pull request #13565](https://github.com/mlflow/mlflow/pull/13565), ces exemples aident √† tester et valider les fonctionnalit√©s MLOps en permettant aux utilisateurs d'automatiser des t√¢ches courantes comme l'ingestion de donn√©es, l'entra√Ænement de mod√®les et l'√©valuation des performances.
<br></br>
Mis √† jour pour garantir la compatibilit√© avec les derni√®res versions de Python (notamment Python 3.9), ce sous-module permet √©galement de tester l'int√©gration des nouvelles fonctionnalit√©s de MLflow et d'assurer leur bon fonctionnement avec les d√©pendances existantes. Ces recettes servent de base pour automatiser les workflows, tout en assurant une gestion coh√©rente des mises √† jour et des tests des composants MLflow.


En analysant le circleCI du projet, plusieurs artefacts ont √©t√© analys√©s.
[Un lien vers les artefacts dans circleCI pour un build r√©cent](https://app.circleci.com/pipelines/github/mlflow/mlflow/45551/workflows/e0615a39-6644-425a-8f3f-4a2c768776ad/jobs/141272/artifacts)

#### Exemple d'**artifact** g√©n√©r√© par le job **mlflow-recipes** qui regroupe toute les √©tapes courantes du proc√©d√© de machine learning  : [Notebook MLflow - R√©gression sur Databricks](https://output.circle-artifacts.com/output/job/17301095-7996-4847-8f33-f075f56c2c95/artifacts/0/examples/recipes/regression/notebooks/dark-jupyter.html)

Dans le cadre de l'exp√©rimentation avec des **mod√®les de r√©gression**, ce notebook permet d'**automatiser l'ex√©cution des √©tapes** du pipeline de **donn√©es et de mod√©lisation** sur **Databricks** (plateforme cloud pour le **big data et l'IA**), tout en g√©n√©rant des **r√©sultats pour √©valuer la performance du mod√®le**. Voici les √©tapes principales du processus :

| **√âtape**                       | **Description**                                                                                                                                           | **R√©sultats Cl√©s**                                               |
|----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------|
| **1. Chargement & Pr√©paration**  | Pr√©paration de l'environnement d'ex√©cution et chargement des d√©pendances.                                                                                 | Environnement pr√™t, d√©pendances charg√©es.                        |
| **2. Cr√©ation de la Recette**    | Initialisation de la recette MLflow, nettoyage et inspection des composants.                                                                               | Recette initialis√©e et valid√©e.                                 |
| **3. Ingestion des Donn√©es**     | Chargement des donn√©es du fichier `sample.parquet`, cr√©ation du profil et sch√©mas.                                                                         | Donn√©es ing√©r√©es avec informations, aper√ßu des donn√©es g√©n√©r√© et r√©sum√© avec le **nombre de lignes ing√©r√©s**, la **dur√©e d'ex√©cution** et la **date du dernier update**                                 |
| **4. Division des Donn√©es**      | S√©paration des donn√©es en ensembles d'**entra√Ænement**, **validation** et **test**.                                                                                   | Tableau comparatif des **statistiques descriptives** de plusieurs **variables num√©riques et temporelles** sur diff√©rents **groupes de donn√©es**. Une partie r√©sume les **dimensions** des ensembles d'entrainement, validation et test. Le nombre de **ligne supprim√©** , **dur√©e d'ex√©cution** |
| **5. Transformation des Donn√©es**| Application de transformations comme la normalisation et cr√©ation de nouvelles features                                                                  | Donn√©es transform√©es, **avertissements g√©r√©s si n√©cessaires**. Vue sur le profile des donn√©es, les **inputs, outputs Schema**, la **pr√©visualisation** des donn√©es et un r√©sum√©.      |
| **6. Entra√Ænement du Mod√®le**    | Entra√Ænement du mod√®le de r√©gression avec les donn√©es pr√©par√©es.                                                                                          | Mod√®le entra√Æn√©, ajust√© aux donn√©es d'entra√Ænement. Acc√®s aux **performances** du mod√®les avec **validations**, le profile de **pr√©diction**, l'**architecture** du mod√®le, les moins bonnes pr√©dictions, une comparaison  **(Worst VS Train)**, un **classement**, les **meilleurs param√®tres**, warning logs et r√©sum√©      |
| **7. R√©sultats & Validation**    | √âvaluation du mod√®le √† l'aide de m√©triques comme RMSE et \( R^2 \).                                                                                       | **M√©triques** calcul√©es : RMSE, \( R^2 \) pour la performance du mod√®le et valid√© avec des **seuils de m√©triques**|
| **8. Enregistrement des Artifacts** | **Enregistrement des artifacts du mod√®le** (mod√®le, donn√©es d'entra√Ænement) pour analyse ult√©rieure.                                                          | Mod√®le et donn√©es sauvegard√©s sous forme d'artifacts r√©cup√©rables. |

*Les points cl√©s de ces r√©sultats sont :*

- **Automatisation & Tra√ßabilit√© :** Chaque √©tape du pipeline est automatis√©e et trace les r√©sultats pour une gestion compl√®te du cycle de vie du mod√®le.
- **MLOps :** L'int√©gration avec MLflow permet de g√©rer le mod√®le et les donn√©es tout au long de son cycle de vie.
- **√âvaluation Continue :** Les m√©triques √† chaque √©tape garantissent une √©valuation pr√©cise de la performance du mod√®le.
<br></br>



#### DevContainer
***
Garantit un environnement de d√©veloppement coh√©rent et portable, permettant √† chaque membre de l'√©quipe de travailler dans des conditions identiques, quel que soit le syst√®me local.

*Voici un diagramme qui d√©crit le fonctionnement de DevContainer :*

<p style="text-align: center;">
  <img src="assets/images/mlflow/devcontainer/devcontainer.svg" alt="Devcontainer Workflow" width="800">
</p>
<p style="text-align: center;"><em>Figure XX.</em> DevContainer : √âtapes et fonctionnement</p>
<br></br>

Le diagramme pr√©sent√© illustre les diff√©rentes √©tapes du fonctionnement d'un environnement DevContainer :

1. **VSCode** utilise l'extension Remote - Containers pour ouvrir le projet.
2. **Docker** construit une image DevContainer configur√©e avec les outils n√©cessaires comme Python, MLflow, et Node.js.
3. Un **environnement de d√©veloppement** complet est cr√©√©, pr√™t pour l'ex√©cution de tests (pytest) et le contr√¥le qualit√© via les **hooks de pre-commit**. Ces derniers peuvent emp√™cher des fichiers inutiles ou des erreurs d'√™tre commis (par exemple, √©viter les commits avec des fichiers temporaires ou binaires).
4. Les changements sont pouss√©s vers le d√©p√¥t GitHub, d√©clenchant des workflows **CI/CD** automatis√©s.
5. Les artefacts sont g√©n√©r√©s et d√©ploy√©s, garantissant des r√©sultats reproductibles.

<br></br>

Le **DevContainer** est un outil fondamental pour optimiser le d√©veloppement logiciel en assurant un **environnement de travail uniforme** et facilement reproductible. En configurant √† l'avance des outils cl√©s comme **Python**, **MLflow**, et **Node.js**, il √©limine les probl√®mes li√©s aux diff√©rences de configurations locales entre les d√©veloppeurs. Cela permet non seulement d‚Äô√©conomiser du temps en √©vitant les √©tapes de **configuration manuelle**, mais aussi de faciliter l‚Äô**int√©gration continue**, gr√¢ce √† l‚Äôutilisation de **tests automatis√©s**, de **hooks de pre-commit**, et de **pipelines CI/CD** bien structur√©s. De plus, la gestion des versions de Python via **pyenv** assure une flexibilit√© qui garantit la compatibilit√© des environnements de d√©veloppement. Globalement, le DevContainer am√©liore la **collaboration** au sein des √©quipes, r√©duit les erreurs dues √† des **configurations incompatibles** et permet aux d√©veloppeurs de se concentrer sur l‚Äôessentiel : le **d√©veloppement du code**.

<br></br>

#### GitHub Actions
***


[JOB qui contient actions de tests unitaires concernant les ML](https://github.com/mlflow/mlflow/actions/runs/12313242985/job/34366868186)

<br></br>
Pour MLflow, les actions GitHub incluent g√©n√©ralement des tests unitaires, des tests d'int√©gration et des tests linting dans le cadre du pipeline CI. Ces tests garantissent que les modifications de code ne perturbent pas les fonctionnalit√©s existantes et maintiennent la qualit√© du code. Les flux de travail CI/CD dans MLflow impliquent √©galement des √©tapes de gestion des mod√®les, telles que le test de la s√©rialisation/d√©s√©rialisation des mod√®les et la garantie de la compatibilit√© avec diff√©rentes versions de mod√®les MLflow.
<br></br>
Les tests sont d√©clench√©s sur divers √©v√©nements tels que les commits et les pull requests. Par exemple, les tests unitaires et d'int√©gration sont ex√©cut√©s sur les pull requests pour garantir la qualit√© du code avant un merge, comme d√©crit dans [l'issue GitHub](https://github.com/mlflow/mlflow/issues/10011). De plus, il existe des tests planifi√©s, tels que des t√¢ches cron hebdomadaires, pour ex√©cuter des tests plus lents comme la cr√©ation d'images Docker, garantissant ainsi des tests complets au fil du temps comme le montre la [GitHub pull request](https://github.com/mlflow/mlflow/pull/11004).
<br></br>


#### Environnements et configurations sp√©cifiques
GitHub Actions offre aussi la possibilit√© de configurer des environnements sp√©cifiques pour diff√©rentes technologies, comme Java, Node.js, Python, et Pyenv. Voici quelques exemples de configurations fr√©quemment utilis√©es :

- **Java (OpenJDK 17)** : L'action `setup-java` configure un environnement Java avec la version d'OpenJDK souhait√©e.
- **Node.js** : Gr√¢ce √† l'action `setup-node`, on peut facilement configurer Node.js avec une version sp√©cifique pour les applications JavaScript.
- **Python avec Pyenv** : L'action `pyenv/pyenv-action` permet de g√©rer diff√©rentes versions de Python dans le cadre de tests ou de d√©ploiements.
- **Python (installation avec `pip`)** : L'action `setup-python` permet d'installer une version sp√©cifique de Python et d'ex√©cuter les tests avec `pip`.

Ces actions facilitent l'automatisation des configurations de d√©veloppement et permettent de garantir la coh√©rence des environnements de travail, tout en r√©duisant le temps de configuration et de d√©ploiement.

<br></br>

*Voici quelque workflows et actions pr√©sents dans le projet MLFlow:*

| Workflow                         | D√©clencheur(s)                      | Actions principales                                     | Environnement                  | R√©sultat attendu                        |
|-----------------------------------|-------------------------------------|---------------------------------------------------------|---------------------------------|------------------------------------------|
| **Tests lents (Slow Tests)**      | Quotidien √† 13:00 UTC, PR modifi√©e  | Installation des d√©pendances, ex√©cution des tests en parall√®le | MLFLOW_RUN_SLOW_TESTS          | Rapport des tests et validation des fonctionnalit√©s complexes |
| **Revue automatis√©e des PR**      | Requ√™te de revue sur une PR         | Assignation dynamique des reviewers, retrait des reviewers inutiles | Liste dynamique des reviewers   | Revue de code optimis√©e sans redondances |
| **Tests d'int√©gration UnityCatalog** | Quotidien ou PR sur fichiers critiques | Clonage d√©p√¥t, construction serveur local, tests d'int√©gration | Java avec OpenJDK 17            | Validation de l'int√©gration MLflow / UnityCatalog |
| **Tests Docker**                  | PR ou tests quotidiens              | Installation des d√©pendances, tests segment√©s           | Docker avec tf-keras et pyarrow | Validation des images Docker MLflow      |
| **Workflow g√©n√©ral UC-OSS**       | Quotidien √† 13:00 UTC, PR sur fichiers cl√©s | Construction et ex√©cution des tests dans un environnement Python/Java, int√©gration avec UnityCatalog | Python/Java                    | Validation des composants critiques (protos, store) |
| **Deployments**                   | PR (opened, synchronize, reopened, ready_for_review), Push (branches master et branch-[0-9]+.[0-9]+) | V√©rification PR, installation des d√©pendances, tests avec pytest | Ubuntu-latest              | Validation des d√©ploiements avec tests r√©ussis         |
| **Devcontainer**                  | Push sur master, PR (opened, synchronize, reopened, ready_for_review) sur .devcontainer/** | Construction d'image Docker, tests de l'image, push vers ghcr.io | Ubuntu-latest, QEMU (si n√©cessaire), Docker | Construction et validation de l'image Docker, push vers le registre |
| **Maintainer approval**           | PR                                                   | V√©rification de l'approbation par un mainteneur core      | Ubuntu-latest              | Validation de l'approbation mainteneur avant fusion    |
| **Tests sur nouvelles fonctionnalit√©s** | PR ou push sur des branches sp√©cifiques | Tests fonctionnels et de r√©gression des nouvelles fonctionnalit√©s | Python avec dependencies sp√©cifiques | Validation du bon fonctionnement des nouvelles fonctionnalit√©s |
| **V√©rification de la conformit√© du code** | PR sur la branche principale  | Analyse statique du code, tests de conformit√©, mise √† jour des d√©pendances | Ubuntu-latest, outils de linting | Validation de la qualit√© du code et conformit√© aux standards |

<br></br>

| Action                         | D√©clencheur(s)                      | Actions principales                                     | Environnement                  | R√©sultat attendu                        |
|-----------------------------------|-------------------------------------|---------------------------------------------------------|---------------------------------|------------------------------------------|
| **Cache des d√©pendances Python** | Lors de l'ex√©cution des tests | Mise en cache du r√©pertoire `.venv` pour acc√©l√©rer l'installation des d√©pendances | Python | Acc√©l√©ration des installations en r√©utilisant les fichiers t√©l√©charg√©s |
| **Lib√©ration d'espace disque**    | Apr√®s chaque ex√©cution d'un test    | Suppression de r√©pertoires sp√©cifiques sur le serveur d'int√©gration | Environnement d'int√©gration | Optimisation de l'espace disque pendant l'int√©gration |
| **Validation de l'auteur des commits** | Lors d'une pull request | V√©rification de la conformit√© de l'auteur des commits avec les conventions | - | Maintien du contr√¥le sur l'int√©grit√© et la qualit√© des contributions |


En analysant ces workflows et actions, plusieurs aspects importants permettent de v√©rifier la fiabilit√© et la testabilit√© de l'outil. La **s√©curit√©** est assur√©e gr√¢ce √† des m√©canismes comme l'**approbation par un mainteneur** avant la fusion des pull requests, garantissant ainsi un contr√¥le qualit√© strict sur les contributions. L'outil prend √©galement en charge **plusieurs environnements** (Java, Python, Docker, etc.), ce qui permet d'automatiser et de garantir la coh√©rence des tests dans divers contextes de d√©veloppement. La **simplification de l'utilisation** est un autre point fort, avec des actions comme le **cache des d√©pendances Python** pour acc√©l√©rer les installations et la **lib√©ration d'espace disque** pour optimiser les ressources durant l'int√©gration. Enfin, la fiabilit√© et la testabilit√© sont renforc√©es par des **tests d'int√©gration** et des **tests de r√©gression**, validant ainsi les nouvelles fonctionnalit√©s et assurant la robustesse du syst√®me. Ensemble, ces √©l√©ments contribuent √† une gestion efficace des workflows tout en garantissant une haute qualit√© et une tra√ßabilit√© des changements.



[Lien vers le readme des workflows dans .github](https://github.com/mlflow/mlflow/blob/master/.github/workflows/README.md)


### Conclusion
***
MLFlow est un outil tr√®s utile et d√©velopp√© pour g√©rer tout le cycle de vie des mod√®les d'apprentissage automatique. Il permet de suivre les exp√©riences, de g√©rer les versions des mod√®les, et de les d√©ployer facilement en production. Il automatise les tests et le d√©ploiement des mod√®les, offrant ainsi une solution efficace pour les √©quipes MLOps tout en assurant la tra√ßabilit√© et la reproductibilit√© des mod√®les et la s√©curit√©. Il permet √©galement de tester la fiabilit√© des mod√®les avec des m√©triques et des seuils de validations avec des alertes de r√©sultat anormales. Finalement MLFlow a plusieurs fonctionnalit√©s pour le MLOps mais √©galement le DevOps.
<br></br>


## üåÄ MetaFlow


### Fonctionnement 
***

### Analyses
***

Lors de notre √©tude de l'outil Metaflow, nous avons observ√© que ses pipelines sont structur√©s en plusieurs fichiers YAML, chacun d√©di√© √† des t√¢ches sp√©cifiques. Ces pipelines, con√ßus avec GitHub Actions. Voici une description d√©taill√©e des principaux fichiers de pipeline identifi√©s :


1. test.yml:

Ce workflow GitHub Actions est structur√© pour tester et valider la qualit√© du code dans plusieurs environnements, incluant Python et R, tout en automatisant des processus comme l'installation de d√©pendances et l'ex√©cution des tests. Voici une explication d√©taill√©e des √©tapes et des t√¢ches r√©alis√©es dans ce pipeline :

- D√©clenchement du Workflow : Le workflow est activ√© lors d'un push ou d'une pull_request sur la branche master.

- Job pre-commit :

Ce job inclut trois √©tapes principales :
- Checkout du code
- Installation de Python
- Pr√©paration du Commit : L'action pre-commit/action est utilis√©e pour lancer des hooks de pr√©-commit, permettant de s'assurer que le code respecte les normes d√©finies avant d'√™tre commit√©.

- Job Python :
- Installation des d√©pendances
- Ex√©cution des tests : La commande tox est utilis√©e pour ex√©cuter les tests d√©finis pour chaque version de Python, garantissant la compatibilit√© et la fiabilit√© du code sur plusieurs versions de Python.

#### Tests

  les tests concistent √†


  - Tests sur le stockage des donn√©es (S3)
  Pour tester la fiabilit√© de Metaflow dans la gestion des donn√©es, il est essentiel de v√©rifier sa capacit√© √† interagir correctement avec les syst√®mes de stockage comme Amazon S3. Ces tests consistent √† sauvegarder des donn√©es (par exemple, un fichier ou un tableau), puis √† les r√©cup√©rer pour v√©rifier leur int√©grit√©. Il est √©galement important de simuler des erreurs, comme un fichier manquant, afin de s‚Äôassurer que Metaflow affiche des messages d‚Äôerreur clairs et appropri√©s. Ces tests permettent de valider la capacit√© de l‚Äôoutil √† manipuler les donn√©es de mani√®re s√©curis√©e, m√™me en cas de coupures r√©seau ou d‚Äôautres probl√®mes inattendus.

  - Tests d'int√©gration (fonctionnement global)
  Les tests d'int√©gration v√©rifient si toutes les parties principales de Metaflow, comme la gestion des √©tapes, le stockage des donn√©es et l‚Äôex√©cution du code, fonctionnent bien ensemble. Un exemple de test consiste √† cr√©er un petit workflow avec plusieurs √©tapes (par exemple : importer des donn√©es, les transformer et les sauvegarder) et √† ex√©cuter ce workflow pour s‚Äôassurer que chaque √©tape s‚Äôex√©cute correctement. Cela permet de v√©rifier si Metaflow peut g√©rer des projets du d√©but √† la fin sans erreurs. Ce type de test est crucial pour valider le bon fonctionnement global de l‚Äôoutil dans des sc√©narios r√©els.

  - Tests des graphes (processus en √©tapes)
  Metaflow permet de g√©rer des workflows en √©tapes, qu‚Äôelles soient lin√©aires (une √©tape suit une autre) ou parall√®les (plusieurs √©tapes s‚Äôex√©cutent simultan√©ment). Les tests ici consistent √† cr√©er des workflows simples et complexes pour v√©rifier que chaque √©tape s‚Äôex√©cute dans le bon ordre et au bon moment. En simulant des √©checs d‚Äô√©tapes, on peut √©galement √©valuer la capacit√© de Metaflow √† g√©rer les erreurs proprement, soit en continuant le workflow, soit en l‚Äôarr√™tant de mani√®re contr√¥l√©e. Ces tests permettent de garantir que Metaflow est adapt√© √† des workflows complexes et vari√©s.

  - Tests des environnements d'ex√©cution ("contexts")
  Metaflow peut √™tre utilis√© dans divers environnements, comme un ordinateur local, un service cloud ou avec diff√©rentes versions de Python. Les tests d‚Äôenvironnements d'ex√©cution consistent √† ex√©cuter les m√™mes workflows sur ces diff√©rentes plateformes et √† comparer les r√©sultats pour s‚Äôassurer qu‚Äôils sont identiques. Il est √©galement utile de tester des situations o√π les ressources (comme la m√©moire ou le CPU) sont limit√©es pour voir comment Metaflow s‚Äôadapte. Ces tests valident la flexibilit√© et la compatibilit√© de Metaflow avec divers contextes techniques, ce qui est essentiel pour son adoption dans diff√©rents syst√®mes.

  - Tests des interfaces utilisateur (CLI et API)
  Metaflow propose une interface en ligne de commande (CLI) et une API Python pour interagir avec l‚Äôoutil. Les tests dans ce domaine consistent √† v√©rifier que les commandes et fonctions principales sont faciles √† utiliser et donnent les r√©sultats attendus. Par exemple, il s‚Äôagit de tester des commandes simples pour cr√©er un workflow, lancer une ex√©cution ou r√©cup√©rer des donn√©es. Il est √©galement important de s‚Äôassurer que les messages d‚Äôerreur sont clairs et informatifs. Ces tests permettent de s‚Äôassurer que l‚Äôexp√©rience utilisateur est fluide et que les d√©veloppeurs peuvent utiliser Metaflow sans difficult√©.

  - R√©sum√© des tests
  Ces tests se concentrent sur diff√©rents aspects fondamentaux de Metaflow : la gestion fiable des donn√©es, le fonctionnement global, la flexibilit√© dans diff√©rents environnements et la simplicit√© des interfaces utilisateur. En menant ces tests, on √©value Metaflow en tant qu‚Äôoutil logiciel, ind√©pendamment des workflows de Machine Learning qui l‚Äôutiliseront. Cela permet de v√©rifier que l‚Äôoutil est robuste, fiable et pratique pour les d√©veloppeurs, ce qui est essentiel pour garantir son adoption et son succ√®s.

- Job R :
Installation des d√©pendances syst√®me.
Ex√©cution des tests R : Les tests sont ex√©cut√©s √† l'aide de Rscript, garantissant que le code R fonctionne correctement. Metaflow, qui prend en charge √† la fois Python et R, permet de tirer parti des outils sp√©cifiques √† chaque langage. Cette approche offre plus de flexibilit√©, permettant d'effectuer des tests et validations dans les deux environnements de mani√®re coh√©rente. Les m√™mes tests qui sont r√©alis√©s en Python sont √©galement lanc√©s en R, assurant une couverture compl√®te des validations.


## Comparaison des outils
Pour cette comparaison g√©n√©r√© √† l'aide du script python `assets\codes\datefisrtcommit.py`, nous pouvons observer plusieurs points int√©ressants :

**MLflow :** Ce projet, qui est le plus r√©cent parmi ceux analys√©s, dispose √©galement du plus grand nombre de contributeurs. Cette forte implication de contributeurs pourrait expliquer pourquoi il est aussi d√©velopp√©, avec une progression rapide et continue.

**KitOps vs ClearML :** Bien que KitOps semble √™tre davantage d√©velopp√© que ClearML, il est port√© par un nombre plus restreint de contributeurs. Cependant, le fait que KitOps soit un projet plus r√©cent pourrait indiquer que ClearML, bien qu'ayant mobilis√© plus de contributeurs par le pass√©, est peut-√™tre moins actif actuellement, laissant la place √† un d√©veloppement accru de KitOps.

**Metaflow :** Ce projet, plus ancien que les autres, pr√©sente un nombre de contributeurs similaire √† celui de ClearML. Cela pourrait refl√©ter une certaine stabilit√© dans sa communaut√©, m√™me si son anciennet√© pourrait √©galement expliquer une diminution relative de l'activit√© r√©cente compar√©e aux projets plus jeunes comme MLflow.
<br></br>
<img src="./assets/images/datesetnbcontributorTableau.png" alt="Alt text" style="width: '30%;"/>
<div style="text-align: center;">  
  <p><em>Figure XX : Date 1er commit et nombre de contributeur</em></p>
</div>
<br></br>

Cette √©tude se concentre sur l'**activit√© de d√©veloppement** li√©e aux **tests** et aux **workflows** dans les quatre outils **MLOps : kitops, clearml, mlflow et metaflow**. En utilisant l'**API GitHub** et **Plotly** (`assets\codes\gitnbcommits.py`) pour la visualisation, nous avons sp√©cifiquement analys√© les **commits** dans les r√©pertoires de tests et les workflows. Les donn√©es montrent que **MLflow** pr√©sente une activit√© particuli√®rement intensive dans ces domaines, notamment dans ses dossiers de tests, tandis que **Metaflow** et **kitops** affichent une activit√© plus mod√©r√©e. **ClearML** n'a pas de tests mais a seulement **2 commit** pour son worflow. Cette analyse cibl√©e nous donne un aper√ßu de l'importance accord√©e aux **tests automatis√©s** et √† l'**int√©gration continue** dans ces projets, bien qu'elle ne repr√©sente qu'une partie de leur activit√© de d√©veloppement globale.

*L'image en html pour l'interactivit√© est disponible dans `assets/images/commit_graph.html`* 
<br></br>
<img src="./assets/images/nbCommitParCheminCibleparDepot.png" alt="Alt text" style="width: '10%;"/>
<div style="text-align: center;">  
  <p><em>Figure XX : Nombre de commits par chemin cible pour chaque outil</em></p>
</div>
<br></br>

Parmi les analyses que nous avons r√©alis√©es pour comparer les outils, nous avons pris en compte leur popularit√©, en nous basant sur le nombre d'√©toiles sur GitHub, comme illustr√© dans la Figure XX. Nous avons constat√© que MetaFlow est l'outil le plus appr√©ci√©, en grande partie gr√¢ce √† l'int√©gration avec l'outil de Netflix. Cependant, un point surprenant est que ClearML dispose de plus d'√©toiles que KitOps, bien qu'il semble que KitOps soit plus test√© que ClearML. Nous supposons que cette diff√©rence peut s'expliquer par le fait que ClearML est un outil plus ancien que KitOps.


![Alt text](./assets/images/la%20popularit√©%20des%20outils.png)
<div style="text-align: center;">  
  <p><em>Figure XX : Comparaison de la popularit√© des outils bas√©e sur le nombre d'√©toiles GitHub.</em></p>
</div>

Nous avons √©galement analys√© le pourcentage de bugs parmi les issues GitHub pour chaque outil, comme le montre la Figure XX. L√† encore, un r√©sultat inattendu ressort : bien que MLFlow teste plusieurs phases des t√¢ches de machine learning, il semble avoir plus de bugs que ClearML, qui, paradoxalement, ne semble pas avoir de tests aussi pouss√©s.


![Alt text](./assets/images/le%20pourcentage%20de%20bogues%20dans%20les%20issues%20.png)
<div style="text-align: center;">  
  <p><em>Figure XX : Pourcentage de bugs parmi les issues GitHub pour chaque outil.</em></p>
</div>  

## Conclusion g√©n√©rale

En conclusion, cette √©tude montre que les outils MLOps ne suivent pas tous les m√™mes pratiques en mati√®re de tests. Certains outils ne disposent pas de tests du tout, tandis que d'autres, comme MLFlow et MetaFlow, proposent des tests plus approfondis. En revanche, des outils comme kitops semblent se concentrer davantage sur des tests de base (les tests sur les pack et unack des models et des fichiers). Globalement, les tests pr√©sents dans ces outils peuvent √™tre assimil√©s √† des tests classiques utilis√©s dans les applications traditionnelles, tels que les tests d'int√©gration et les tests unitaires, bien qu'ils pr√©sentent des sp√©cificit√©s li√©es au domaine du machine learning.

## V. Outils
Les diff√©rents scripts d√©velopp√© pour g√©n√©rer certains r√©sultats sont dans le r√©pertoire `assets/codes`.

Un `readme.md` d√©taille l'ex√©cution de ces scripts.

## VI. References

### Github Repository des outils
***
[25 janv. 2025] **MLflow, "mlflow"**, GitHub repository. Disponible : https://github.com/mlflow/mlflow.

[19 janv. 2025] **ClearML, "clearML"**, GitHub repository. Disponible : https://github.com/clearml/clearml/tree/master.

[23 janv. 2025] **Netflix, "metaflow"**, GitHub repository. Disponible : https://github.com/Netflix/metaflow.

[25 janv. 2025] **jozu-ai, "kitops"** GitHub repository. Disponible : https://github.com/jozu-ai/kitops/tree/main.

### Documentations
***

**DOCUMENTATION KitOps:** Simple, secure, and reproducible packaging for AI/ML projects : https://kitops.ml/

**DOCUMENTATION clearML:** An end-to-end AI Platform to streamline AI adoption and the entire development lifecycle : https://clear.ml/docs/latest/docs/

**DOCUMENTATION MLflow:** A Tool for Managing the Machine Learning Lifecycle : 
https://mlflow.org/docs/latest/index.html

### Articles
***
[12 Jul 2020] by dzlab : **Machine Learning Continuous Integration with MLflow:** https://dzlab.github.io/ml/2020/07/12/ml-ci-mlflow/

[18 janv. 2022] by Maxime Jumelle : **MLflow : tout savoir sur l‚Äôoutil indispensable du MLOps :** https://blent.ai/blog/a/mlflow-tout-savoir-sur-loutil-indispensable-du-mlops
